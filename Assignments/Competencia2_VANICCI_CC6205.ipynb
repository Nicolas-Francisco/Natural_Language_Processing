{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0tyIsliieNr"
      },
      "source": [
        "# **Competencia 2 - CC6205 Natural Language Processing 📚**\n",
        "\n",
        "Integrantes: \n",
        "\n",
        "\n",
        "*   Valentina Canales\n",
        "*   Nicolás García\n",
        "*   Ricardo Valdivia \n",
        "\n",
        "\n",
        "\n",
        "Usuario del equipo en CodaLab (Obligatorio): VANICCI\n",
        "\n",
        "Fecha límite de entrega 📆: 29 de Junio.\n",
        "\n",
        "Tiempo estimado de dedicación: 5 horas\n",
        "\n",
        "Link competencia: Poner el link [aquí](https://codalab.lisn.upsaclay.fr/competitions/5098?secret_key=09955d45-6210-4a35-a171-8050aa050855#learn_the_details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQlYlmGaSFH"
      },
      "source": [
        "## **Introducción**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVL0-01AUOzL"
      },
      "source": [
        "En el presente trabajo se busca encontrar un modelo de RNN que permita resolver la tarea **NER** para el conjunto de datos chilenos de **interconsultas de lista de espera NO GES en Chile**. Este corpus Chileno estará constituido por 5 tipos de entidades, las que corresponden a:\n",
        "- **Disease**\n",
        "- **Body_Part**\n",
        "- **Medication** \n",
        "- **Procedures** \n",
        "- **Family_Member**\n",
        "\n",
        "Cada modelo creado en el desarrollo de este trabajo están basados en los modelos vistos en clases y explicados de mejor manera en clases auxiliares. Los modelos implimentados corresponden a RNNs de tipo **LSMT** y **GRU**, además de incluir embeddings en español como *es-clinical* de la libreria Flair.\n",
        "\n",
        "Los resultados obtenidos en la competencia concluyeron que el modelo que entrega mejores resultados fue ..., obteniendo un **F1-Score promedio de 0,86.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbA1EmhCaSFI"
      },
      "source": [
        "## **Modelos**\n",
        "A continuación, se mostrará un resumen de los hiperparámetros y embeddings usados en cada uno de los ocho modelos entrenados:\n",
        "\n",
        "### **Modelo 1**\n",
        "* BiLSTM + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-forward\n",
        "* Hiperparámetros: \n",
        "  * hidden dimmension = 256\n",
        "  * number of layers = 5 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n",
        "\n",
        "### **Modelo 2**\n",
        "* BiLSTM + GloVe + Embedding : \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperparámetros: \n",
        "  * hidden dimmension = 256\n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n",
        "\n",
        "### **Modelo 3**\n",
        "* BiGRU + GloVe + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperparámetros: \n",
        "  * hidden dimmension = 1024 \n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n",
        "\n",
        "### **Modelo 4**\n",
        "* BiRNN + GloVe + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperparámetros: \n",
        "  * hidden dimmension = 1024 \n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n",
        "\n",
        "### **Modelo 5**\n",
        "* BiLSTM\n",
        "* Hiperparámetros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 64 \n",
        "  * number of layers = 2\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n",
        "\n",
        "### **Modelo 6**\n",
        "* BiLSTM + ReLU + Attention layer\n",
        "* Hiperparámetros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 3\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n",
        "\n",
        "### **Modelo 7**\n",
        "* BiGRU\n",
        "* Hiperparámetros: \n",
        "  * embedding dimmension = 100\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 2\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n",
        "\n",
        "### **Modelo 8**\n",
        "* BiGRU + ReLU + Attention Layer\n",
        "* Hiperparámetros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 3\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaVhZ5iaaSFK"
      },
      "source": [
        "## **Métricas de evaluación**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXl3GaVMUYA7"
      },
      "source": [
        "- **Métrica estricta:** Para evaluar el rendimiento de la red se utilizó una metrica estricta para las tres medidas: *precision*, *recall* y *f1-score*. Esta métrica sólo considera correcta una predicción del modelo, sólo si al compararlo con las entidades reales coinciden tanto los límites de la entidad como el tipo.\n",
        "\n",
        "- **Precision:** La precisión es la relación entre las observaciones positivas predichas correctamente y el total de observaciones positivas predichas.\n",
        "Consideraremos precision sobre 0.5 como resultados buenos para el modelo.\n",
        "\n",
        "  $$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}} $$\n",
        "\n",
        "- **Recall:** Entrega el resultado de la razón entre los valores positivos acertados y las suma entre todos los valores positivos de la muestra (acertados y no acertados). En otras palabras, es el valor que se le da al resultado con respecto a la detección de los valores positivos de toda la muestra.\n",
        "\n",
        "  $$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}} $$\n",
        "\n",
        "- **Micro F1 score:** Representa la media armónica de *precision* y *recall*. Esta métrica ayuda a diferenciar el desempeño de un modelo en especifico considerando *precision* y *recall*, de esta manera tendremos un número que nos proporcionará mayor información para saber si un modelo es mejor que otro tomando en cuenta las 2 métricas anteriores. Se utiliza *Micro F1 Score* dado que los valores de cada tag no están en la misma proporcion, por lo que se debe considerar el *F1 Score* del modelo en su totalidad y no individualmente (lo que se realiza en el caso de *Macro F1 Score*):\n",
        "\n",
        "  $$\\text{F1 Score} = \\frac{2 \\times \\text{Recall} \\times \\text{Precision}}{\\text{Recall}+\\text{Precision}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27WffRVUj4v"
      },
      "source": [
        "## **Diseño experimental**\n",
        "\n",
        "Los primeros 4 modelos entrenados tienen el fin de emplear Flair Embedding bajo distintos escenarios, junto con bidireccionalidad, un dropout de 0.5 y epochs iguales a 15. Por otro lado, en los últimos 4 modelos se buscó variar de distintas maneras los hiperparámetros como la cantidad de capas RNN, variar el valor del dropout, la loss function (por ReLU) y añadiendo una capa de atención. A continuación, se describirá con más detalle el diseño de los modelos y los experimentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O228DbeUmE7"
      },
      "source": [
        "### **Modelo 1**\n",
        "\n",
        "En este primer modelo se decidió implementar una red LSTM empleando Flair Embedding con `'es-clinical-forward'`, bidireccionalidad, dropout de 0.5 y 5 capas. Además, se decidió variar la cantidad de dimensiones ocultas hasta encontrar los mejores resultados en este escenario. Así, se encontró que con 256 dimensiones ocultas de las capas de LSTM se logró obtener un presicion de 0.85, un recall de\t0.87, y finalmente un f1-score de **0.83**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDvbvKkBJ9XW"
      },
      "source": [
        "### **Modelo 2**\n",
        "En el segundo modelo se decidió emprear Flair Embedding pero ahora con `'es-clinical-forward'`, `'es-clinical-backward'` y `'GloVe' WordEmbedding` a la vez. Como en el modelo anterior, se mantuvo la red LSTM, también se decidió mantener bidireccionalidad, el dropout y el valor de epoch, mientras que para reducir el tiempo de ejecución se decidió reducir la cantidad de capas de 5 a 3. En este caso, utilizando 256 capas ocultas, se obtuvieron los mejores resultados de este estudio, obteniendo unas métricas de presiction igual a 0.86, recall de 0.88 y un **F1-Score de 0.85**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmSiFGEmJ_bB"
      },
      "source": [
        "### **Modelo 3**\n",
        "En este tercer modelo nuevamente se ocuparon los dos embeddings de Flair y el embedding Glove, pero utilizando esta vez una red GRU y aumentando la cantidad de dimensiones ocultas con el fin de verificar si esta mejora se ve reflejada en los resultados del modelo. Sin embargo, lo anterior fué desmentido en los resultados de prueba, donde se obtuvieron peores resultados que con solo 256 dimensiones: presiction de 0.66, recall de 0.73, y un F1-Score de 0.61.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUcpM9rvJ_pv"
      },
      "source": [
        "### **Modelo 4**\n",
        "En este último modelo con embeddings, se decidió modificar la red ocupada desde el modelo anterior a una red RNN con tanh y manteniendo los mismos hiperparámetros. Los resultados de este último modelo fueron aún peores, obteniendo un presicion de 0.22, un recall de 0.47 y un **F1-Score de 0.15**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URv3KgfHJ_xJ"
      },
      "source": [
        "### **Modelo 5**\n",
        "Como se dijo anteriormente, en los últimos 4 modelos entrenados se decidió ver el comportamiento del modelo variando los hiperparámetros de dimensiones de embeddings, dimensiones ocultas, el número de capas y el tipo de red.\n",
        "Para este quinto modelo, se decidió utilizar una red de tipo LSTM con 200 dimensiones de embedding y 64 dimensiones ocultas. También se eligió un modelo con solo 2 capas y un epoch de 10.\n",
        "Los resultados de este modelo fueron mucho mejores que ocupando los embeddings de Flair con una red tipo GRU o RNN, obteniendo así un presicion de 0.75, un recall de 0.74 y un F1-Score de 0.74.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwkUikD9J_3w"
      },
      "source": [
        "### **Modelo 6**\n",
        "Para este sexto modelo se decidió mantener el tipo de red, pero cambiando la loss function por ReLU y añadiendo una capa de atención. Además, se aumentó tanto la cantidad de dimensiones ocultas y el número de capas. \n",
        "Los resultados obtenidos son tan buenos como los anteriores, obteniendo un presicion de 0.75, un recall de 0.73 y un F1-Score de 0.74."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCO4XBTZKAK2"
      },
      "source": [
        "### **Modelo 7**\n",
        "En este modelo se decidió mantener el setup del modelo número 5, pero aumentando la cantidad de dimensiones ocultas y cambiando el tipo de red por GRU. Los resultados obtenidos siguen sin muchas variaciones, obteniendo un presicion de 0.77, recall de 0.69 y un F1-Score de 0.73.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I_Vb__GKAP_"
      },
      "source": [
        "### **Modelo 8**\n",
        "Manteniendo la idea del modelo anterior, se decidió repetir el setup del sexto modelo, pero cambiando el tipo de red por una de tipo GRU y añadiendo una capa de atención. Este último modelo mantuvo el rendimiento mostrado en los últimos 4, obteniendo un presicion de 0.75, un recall de 0.71, y un F1-Score de 0.73."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFM-wNt8aSFM"
      },
      "source": [
        "## **Experimentos**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMgKjfYC_Go-"
      },
      "source": [
        "###  **Carga de datos y Preprocesamiento**\n",
        "\n",
        "Para cargar los datos y preprocesarlos usaremos la librería [`torchtext`](https://github.com/pytorch/text). Tener cuidado ya que hace algunos meses esta librería tuvo cambios radicales, quedando las funcionalidades pasadas en un nuevo paquete llamado legacy. Esto ya que si quieren usar más funciones de la librería entonces vean los cambios en la documentación.\n",
        "\n",
        "En particular usaremos su módulo `data`, el cual según su documentación original provee: \n",
        "\n",
        "    - Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format\n",
        "    - Ability to define a preprocessing pipeline\n",
        "    - Batching, padding, and numericalizing (including building a vocabulary object)\n",
        "    - Wrapper for dataset splits (train, validation, test)\n",
        "\n",
        "\n",
        "El proceso será el siguiente: \n",
        "\n",
        "1. Descargar los datos desde github y examinarlos.\n",
        "2. Definir los campos (`fields`) que cargaremos desde los archivos.\n",
        "3. Cargar los datasets.\n",
        "4. Crear el vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27csY87GaSFO",
        "scrolled": false,
        "outputId": "8db730d6-2de8-4100-bbe2-d523e0fa155c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# Instalamos torchtext que nos facilitará la vida en el pre-procesamiento del formato ConLL.\n",
        "!pip install -U torchtext==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng7wRGEyawjM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext import data, datasets, legacy\n",
        "\n",
        "\n",
        "# Garantizar reproducibilidad de los experimentos\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BehSou6rCvwg"
      },
      "source": [
        "#### **Obtener datos**\n",
        "\n",
        "Descargamos los datos de entrenamiento, validación y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbT0g_kC18Jb",
        "outputId": "e752fa10-b6f2-4577-89fe-02eb238e4f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘train.txt’ already there; not retrieving.\n",
            "\n",
            "File ‘dev.txt’ already there; not retrieving.\n",
            "\n",
            "File ‘test.txt’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#%%capture\n",
        "\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc # Dataset de Entrenamiento\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc    # Dataset de Validación (Para probar y ajustar el modelo)\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. ¡¡SON LOS QUE DEBEN SER PREDICHOS!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMud7YGMBZvg"
      },
      "source": [
        "####  **Fields**\n",
        "\n",
        "Un `field`:\n",
        "\n",
        "* Define un tipo de datos junto con instrucciones para convertir el texto a Tensor.\n",
        "* Contiene un objeto `Vocab` que contiene el vocabulario (palabras posibles que puede tomar ese campo).\n",
        "* Contiene otros parámetros relacionados con la forma en que se debe numericalizar un tipo de datos, como un método de tokenización y el tipo de Tensor que se debe producir.\n",
        "\n",
        "\n",
        "Analizemos el siguiente cuadro el cual contiene un ejemplo cualquiera de entrenamiento:\n",
        "\n",
        "\n",
        "```\n",
        "El O\n",
        "paciente O\n",
        "padece O\n",
        "de O\n",
        "cancer B-Disease\n",
        "de I-Disease\n",
        "colon I-Disease\n",
        ". O\n",
        "```\n",
        "\n",
        "Cada linea contiene un token y el tipo de entidad asociado en el formato IOB2 ya explicado. Para que `torchtext` pueda cargar estos datos, debemos definir como va a leer y separar los componentes de cada una de las lineas.\n",
        "Para esto, definiremos un field para cada uno de esos componentes: Las palabras (`TEXT`) y las etiquetas o categorías (`NER_TAGS`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DcM_IjgCdzz"
      },
      "outputs": [],
      "source": [
        "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
        "TEXT = legacy.data.Field(lower=False) \n",
        "\n",
        "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
        "NER_TAGS = legacy.data.Field(unk_token=None)\n",
        "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZPxDaaKgeqI",
        "outputId": "1e366483-2e0f-422c-d7f9-ae0ef895f05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('text', <torchtext.legacy.data.field.Field at 0x7faf17e01950>),\n",
              " ('nertags', <torchtext.legacy.data.field.Field at 0x7faf17e01b10>))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCKTJOdgC5eC"
      },
      "source": [
        "####  **SequenceTaggingDataset**\n",
        "\n",
        "`SequenceTaggingDataset` es una clase de torchtext diseñada para contener datasets de sequence labeling. Los ejemplos que se guarden en una instancia de estos serán arreglos de palabras asociados con sus respectivos tags.\n",
        "\n",
        "Por ejemplo, para Part-of-speech tagging:\n",
        "\n",
        "[I, love, PyTorch, .] estará asociado con [PRON, VERB, PROPN, PUNCT]\n",
        "\n",
        "\n",
        "La idea es que usando los fields que definimos antes, le indiquemos a la clase cómo cargar los datasets de prueba, validación y test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsHdGml62J21"
      },
      "outputs": [],
      "source": [
        "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train.txt\",\n",
        "    validation=\"dev.txt\",\n",
        "    test=\"test.txt\",\n",
        "    fields=fields,\n",
        "    encoding=\"utf-8\",\n",
        "    separator=\" \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2qf7YTWHEYR",
        "outputId": "4c061009-c2d2-49c5-fa56-af92623262f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.legacy.datasets.sequence_tagging.SequenceTaggingDataset at 0x7faf17e09fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu7q3HCliia5",
        "outputId": "496b5081-a757-4509-bcb3-1825677d80f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de ejemplos de entrenamiento: 8025\n",
            "Número de ejemplos de validación: 891\n",
            "Número de ejemplos de test (competencia): 992\n"
          ]
        }
      ],
      "source": [
        "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
        "print(f\"Número de ejemplos de validación: {len(valid_data)}\")\n",
        "print(f\"Número de ejemplos de test (competencia): {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDRnhXAdFGL-"
      },
      "source": [
        "Visualizemos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T023Ld4RaSF4",
        "outputId": "5d2f4012-c6e5-45c4-919a-ec02e4614544",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('consulto', 'O'),\n",
              " ('en', 'O'),\n",
              " ('SU', 'O'),\n",
              " ('el', 'O'),\n",
              " ('9', 'O'),\n",
              " ('-', 'O'),\n",
              " ('11', 'O'),\n",
              " ('-', 'O'),\n",
              " ('15', 'O'),\n",
              " ('por', 'O'),\n",
              " ('dolor', 'O'),\n",
              " ('precorddial', 'O'),\n",
              " ('irradiado', 'O'),\n",
              " ('e', 'O'),\n",
              " ('brazo', 'B-Body_Part'),\n",
              " ('izq', 'I-Body_Part'),\n",
              " ('.', 'I-Body_Part'),\n",
              " (',', 'O'),\n",
              " ('además', 'O'),\n",
              " ('singulto', 'O'),\n",
              " ('.', 'O')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import random\n",
        "random_item_idx = random.randint(0, len(train_data))\n",
        "random_example = train_data.examples[random_item_idx]\n",
        "list(zip(random_example.text, random_example.nertags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l05KYy5FSUy"
      },
      "source": [
        "#### **Construir los vocabularios para el texto y las etiquetas**\n",
        "\n",
        "Los vocabularios son los objetos que contienen todos los tokens (de entrenamiento) posibles para ambos fields. El siguiente paso consiste en construirlos. Para esto, hacemos uso del método `Field.build_vocab` sobre cada uno de nuestros `fields`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBhp7WICiibL"
      },
      "outputs": [],
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "NER_TAGS.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4OgUKM_iibO",
        "outputId": "021f652a-68b4-4d62-f8fd-36c010079aa2",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens únicos en TEXT: 17591\n",
            "Tokens únicos en NER_TAGS: 12\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tokens únicos en TEXT: {len(TEXT.vocab)}\")\n",
        "print(f\"Tokens únicos en NER_TAGS: {len(NER_TAGS.vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4FeyL9nFnId",
        "outputId": "5049dcf4-eec0-49f6-e44f-ee3d7fd152b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " 'O',\n",
              " 'I-Disease',\n",
              " 'B-Disease',\n",
              " 'I-Body_Part',\n",
              " 'B-Body_Part',\n",
              " 'B-Procedure',\n",
              " 'I-Procedure',\n",
              " 'B-Medication',\n",
              " 'B-Family_Member',\n",
              " 'I-Medication',\n",
              " 'I-Family_Member']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Veamos las posibles etiquetas que hemos cargado:\n",
        "NER_TAGS.vocab.itos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQDoUqSHFKj"
      },
      "source": [
        "Observen que ademas de los tags NER, tenemos \\<pad\\>, el cual es generado por el dataloader para cumplir con el padding de cada oración.\n",
        "\n",
        "Veamos ahora los tokens mas frecuentes y especiales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5eSLm4diibR",
        "outputId": "54b3b3a2-5888-46ed-b31d-08828d03a744"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 7396),\n",
              " (',', 6821),\n",
              " ('-', 4985),\n",
              " ('de', 3811),\n",
              " ('DE', 3645),\n",
              " ('/', 2317),\n",
              " (':', 2209),\n",
              " ('con', 1484),\n",
              " ('y', 1439),\n",
              " ('APS', 1429)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Tokens mas frecuentes (Será necesario usar stopwords, eliminar símbolos o nos entregan información (?) )\n",
        "TEXT.vocab.freqs.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDyNLMPz9duD"
      },
      "outputs": [],
      "source": [
        "# Seteamos algunas variables que nos serán de utilidad mas adelante...\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "O_TAG_IDX = NER_TAGS.vocab.stoi['O']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrYvF3X0sjWL"
      },
      "source": [
        "#### **Frecuencia de los Tags**\n",
        "\n",
        "Visualizemos rápidamente las cantidades y frecuencias de cada tag:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuXOsbJUiibh",
        "outputId": "673dde73-806d-4618-f038-96608c9d8283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag Ocurrencia Porcentaje\n",
            "\n",
            "O\t101671\t68.1%\n",
            "I-Disease\t21629\t14.5%\n",
            "B-Disease\t8831\t 5.9%\n",
            "I-Body_Part\t6489\t 4.3%\n",
            "B-Body_Part\t3755\t 2.5%\n",
            "B-Procedure\t2891\t 1.9%\n",
            "I-Procedure\t2819\t 1.9%\n",
            "B-Medication\t784\t 0.5%\n",
            "B-Family_Member\t228\t 0.2%\n",
            "I-Medication\t116\t 0.1%\n",
            "I-Family_Member\t9\t 0.0%\n"
          ]
        }
      ],
      "source": [
        "def tag_percentage(tag_counts):\n",
        "    \n",
        "    total_count = sum([count for tag, count in tag_counts])\n",
        "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
        "  \n",
        "    return tag_counts_percentages\n",
        "\n",
        "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
        "\n",
        "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
        "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4wPiydnaSGs"
      },
      "source": [
        "#### **Configuramos pytorch y dividimos los datos.**\n",
        "\n",
        "Importante: si tienes problemas con la ram de la gpu, disminuye el tamaño de los batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB7cwLWpaSGs",
        "outputId": "b2f84c06-8546-42e8-8566-14bca2c076c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 22  # disminuir si hay problemas de ram.\n",
        "\n",
        "# Usar cuda si es que está disponible.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "# Dividir datos entre entrenamiento y test. Si van a hacer algún sort no puede ser sobre\n",
        "# el conjunto de testing ya que al hacer sus predicciones sobre el conjunto de test sin etiquetas\n",
        "# debe conservar el orden original para ser comparado con los golden_labels. \n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B21E1eAFId16"
      },
      "source": [
        "#### **Métricas de evaluación**\n",
        "\n",
        "Además, definiremos las métricas que serán usadas tanto para la competencia como para evaluar el modelo: `precision`, `recall` y `micro f1-score`.\n",
        "**Importante**: Noten que la evaluación solo se hace para las Named Entities (sin contar 'O'), toda esta funcionalidad nos la entrega la librería seqeval, pueden revisar más documentación aquí: https://github.com/chakki-works/seqeval. No utilicen el código entregado por sklearn para calcular las métricas ya que esta lo hace a nivel de token y no a nivel de entidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o63ov69_rX2T",
        "outputId": "a01971ec-466b-41ce-8da6-729419e2a8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mUOOLEWiicU"
      },
      "outputs": [],
      "source": [
        "# Definimos las métricas\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "\n",
        "    # filtramos <pad> para calcular los scores.\n",
        "    mask = [(y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    y_pred = [[NER_TAGS.vocab.itos[v] for v in y_pred]]\n",
        "    y_true = [[NER_TAGS.vocab.itos[v] for v in y_true]]\n",
        "    \n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
        "    precision = precision_score(y_true, y_pred, mode='strict')\n",
        "    recall = recall_score(y_true, y_pred, mode='strict')\n",
        "\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hod516H1aSG2"
      },
      "source": [
        "-------------------\n",
        "\n",
        "### **Modelo Baseline**\n",
        "\n",
        "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendrá una capa de embedding, unas cuantas LSTM y una capa de salida y usará dropout en el entrenamiento.\n",
        "\n",
        "Este constará de los siguientes pasos: \n",
        "\n",
        "1. Definir la clase que contendrá la red.\n",
        "2. Definir los hiperparámetros e inicializar la red. \n",
        "3. Definir el número de épocas de entrenamiento\n",
        "4. Definir la función de loss.\n",
        "\n",
        "\n",
        "\n",
        "Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMPL08XqaSG3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx,\n",
        "                                      )\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCl3530VaSG7"
      },
      "source": [
        "#### **Hiperparámetros de la red**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHdi3QdOaSG8"
      },
      "outputs": [],
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.6\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "baseline_model_name = 'baseline'  # nombre que tendrá el modelo guardado..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlF1DhJeaSHA"
      },
      "outputs": [],
      "source": [
        "baseline_n_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3u4imJGaSHE"
      },
      "source": [
        "#### Definimos la función de loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G_4k99_aSHG"
      },
      "outputs": [],
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRYOEDiQaSHK"
      },
      "source": [
        "--------------------\n",
        "### Modelo 1\n",
        "\n",
        "* BiLSTM + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-forward\n",
        "* Hiperparámetros: \n",
        "  * hidden dimmension = 256\n",
        "  * number of layers = 5 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"git+https://github.com/flairNLP/flair\""
      ],
      "metadata": {
        "id": "24xdc50SCv1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "clinical_emb = FlairEmbeddings('es-clinical-forward')\n",
        "\n",
        "\n",
        "\n",
        "matrix_len = len(TEXT.vocab)\n",
        "EMBEDDINGS = torch.zeros((matrix_len, 2048))\n",
        "words_found = 0\n",
        "for i, word in enumerate(TEXT.vocab.itos):\n",
        "    try: \n",
        "        sw = Sentence(word)\n",
        "        clinical_emb.embed(sw)\n",
        "        for token in sw:\n",
        "          EMBEDDINGS[i] = token.embedding\n",
        "    except KeyError:\n",
        "        EMBEDDINGS[i] = torch.zeros((2048,1))"
      ],
      "metadata": {
        "id": "U3lo6cmhRme3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding, freeze=True, padding_idx=pad_idx)\n",
        "        self.embedding_dim = embedding.size()[1]\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(self.embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(self.dropout(embedded))\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "1kt8_ihoRn94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "EMBEDDING_DIM = EMBEDDINGS.size()[1]\n",
        "N_LAYERS = 5  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model_1 = NER_RNN(INPUT_DIM, EMBEDDINGS, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model_1_name = 'model_1'\n",
        "n_epochs_1 = 15"
      ],
      "metadata": {
        "id": "Lpkt_fK9Rqg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV9oLkN1aSHO"
      },
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 2\n",
        "\n",
        "* BiLSTM + GloVe + Embedding : \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperparámetros: \n",
        "  * hidden dimmension = 256\n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWPzETaNaSHP"
      },
      "outputs": [],
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import FlairEmbeddings, WordEmbeddings, StackedEmbeddings\n",
        "\n",
        "embedding_types = [\n",
        "                    FlairEmbeddings('es-clinical-forward'),\n",
        "                    FlairEmbeddings('es-clinical-backward'),\n",
        "                    WordEmbeddings('glove')\n",
        "]\n",
        "EMBEDDINGS_form = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "matrix_len = len(TEXT.vocab)\n",
        "EMBEDDINGS = torch.zeros((matrix_len, 4196))\n",
        "words_found = 0\n",
        "for i, word in enumerate(TEXT.vocab.itos):\n",
        "    try: \n",
        "        sw = Sentence(word)\n",
        "        EMBEDDINGS_form.embed(sw)\n",
        "        for token in sw:\n",
        "          EMBEDDINGS[i] = token.embedding\n",
        "    except KeyError:\n",
        "        EMBEDDINGS[i] = torch.zeros((4196,1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding, freeze=True, padding_idx=pad_idx)\n",
        "        self.embedding_dim = embedding.size()[1]\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(self.embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(self.dropout(embedded))\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "VmEWDiCUC0xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "EMBEDDING_DIM = EMBEDDINGS.size()[1]\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model_2 = NER_RNN(INPUT_DIM, EMBEDDINGS, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model_2_name = 'model_2'\n",
        "n_epochs_2 = 15"
      ],
      "metadata": {
        "id": "1wcfjqtzC3OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpy3p7YaaSHT"
      },
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 3\n",
        "\n",
        "* BiGRU + GloVe + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperparámetros: \n",
        "  * hidden dimmension = 1024 \n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w0CFjA8aSHU"
      },
      "outputs": [],
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import FlairEmbeddings, WordEmbeddings, StackedEmbeddings\n",
        "\n",
        "embedding_types = [\n",
        "                    FlairEmbeddings('es-clinical-forward'),\n",
        "                    FlairEmbeddings('es-clinical-backward'),\n",
        "                    WordEmbeddings('glove')\n",
        "]\n",
        "EMBEDDINGS_form = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "matrix_len = len(TEXT.vocab)\n",
        "EMBEDDINGS = torch.zeros((matrix_len, 4196))\n",
        "words_found = 0\n",
        "for i, word in enumerate(TEXT.vocab.itos):\n",
        "    try: \n",
        "        sw = Sentence(word)\n",
        "        EMBEDDINGS_form.embed(sw)\n",
        "        for token in sw:\n",
        "          EMBEDDINGS[i] = token.embedding\n",
        "    except KeyError:\n",
        "        EMBEDDINGS[i] = torch.zeros((4196,1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "\n",
        "# Definir la red\n",
        "class GRU_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding, freeze=True, padding_idx=pad_idx)\n",
        "        self.embedding_dim = embedding.size()[1]\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.GRU(self.embedding_dim, hidden_dim, num_layers=n_layers,\n",
        "                             bidirectional=bidirectional, dropout=dropout)\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        outputs, hidden = self.lstm(self.dropout(embedded))\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "Nah90_g6SCsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "HIDDEN_DIM = 1024  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "EMBEDDING_DIM = EMBEDDINGS.size()[1]\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model_3 = GRU_RNN(INPUT_DIM, EMBEDDINGS, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model_3_name = 'model_3'\n",
        "n_epochs_3 = 15"
      ],
      "metadata": {
        "id": "vc1w3DVnSD_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 4\n",
        "\n",
        "* BiRNN(tanh) + GloVe + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperparámetros: \n",
        "  * hidden dimmension = 1024 \n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n"
      ],
      "metadata": {
        "id": "4GeIlk8xR2ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import FlairEmbeddings, WordEmbeddings, StackedEmbeddings\n",
        "\n",
        "embedding_types = [\n",
        "                    FlairEmbeddings('es-clinical-forward'),\n",
        "                    FlairEmbeddings('es-clinical-backward'),\n",
        "                    WordEmbeddings('glove')\n",
        "]\n",
        "EMBEDDINGS_form = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "matrix_len = len(TEXT.vocab)\n",
        "EMBEDDINGS = torch.zeros((matrix_len, 4196))\n",
        "words_found = 0\n",
        "for i, word in enumerate(TEXT.vocab.itos):\n",
        "    try: \n",
        "        sw = Sentence(word)\n",
        "        EMBEDDINGS_form.embed(sw)\n",
        "        for token in sw:\n",
        "          EMBEDDINGS[i] = token.embedding\n",
        "    except KeyError:\n",
        "        EMBEDDINGS[i] = torch.zeros((4196,1))"
      ],
      "metadata": {
        "id": "wTuT5R1bSHI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "\n",
        "# Definir la red\n",
        "class RNN_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding, freeze=True, padding_idx=pad_idx)\n",
        "        self.embedding_dim = embedding.size()[1]\n",
        "        # Capa rnn\n",
        "        self.lstm = nn.RNN(self.embedding_dim, hidden_dim, num_layers=n_layers,\n",
        "                             bidirectional=bidirectional, dropout=dropout)\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        outputs, hidden = self.lstm(self.dropout(embedded))\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "_Q4O1t7gSIXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "HIDDEN_DIM = 1024  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "EMBEDDING_DIM = EMBEDDINGS.size()[1]\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model_4 = RNN_RNN(INPUT_DIM, EMBEDDINGS, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model_4_name = 'model_4'\n",
        "n_epochs_4 = 15"
      ],
      "metadata": {
        "id": "wxZLFDFsSI95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 5\n",
        "\n",
        "* BiLSTM\n",
        "* Hiperparámetros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 64 \n",
        "  * number of layers = 2\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OMc3Q7ljR4yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class NER_RNN_LSTM(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx,\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "5zK6IAKjZMNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM_LSTM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM_LSTM = 200  # dimensión de los embeddings.\n",
        "HIDDEN_DIM_LSTM = 64  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM_LSTM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS_LSTM = 2  # número de capas.\n",
        "DROPOUT_LSTM = 0.5\n",
        "BIDIRECTIONAL_LSTM = False\n",
        "\n",
        "LSTM_n_epochs = 10\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "LSTM = NER_RNN_LSTM(INPUT_DIM_LSTM, \n",
        "                      EMBEDDING_DIM_LSTM, \n",
        "                      HIDDEN_DIM_LSTM, \n",
        "                      OUTPUT_DIM_LSTM,\n",
        "                      N_LAYERS_LSTM, \n",
        "                      BIDIRECTIONAL_LSTM, \n",
        "                      DROPOUT_LSTM, \n",
        "                      PAD_IDX,\n",
        "                      )\n",
        "\n",
        "LSTM_name = 'LSTM'  \n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "LSTM_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "Pp2BofgLZdDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM\n",
        "model_name = LSTM_name\n",
        "criterion = LSTM_criterion\n",
        "n_epochs = LSTM_n_epochs"
      ],
      "metadata": {
        "id": "ew_02UELZihl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights_modelLSTM(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM_LSTM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM_LSTM)\n",
        "        \n",
        "model.apply(init_weights_modelLSTM)"
      ],
      "metadata": {
        "id": "aGY-k2-iZlUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 6\n",
        "\n",
        "* BiLSTM + ReLU + Attention layer\n",
        "* Hiperparámetros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 3\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n",
        "\n"
      ],
      "metadata": {
        "id": "VvUoJBkQR50p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class LSTM_SelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, encoder_outputs):\n",
        "        # encoder_outputs = [batch size, sent len, hid dim]\n",
        "        energy = self.projection(encoder_outputs)\n",
        "        # energy = [batch size, sent len, 1]\n",
        "        weights = F.softmax(energy.squeeze(-1), dim=1)\n",
        "        # weights = [batch size, sent len]\n",
        "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
        "        # outputs = [batch size, hid dim]\n",
        "        return outputs, weights\n",
        "\n",
        "\n",
        "class NER_RNN_LSTM_Attention(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx,\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "        \n",
        "        self.attention = LSTM_SelfAttention(hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # embedded = self.dropout(self.embedding(text))\n",
        "        output, (hidden, cell) = self.lstm(self.embedding(text))\n",
        "        new_embed, weights = self.attention(output)\n",
        "        new_embed = self.dropout(output)\n",
        "        predictions = self.fc(self.dropout(new_embed))\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "vAD1dTq_Zobn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM_LSTMAttention = len(TEXT.vocab)\n",
        "EMBEDDING_DIM_LSTMAttention = 200  # dimensión de los embeddings.\n",
        "HIDDEN_DIM_LSTMAttention = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM_LSTMAttention = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS_LSTMAttention = 3  # número de capas.\n",
        "DROPOUT_LSTMAttention = 0.5\n",
        "BIDIRECTIONAL_LSTMAttention = False\n",
        "\n",
        "LSTMAttention_n_epochs = 10\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "LSTMAttention = NER_RNN_LSTM_Attention(INPUT_DIM_LSTMAttention, \n",
        "                      EMBEDDING_DIM_LSTMAttention, \n",
        "                      HIDDEN_DIM_LSTMAttention, \n",
        "                      OUTPUT_DIM_LSTMAttention,\n",
        "                      N_LAYERS_LSTMAttention, \n",
        "                      BIDIRECTIONAL_LSTMAttention, \n",
        "                      DROPOUT_LSTMAttention, \n",
        "                      PAD_IDX,\n",
        "                      )\n",
        "\n",
        "LSTMAttention_name = 'LSTMAttention'  \n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "LSTMAttention_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "DPqFBb92ZvOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMAttention\n",
        "model_name = LSTMAttention_name\n",
        "criterion = LSTMAttention_criterion\n",
        "n_epochs = LSTMAttention_n_epochs"
      ],
      "metadata": {
        "id": "pgaqx2I6Z1IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights_modelLSTMAttention(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM_LSTMAttention)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM_LSTMAttention)\n",
        "        \n",
        "model.apply(init_weights_modelLSTMAttention)"
      ],
      "metadata": {
        "id": "LRne61t9Z6uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 7\n",
        "\n",
        "* BiGRU\n",
        "* Hiperparámetros: \n",
        "  * embedding dimmension = 100\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 2\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n"
      ],
      "metadata": {
        "id": "NkP71XZIR6vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class NER_RNN_GRU(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx,\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "        \n",
        "        self.lstm = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        outputs, hidden = self.lstm(embedded)\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "MHistYQUaGUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM_GRU = len(TEXT.vocab)\n",
        "EMBEDDING_DIM_GRU = 100  # dimensión de los embeddings.\n",
        "HIDDEN_DIM_GRU = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM_GRU = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS_GRU = 2  # número de capas.\n",
        "DROPOUT_GRU = 0.5\n",
        "BIDIRECTIONAL_GRU = False\n",
        "\n",
        "GRU_n_epochs = 10\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "GRU = NER_RNN_GRU(INPUT_DIM_GRU, \n",
        "                      EMBEDDING_DIM_GRU, \n",
        "                      HIDDEN_DIM_GRU, \n",
        "                      OUTPUT_DIM_GRU,\n",
        "                      N_LAYERS_GRU, \n",
        "                      BIDIRECTIONAL_GRU, \n",
        "                      DROPOUT_GRU, \n",
        "                      PAD_IDX,\n",
        "                      )\n",
        "GRU_name = 'GRU'  \n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "GRU_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "n7FFei7NaPe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRU\n",
        "model_name = GRU_name\n",
        "criterion = GRU_criterion\n",
        "n_epochs = GRU_n_epochs"
      ],
      "metadata": {
        "id": "Yt0CXJgUaVGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights_modelGRU(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM_GRU)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM_GRU)\n",
        "        \n",
        "model.apply(init_weights_modelGRU)"
      ],
      "metadata": {
        "id": "PlW0XMhgaXWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 8\n",
        "\n",
        "* BiGRU + ReLU + Attention Layer\n",
        "* Hiperparámetros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 3\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n"
      ],
      "metadata": {
        "id": "w7IOSkE1R8SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class GRU_SelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, encoder_outputs):\n",
        "        # check, same as lstm attention\n",
        "        energy = self.projection(encoder_outputs)\n",
        "        weights = F.softmax(energy.squeeze(-1), dim=1)\n",
        "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
        "        return outputs, weights\n",
        "        \n",
        "class NER_RNN_GRU_Attention(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx,\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "        \n",
        "        self.lstm = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout)\n",
        "\n",
        "        self.attention = GRU_SelfAttention(hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        output, hidden = self.lstm(self.embedding(text))\n",
        "        new_embed, weights = self.attention(output)\n",
        "        new_embed = self.dropout(output)\n",
        "        predictions = self.fc(self.dropout(new_embed))\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "4ia_ItOJalZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM_GRUAttention = len(TEXT.vocab)\n",
        "EMBEDDING_DIM_GRUAttention = 200  # dimensión de los embeddings.\n",
        "HIDDEN_DIM_GRUAttention = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM_GRUAttention = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS_GRUAttention = 3  # número de capas.\n",
        "DROPOUT_GRUAttention = 0.5\n",
        "BIDIRECTIONAL_GRUAttention = False\n",
        "\n",
        "GRUAttention_n_epochs = 10\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "GRUAttention = NER_RNN_GRU_Attention(INPUT_DIM_GRUAttention, \n",
        "                      EMBEDDING_DIM_GRUAttention, \n",
        "                      HIDDEN_DIM_GRUAttention, \n",
        "                      OUTPUT_DIM_GRUAttention,\n",
        "                      N_LAYERS_GRUAttention, \n",
        "                      BIDIRECTIONAL_GRUAttention, \n",
        "                      DROPOUT_GRUAttention,\n",
        "                      PAD_IDX,\n",
        "                      )\n",
        "\n",
        "GRUAttention_name = 'GRUAttention'  \n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "GRUAttention_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "OJtSsCYCamJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRUAttention\n",
        "model_name = GRUAttention_name\n",
        "criterion = GRUAttention_criterion\n",
        "n_epochs = GRUAttention_n_epochs"
      ],
      "metadata": {
        "id": "6Yf289kpat3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights_modelGRUAttention(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM_GRUAttention)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM_GRUAttention)\n",
        "        \n",
        "model.apply(init_weights_modelGRUAttention)"
      ],
      "metadata": {
        "id": "nt_Tk8kkawVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPGdirx7aSHZ"
      },
      "source": [
        "------\n",
        "### **Entrenamos y evaluamos**\n",
        "\n",
        "\n",
        "**Importante** : Fijen el modelo, el número de épocas de entrenamiento, la loss y el optimizador que usarán para entrenar y evaluar en las siguientes variables!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8YlGnjxaSHZ"
      },
      "outputs": [],
      "source": [
        "model = model_3\n",
        "model_name = model_3_name\n",
        "criterion = baseline_criterion\n",
        "n_epochs = n_epochs_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu_lXic2aSHd"
      },
      "source": [
        "\n",
        "\n",
        "#### **Inicializamos la red**\n",
        "\n",
        "Iniciamos los pesos de la red de forma aleatoria (Usando una distribución normal).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-G_NWFcaSHe"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "        \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjWDX2CJaSHh"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVqBqerlaSHk"
      },
      "source": [
        "Notar que definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZvHtwpaSHq"
      },
      "source": [
        "#### **Definimos el optimizador y el scheduler**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH6o8_cTaSHq"
      },
      "outputs": [],
      "source": [
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import lr_scheduler\n",
        "# learning rate\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
      ],
      "metadata": {
        "id": "OAPnDokXSOwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz39wa78wGYR"
      },
      "source": [
        "#### **Enviamos el modelo a cuda**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqr0AJ6_iicR"
      },
      "outputs": [],
      "source": [
        "# Enviamos el modelo y la loss a cuda (en el caso en que esté disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Defininimos Early-Stopping**"
      ],
      "metadata": {
        "id": "PQHBEozTSVMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping():\n",
        "    \"\"\"\n",
        "    Early stopping to stop the training when the loss does not improve after\n",
        "    certain epochs.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        \"\"\"\n",
        "        :param patience: how many epochs to wait before stopping when loss is\n",
        "               not improving\n",
        "        :param min_delta: minimum difference between new loss and old loss for\n",
        "               new loss to be considered as an improvement\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss == None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss < self.min_delta:\n",
        "            self.counter += 1\n",
        "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                print(f'INFO: Early stopping')\n",
        "                self.early_stop = True"
      ],
      "metadata": {
        "id": "p9YG1JblSUzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xlq48WjiW6U"
      },
      "source": [
        "#### **Definimos el entrenamiento de la red**\n",
        "\n",
        "Algunos conceptos previos: \n",
        "\n",
        "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
        "- `batch`: una fracción de la época. Se utilizan para entrenar mas rápidamente la red. (mas eficiente pasar n datos que uno en cada ejecución del backpropagation)\n",
        "\n",
        "Esta función está encargada de entrenar la red en una época. Para esto, por cada batch de la época actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\n",
        "\n",
        "Observación: En algunos comentarios aparecerá el tamaño de los tensores entre corchetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV6YLt0oiicW"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la época:\n",
        "    for batch in iterator:\n",
        "\n",
        "        # Extraemos el texto y los tags del batch que estamos procesado\n",
        "        text = batch.text\n",
        "        tags = batch.nertags\n",
        "\n",
        "        # Reiniciamos los gradientes calculados en la iteración anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "\n",
        "\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "        \n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los parámetros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las métricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYNcwKnAz5Hf"
      },
      "source": [
        "#### **Definimos la función de evaluación**\n",
        "\n",
        "Evalua el rendimiento actual de la red usando los datos de validación. \n",
        "\n",
        "Por cada batch de estos datos, calcula y reporta el loss y las métricas asociadas al conjunto de validación. \n",
        "Ya que las métricas son calculadas por cada batch, estas son retornadas promediadas por el número de batches entregados. (ver linea del return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsRuiUuHiicY"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.nertags\n",
        "\n",
        "            # Predecimos\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las métricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las métricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs-n9Y5yiica"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy3MVf5H0A94"
      },
      "source": [
        "\n",
        "#### **Entrenamiento de la red**\n",
        "\n",
        "En este cuadro de código ejecutaremos el entrenamiento de la red.\n",
        "Para esto, primero definiremos el número de épocas y luego por cada época, ejecutaremos `train` y `evaluate`.\n",
        "\n",
        "**Importante: Reiniciar los pesos del modelo**\n",
        "\n",
        "Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez. \n",
        "Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la función `init_weights`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK5lQqpviicf"
      },
      "outputs": [],
      "source": [
        "best_valid_loss = float('inf')\n",
        "early_stopping = EarlyStopping(patience=3)\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model, train_iterator, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validación)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    scheduler.step(valid_loss)\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )\n",
        "    early_stopping(valid_loss)\n",
        "    if early_stopping.early_stop:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcZPraG-9duO"
      },
      "source": [
        "**Importante**: Recuerden que el último modelo entrenado no es el mejor (probablemente esté *overfitteado*), si no el que guardamos con la menor loss del conjunto de validación. Este problema lo pueden solucionar con *early stopping*.\n",
        "Para cargar el mejor modelo entrenado, ejecuten la siguiente celda.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y27CNYfrjtQ-"
      },
      "outputs": [],
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model.load_state_dict(torch.load('{}.pt'.format(model_name)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLuqFKFR9duO"
      },
      "outputs": [],
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "metadata": {
        "id": "GzjsUwYfIXWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBctQHTh0lxD"
      },
      "source": [
        "#### **Evaluamos el set de validación con el modelo final**\n",
        "\n",
        "Estos son los resultados de predecir el dataset de evaluación con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0gVbP8yiicj"
      },
      "outputs": [],
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF1ysw_Kw6zz"
      },
      "source": [
        "### **Predecir datos para la competencia**\n",
        "\n",
        "Ahora, a partir de los datos de **test** y nuestro modelo entrenado, vamos a predecir las etiquetas que serán evaluadas en la competencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RBs3UU4wLk3"
      },
      "outputs": [],
      "source": [
        "def predict_labels(model, iterator, criterion, fields=fields):\n",
        "\n",
        "    # Extraemos los vocabularios.\n",
        "    text_field = fields[0][1]\n",
        "    nertags_field = fields[1][1]\n",
        "    tags_vocab = nertags_field.vocab.itos\n",
        "    words_vocab = text_field.vocab.itos\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            text_batch = batch.text\n",
        "            text_batch = torch.transpose(text_batch, 0, 1).tolist()\n",
        "\n",
        "            # Predecir los tags de las sentences del batch\n",
        "            predictions_batch = model(batch.text)\n",
        "            predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
        "\n",
        "            # por cada oración predicha:\n",
        "            for sentence, sentence_prediction in zip(text_batch,\n",
        "                                                     predictions_batch):\n",
        "                for word_idx, word_predictions in zip(sentence,\n",
        "                                                      sentence_prediction):\n",
        "                    # Obtener el indice del tag con la probabilidad mas alta.\n",
        "                    argmax_index = word_predictions.topk(1)[1]\n",
        "\n",
        "                    current_tag = tags_vocab[argmax_index]\n",
        "                    # Obtenemos la palabra\n",
        "                    current_word = words_vocab[word_idx]\n",
        "\n",
        "                    if current_word != '<pad>':\n",
        "                        predictions.append([current_word, current_tag])\n",
        "                predictions.append(['EOS', 'EOS'])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict_labels(model, test_iterator, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "pcmEqmemHqJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwQp1Ru8Oht8"
      },
      "source": [
        "### **Generar el archivo para la submission**\n",
        "\n",
        "No hay problema si aparecen unk en la salida. Estos no son relevantes para evaluarlos, usamos solo los tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPfZkjJGkWyq"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "if (os.path.isfile('./predictions.zip')):\n",
        "    os.remove('./predictions.zip')\n",
        "\n",
        "if (not os.path.isdir('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "f = open('predictions/predictions.txt', 'w')\n",
        "for i, (word, tag) in enumerate(predictions[:-1]):\n",
        "    if word=='EOS' and tag=='EOS': f.write('\\n')\n",
        "    else: \n",
        "      if i == len(predictions[:-1])-1:\n",
        "        f.write(word + ' ' + tag)\n",
        "      else: f.write(word + ' ' + tag + '\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAtK7y43V7Z_"
      },
      "source": [
        "## **Resultados**\n",
        "\n",
        "A continuación se muestra una tabla resumen de los 8 modelos mostrados a lo largo de este trabajo:\n",
        "\n",
        "\n",
        "| Model | Approach                 |Red| type    |Precision| Recall| F1Score  |\n",
        "|-----|----------------------------||-------------|---------|-------|----------|\n",
        "| 1   | flair-clinical forward | lstm | Train     |     0.85    | 0.87      |  0.83\n",
        "| | | | Val|  0.74| 0.76|0.72| \n",
        "2   | flair-clinical forward y backward + Glove| lstm | Train     |     0.86    | 0.88      |  0.85\n",
        "| | | | Val|  0.74| 0.76|0.72|\n",
        "| 3   | flair-clinical forward y backward + Glove| Gru | Train     |     0.66    | 0.73     |  061\n",
        "| | | | Val|  0.65| 0.65|0.66|\n",
        "| 4   | flair-clinical forward y backward + Glove| RNN tanh | Train     |     0.22    | 0.47      |  0.15\n",
        "| | | | Val|  0.25| 0.35|0.19|\n",
        "| 5   | Varir capas RNN, drop out | lstm | Train     |   0.75     |  0.74    |  0.74\n",
        "| | | | Val|  |  |  |\n",
        "| 6   | Capa de atención | lstm | Train     | 0.75       |   0.73    |  0.74\n",
        "| | | | Val|  | | |\n",
        "| 7   | Variar capas RNN, drop out| Gru | Train     |    0.77     | 0.69    |  0.73\n",
        "| | | | Val| | ||\n",
        "| 8   | Capa de atención| Gru | Train     |   0.75     | 0.71     | 0.73\n",
        "| | | |   | ||\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZEWJXrNaSIf"
      },
      "source": [
        "## **Conclusiones**\n",
        "\n",
        "El modelo que obtuvo mejores resultados fué el segundo, mostrando que la combinación de los embeddings de Flair, más WordEmbedding GloVe y con una red LSTM presentan el mejor setup para el modelo. Sin embargo, pese a que en este trabajo se obtuvieron valores sobre 0.85, en CodeLab este modelo obtuvo un máximo de 0.62 en todas sus métricas. Lo anterior podría deberse a que no se probaron tantos modelos ocupando el setup anterior, y quizá sea mejor encontrar buenas combinaciones de cantidad de capas y dimensiones ocultas que obtengan los mejores resultados.\n",
        "\n",
        "Por otro lado, los peores modelos son los que juntaron todos los embeddings descritos pero con redes de tipo GRU o RNN con tanh. Lo anterior indica que si bien los embeddings pueden presentar muy buenos resultados, estos varían enormemente dependiendo del tipo de red en el que se entrenen.\n",
        "\n",
        "En cuanto a los modelos que probaron variaciones de parámetros y de tipos de red, se observó que los resultados de todos estos no variaron mucho, lo que podría indicar que a mayor cantidad de capas o de dimensiones ocultas no se obtienen necesariamente mejores resultados. Por otro lado, el cambiar la loss function por ReLU no presentó mayores diferencias entre los últimos modelos.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LMgKjfYC_Go-"
      ],
      "name": "Competencia2_VANICCI_CC6205.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}