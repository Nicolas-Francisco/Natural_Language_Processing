{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0tyIsliieNr"
      },
      "source": [
        "# **Competencia 2 - CC6205 Natural Language Processing üìö**\n",
        "\n",
        "Integrantes: \n",
        "\n",
        "\n",
        "*   Valentina Canales\n",
        "*   Nicol√°s Garc√≠a\n",
        "*   Ricardo Valdivia \n",
        "\n",
        "\n",
        "\n",
        "Usuario del equipo en CodaLab (Obligatorio): VANICCI\n",
        "\n",
        "Fecha l√≠mite de entrega üìÜ: 29 de Junio.\n",
        "\n",
        "Tiempo estimado de dedicaci√≥n: 5 horas\n",
        "\n",
        "Link competencia: Poner el link [aqu√≠](https://codalab.lisn.upsaclay.fr/competitions/5098?secret_key=09955d45-6210-4a35-a171-8050aa050855#learn_the_details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQlYlmGaSFH"
      },
      "source": [
        "## **Introducci√≥n**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVL0-01AUOzL"
      },
      "source": [
        "En el presente trabajo se busca encontrar un modelo de RNN que permita resolver la tarea **NER** para el conjunto de datos chilenos de **interconsultas de lista de espera NO GES en Chile**. Este corpus Chileno estar√° constituido por 5 tipos de entidades, las que corresponden a:\n",
        "- **Disease**\n",
        "- **Body_Part**\n",
        "- **Medication** \n",
        "- **Procedures** \n",
        "- **Family_Member**\n",
        "\n",
        "Cada modelo creado en el desarrollo de este trabajo est√°n basados en los modelos vistos en clases y explicados de mejor manera en clases auxiliares. Los modelos implimentados corresponden a RNNs de tipo **LSMT** y **GRU**, adem√°s de incluir embeddings en espa√±ol como *es-clinical* de la libreria Flair.\n",
        "\n",
        "Los resultados obtenidos en la competencia concluyeron que el modelo que entrega mejores resultados fue ..., obteniendo un **F1-Score promedio de 0,86.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbA1EmhCaSFI"
      },
      "source": [
        "## **Modelos**\n",
        "A continuaci√≥n, se mostrar√° un resumen de los hiperpar√°metros y embeddings usados en cada uno de los ocho modelos entrenados:\n",
        "\n",
        "### **Modelo 1**\n",
        "* BiLSTM + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-forward\n",
        "* Hiperpar√°metros: \n",
        "  * hidden dimmension = 256\n",
        "  * number of layers = 5 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n",
        "\n",
        "### **Modelo 2**\n",
        "* BiLSTM + GloVe + Embedding : \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperpar√°metros: \n",
        "  * hidden dimmension = 256\n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n",
        "\n",
        "### **Modelo 3**\n",
        "* BiGRU + GloVe + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperpar√°metros: \n",
        "  * hidden dimmension = 1024 \n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n",
        "\n",
        "### **Modelo 4**\n",
        "* BiRNN + GloVe + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperpar√°metros: \n",
        "  * hidden dimmension = 1024 \n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n",
        "\n",
        "### **Modelo 5**\n",
        "* BiLSTM\n",
        "* Hiperpar√°metros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 64 \n",
        "  * number of layers = 2\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n",
        "\n",
        "### **Modelo 6**\n",
        "* BiLSTM + ReLU + Attention layer\n",
        "* Hiperpar√°metros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 3\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n",
        "\n",
        "### **Modelo 7**\n",
        "* BiGRU\n",
        "* Hiperpar√°metros: \n",
        "  * embedding dimmension = 100\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 2\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n",
        "\n",
        "### **Modelo 8**\n",
        "* BiGRU + ReLU + Attention Layer\n",
        "* Hiperpar√°metros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 3\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaVhZ5iaaSFK"
      },
      "source": [
        "## **M√©tricas de evaluaci√≥n**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXl3GaVMUYA7"
      },
      "source": [
        "- **M√©trica estricta:** Para evaluar el rendimiento de la red se utiliz√≥ una metrica estricta para las tres medidas: *precision*, *recall* y *f1-score*. Esta m√©trica s√≥lo considera correcta una predicci√≥n del modelo, s√≥lo si al compararlo con las entidades reales coinciden tanto los l√≠mites de la entidad como el tipo.\n",
        "\n",
        "- **Precision:** La precisi√≥n es la relaci√≥n entre las observaciones positivas predichas correctamente y el total de observaciones positivas predichas.\n",
        "Consideraremos precision sobre 0.5 como resultados buenos para el modelo.\n",
        "\n",
        "  $$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}} $$\n",
        "\n",
        "- **Recall:** Entrega el resultado de la raz√≥n entre los valores positivos acertados y las suma entre todos los valores positivos de la muestra (acertados y no acertados). En otras palabras, es el valor que se le da al resultado con respecto a la detecci√≥n de los valores positivos de toda la muestra.\n",
        "\n",
        "  $$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}} $$\n",
        "\n",
        "- **Micro F1 score:** Representa la media arm√≥nica de *precision* y *recall*. Esta m√©trica ayuda a diferenciar el desempe√±o de un modelo en especifico considerando *precision* y *recall*, de esta manera tendremos un n√∫mero que nos proporcionar√° mayor informaci√≥n para saber si un modelo es mejor que otro tomando en cuenta las 2 m√©tricas anteriores. Se utiliza *Micro F1 Score* dado que los valores de cada tag no est√°n en la misma proporcion, por lo que se debe considerar el *F1 Score* del modelo en su totalidad y no individualmente (lo que se realiza en el caso de *Macro F1 Score*):\n",
        "\n",
        "  $$\\text{F1 Score} = \\frac{2 \\times \\text{Recall} \\times \\text{Precision}}{\\text{Recall}+\\text{Precision}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27WffRVUj4v"
      },
      "source": [
        "## **Dise√±o experimental**\n",
        "\n",
        "Los primeros 4 modelos entrenados tienen el fin de emplear Flair Embedding bajo distintos escenarios, junto con bidireccionalidad, un dropout de 0.5 y epochs iguales a 15. Por otro lado, en los √∫ltimos 4 modelos se busc√≥ variar de distintas maneras los hiperpar√°metros como la cantidad de capas RNN, variar el valor del dropout, la loss function (por ReLU) y a√±adiendo una capa de atenci√≥n. A continuaci√≥n, se describir√° con m√°s detalle el dise√±o de los modelos y los experimentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O228DbeUmE7"
      },
      "source": [
        "### **Modelo 1**\n",
        "\n",
        "En este primer modelo se decidi√≥ implementar una red LSTM empleando Flair Embedding con `'es-clinical-forward'`, bidireccionalidad, dropout de 0.5 y 5 capas. Adem√°s, se decidi√≥ variar la cantidad de dimensiones ocultas hasta encontrar los mejores resultados en este escenario. As√≠, se encontr√≥ que con 256 dimensiones ocultas de las capas de LSTM se logr√≥ obtener un presicion de 0.85, un recall de\t0.87, y finalmente un f1-score de **0.83**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDvbvKkBJ9XW"
      },
      "source": [
        "### **Modelo 2**\n",
        "En el segundo modelo se decidi√≥ emprear Flair Embedding pero ahora con `'es-clinical-forward'`, `'es-clinical-backward'` y `'GloVe' WordEmbedding` a la vez. Como en el modelo anterior, se mantuvo la red LSTM, tambi√©n se decidi√≥ mantener bidireccionalidad, el dropout y el valor de epoch, mientras que para reducir el tiempo de ejecuci√≥n se decidi√≥ reducir la cantidad de capas de 5 a 3. En este caso, utilizando 256 capas ocultas, se obtuvieron los mejores resultados de este estudio, obteniendo unas m√©tricas de presiction igual a 0.86, recall de 0.88 y un **F1-Score de 0.85**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmSiFGEmJ_bB"
      },
      "source": [
        "### **Modelo 3**\n",
        "En este tercer modelo nuevamente se ocuparon los dos embeddings de Flair y el embedding Glove, pero utilizando esta vez una red GRU y aumentando la cantidad de dimensiones ocultas con el fin de verificar si esta mejora se ve reflejada en los resultados del modelo. Sin embargo, lo anterior fu√© desmentido en los resultados de prueba, donde se obtuvieron peores resultados que con solo 256 dimensiones: presiction de 0.66, recall de 0.73, y un F1-Score de 0.61.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUcpM9rvJ_pv"
      },
      "source": [
        "### **Modelo 4**\n",
        "En este √∫ltimo modelo con embeddings, se decidi√≥ modificar la red ocupada desde el modelo anterior a una red RNN con tanh y manteniendo los mismos hiperpar√°metros. Los resultados de este √∫ltimo modelo fueron a√∫n peores, obteniendo un presicion de 0.22, un recall de 0.47 y un **F1-Score de 0.15**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URv3KgfHJ_xJ"
      },
      "source": [
        "### **Modelo 5**\n",
        "Como se dijo anteriormente, en los √∫ltimos 4 modelos entrenados se decidi√≥ ver el comportamiento del modelo variando los hiperpar√°metros de dimensiones de embeddings, dimensiones ocultas, el n√∫mero de capas y el tipo de red.\n",
        "Para este quinto modelo, se decidi√≥ utilizar una red de tipo LSTM con 200 dimensiones de embedding y 64 dimensiones ocultas. Tambi√©n se eligi√≥ un modelo con solo 2 capas y un epoch de 10.\n",
        "Los resultados de este modelo fueron mucho mejores que ocupando los embeddings de Flair con una red tipo GRU o RNN, obteniendo as√≠ un presicion de 0.75, un recall de 0.74 y un F1-Score de 0.74.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwkUikD9J_3w"
      },
      "source": [
        "### **Modelo 6**\n",
        "Para este sexto modelo se decidi√≥ mantener el tipo de red, pero cambiando la loss function por ReLU y a√±adiendo una capa de atenci√≥n. Adem√°s, se aument√≥ tanto la cantidad de dimensiones ocultas y el n√∫mero de capas. \n",
        "Los resultados obtenidos son tan buenos como los anteriores, obteniendo un presicion de 0.75, un recall de 0.73 y un F1-Score de 0.74."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCO4XBTZKAK2"
      },
      "source": [
        "### **Modelo 7**\n",
        "En este modelo se decidi√≥ mantener el setup del modelo n√∫mero 5, pero aumentando la cantidad de dimensiones ocultas y cambiando el tipo de red por GRU. Los resultados obtenidos siguen sin muchas variaciones, obteniendo un presicion de 0.77, recall de 0.69 y un F1-Score de 0.73.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I_Vb__GKAP_"
      },
      "source": [
        "### **Modelo 8**\n",
        "Manteniendo la idea del modelo anterior, se decidi√≥ repetir el setup del sexto modelo, pero cambiando el tipo de red por una de tipo GRU y a√±adiendo una capa de atenci√≥n. Este √∫ltimo modelo mantuvo el rendimiento mostrado en los √∫ltimos 4, obteniendo un presicion de 0.75, un recall de 0.71, y un F1-Score de 0.73."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFM-wNt8aSFM"
      },
      "source": [
        "## **Experimentos**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMgKjfYC_Go-"
      },
      "source": [
        "###  **Carga de datos y Preprocesamiento**\n",
        "\n",
        "Para cargar los datos y preprocesarlos usaremos la librer√≠a [`torchtext`](https://github.com/pytorch/text). Tener cuidado ya que hace algunos meses esta librer√≠a tuvo cambios radicales, quedando las funcionalidades pasadas en un nuevo paquete llamado legacy. Esto ya que si quieren usar m√°s funciones de la librer√≠a entonces vean los cambios en la documentaci√≥n.\n",
        "\n",
        "En particular usaremos su m√≥dulo `data`, el cual seg√∫n su documentaci√≥n original provee: \n",
        "\n",
        "    - Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format\n",
        "    - Ability to define a preprocessing pipeline\n",
        "    - Batching, padding, and numericalizing (including building a vocabulary object)\n",
        "    - Wrapper for dataset splits (train, validation, test)\n",
        "\n",
        "\n",
        "El proceso ser√° el siguiente: \n",
        "\n",
        "1. Descargar los datos desde github y examinarlos.\n",
        "2. Definir los campos (`fields`) que cargaremos desde los archivos.\n",
        "3. Cargar los datasets.\n",
        "4. Crear el vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27csY87GaSFO",
        "scrolled": false,
        "outputId": "8db730d6-2de8-4100-bbe2-d523e0fa155c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# Instalamos torchtext que nos facilitar√° la vida en el pre-procesamiento del formato ConLL.\n",
        "!pip install -U torchtext==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng7wRGEyawjM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext import data, datasets, legacy\n",
        "\n",
        "\n",
        "# Garantizar reproducibilidad de los experimentos\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BehSou6rCvwg"
      },
      "source": [
        "#### **Obtener datos**\n",
        "\n",
        "Descargamos los datos de entrenamiento, validaci√≥n y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbT0g_kC18Jb",
        "outputId": "e752fa10-b6f2-4577-89fe-02eb238e4f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‚Äòtrain.txt‚Äô already there; not retrieving.\n",
            "\n",
            "File ‚Äòdev.txt‚Äô already there; not retrieving.\n",
            "\n",
            "File ‚Äòtest.txt‚Äô already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#%%capture\n",
        "\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc # Dataset de Entrenamiento\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc    # Dataset de Validaci√≥n (Para probar y ajustar el modelo)\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. ¬°¬°SON LOS QUE DEBEN SER PREDICHOS!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMud7YGMBZvg"
      },
      "source": [
        "####  **Fields**\n",
        "\n",
        "Un `field`:\n",
        "\n",
        "* Define un tipo de datos junto con instrucciones para convertir el texto a Tensor.\n",
        "* Contiene un objeto `Vocab` que contiene el vocabulario (palabras posibles que puede tomar ese campo).\n",
        "* Contiene otros par√°metros relacionados con la forma en que se debe numericalizar un tipo de datos, como un m√©todo de tokenizaci√≥n y el tipo de Tensor que se debe producir.\n",
        "\n",
        "\n",
        "Analizemos el siguiente cuadro el cual contiene un ejemplo cualquiera de entrenamiento:\n",
        "\n",
        "\n",
        "```\n",
        "El O\n",
        "paciente O\n",
        "padece O\n",
        "de O\n",
        "cancer B-Disease\n",
        "de I-Disease\n",
        "colon I-Disease\n",
        ". O\n",
        "```\n",
        "\n",
        "Cada linea contiene un token y el tipo de entidad asociado en el formato IOB2 ya explicado. Para que `torchtext` pueda cargar estos datos, debemos definir como va a leer y separar los componentes de cada una de las lineas.\n",
        "Para esto, definiremos un field para cada uno de esos componentes: Las palabras (`TEXT`) y las etiquetas o categor√≠as (`NER_TAGS`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DcM_IjgCdzz"
      },
      "outputs": [],
      "source": [
        "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
        "TEXT = legacy.data.Field(lower=False) \n",
        "\n",
        "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
        "NER_TAGS = legacy.data.Field(unk_token=None)\n",
        "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZPxDaaKgeqI",
        "outputId": "1e366483-2e0f-422c-d7f9-ae0ef895f05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('text', <torchtext.legacy.data.field.Field at 0x7faf17e01950>),\n",
              " ('nertags', <torchtext.legacy.data.field.Field at 0x7faf17e01b10>))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCKTJOdgC5eC"
      },
      "source": [
        "####  **SequenceTaggingDataset**\n",
        "\n",
        "`SequenceTaggingDataset` es una clase de torchtext dise√±ada para contener datasets de sequence labeling. Los ejemplos que se guarden en una instancia de estos ser√°n arreglos de palabras asociados con sus respectivos tags.\n",
        "\n",
        "Por ejemplo, para Part-of-speech tagging:\n",
        "\n",
        "[I, love, PyTorch, .] estar√° asociado con [PRON, VERB, PROPN, PUNCT]\n",
        "\n",
        "\n",
        "La idea es que usando los fields que definimos antes, le indiquemos a la clase c√≥mo cargar los datasets de prueba, validaci√≥n y test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsHdGml62J21"
      },
      "outputs": [],
      "source": [
        "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train.txt\",\n",
        "    validation=\"dev.txt\",\n",
        "    test=\"test.txt\",\n",
        "    fields=fields,\n",
        "    encoding=\"utf-8\",\n",
        "    separator=\" \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2qf7YTWHEYR",
        "outputId": "4c061009-c2d2-49c5-fa56-af92623262f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.legacy.datasets.sequence_tagging.SequenceTaggingDataset at 0x7faf17e09fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu7q3HCliia5",
        "outputId": "496b5081-a757-4509-bcb3-1825677d80f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de ejemplos de entrenamiento: 8025\n",
            "N√∫mero de ejemplos de validaci√≥n: 891\n",
            "N√∫mero de ejemplos de test (competencia): 992\n"
          ]
        }
      ],
      "source": [
        "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
        "print(f\"N√∫mero de ejemplos de validaci√≥n: {len(valid_data)}\")\n",
        "print(f\"N√∫mero de ejemplos de test (competencia): {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDRnhXAdFGL-"
      },
      "source": [
        "Visualizemos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T023Ld4RaSF4",
        "outputId": "5d2f4012-c6e5-45c4-919a-ec02e4614544",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('consulto', 'O'),\n",
              " ('en', 'O'),\n",
              " ('SU', 'O'),\n",
              " ('el', 'O'),\n",
              " ('9', 'O'),\n",
              " ('-', 'O'),\n",
              " ('11', 'O'),\n",
              " ('-', 'O'),\n",
              " ('15', 'O'),\n",
              " ('por', 'O'),\n",
              " ('dolor', 'O'),\n",
              " ('precorddial', 'O'),\n",
              " ('irradiado', 'O'),\n",
              " ('e', 'O'),\n",
              " ('brazo', 'B-Body_Part'),\n",
              " ('izq', 'I-Body_Part'),\n",
              " ('.', 'I-Body_Part'),\n",
              " (',', 'O'),\n",
              " ('adem√°s', 'O'),\n",
              " ('singulto', 'O'),\n",
              " ('.', 'O')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import random\n",
        "random_item_idx = random.randint(0, len(train_data))\n",
        "random_example = train_data.examples[random_item_idx]\n",
        "list(zip(random_example.text, random_example.nertags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l05KYy5FSUy"
      },
      "source": [
        "#### **Construir los vocabularios para el texto y las etiquetas**\n",
        "\n",
        "Los vocabularios son los objetos que contienen todos los tokens (de entrenamiento) posibles para ambos fields. El siguiente paso consiste en construirlos. Para esto, hacemos uso del m√©todo `Field.build_vocab` sobre cada uno de nuestros `fields`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBhp7WICiibL"
      },
      "outputs": [],
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "NER_TAGS.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4OgUKM_iibO",
        "outputId": "021f652a-68b4-4d62-f8fd-36c010079aa2",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens √∫nicos en TEXT: 17591\n",
            "Tokens √∫nicos en NER_TAGS: 12\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tokens √∫nicos en TEXT: {len(TEXT.vocab)}\")\n",
        "print(f\"Tokens √∫nicos en NER_TAGS: {len(NER_TAGS.vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4FeyL9nFnId",
        "outputId": "5049dcf4-eec0-49f6-e44f-ee3d7fd152b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " 'O',\n",
              " 'I-Disease',\n",
              " 'B-Disease',\n",
              " 'I-Body_Part',\n",
              " 'B-Body_Part',\n",
              " 'B-Procedure',\n",
              " 'I-Procedure',\n",
              " 'B-Medication',\n",
              " 'B-Family_Member',\n",
              " 'I-Medication',\n",
              " 'I-Family_Member']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Veamos las posibles etiquetas que hemos cargado:\n",
        "NER_TAGS.vocab.itos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQDoUqSHFKj"
      },
      "source": [
        "Observen que ademas de los tags NER, tenemos \\<pad\\>, el cual es generado por el dataloader para cumplir con el padding de cada oraci√≥n.\n",
        "\n",
        "Veamos ahora los tokens mas frecuentes y especiales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5eSLm4diibR",
        "outputId": "54b3b3a2-5888-46ed-b31d-08828d03a744"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 7396),\n",
              " (',', 6821),\n",
              " ('-', 4985),\n",
              " ('de', 3811),\n",
              " ('DE', 3645),\n",
              " ('/', 2317),\n",
              " (':', 2209),\n",
              " ('con', 1484),\n",
              " ('y', 1439),\n",
              " ('APS', 1429)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Tokens mas frecuentes (Ser√° necesario usar stopwords, eliminar s√≠mbolos o nos entregan informaci√≥n (?) )\n",
        "TEXT.vocab.freqs.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDyNLMPz9duD"
      },
      "outputs": [],
      "source": [
        "# Seteamos algunas variables que nos ser√°n de utilidad mas adelante...\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "O_TAG_IDX = NER_TAGS.vocab.stoi['O']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrYvF3X0sjWL"
      },
      "source": [
        "#### **Frecuencia de los Tags**\n",
        "\n",
        "Visualizemos r√°pidamente las cantidades y frecuencias de cada tag:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuXOsbJUiibh",
        "outputId": "673dde73-806d-4618-f038-96608c9d8283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag Ocurrencia Porcentaje\n",
            "\n",
            "O\t101671\t68.1%\n",
            "I-Disease\t21629\t14.5%\n",
            "B-Disease\t8831\t 5.9%\n",
            "I-Body_Part\t6489\t 4.3%\n",
            "B-Body_Part\t3755\t 2.5%\n",
            "B-Procedure\t2891\t 1.9%\n",
            "I-Procedure\t2819\t 1.9%\n",
            "B-Medication\t784\t 0.5%\n",
            "B-Family_Member\t228\t 0.2%\n",
            "I-Medication\t116\t 0.1%\n",
            "I-Family_Member\t9\t 0.0%\n"
          ]
        }
      ],
      "source": [
        "def tag_percentage(tag_counts):\n",
        "    \n",
        "    total_count = sum([count for tag, count in tag_counts])\n",
        "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
        "  \n",
        "    return tag_counts_percentages\n",
        "\n",
        "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
        "\n",
        "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
        "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4wPiydnaSGs"
      },
      "source": [
        "#### **Configuramos pytorch y dividimos los datos.**\n",
        "\n",
        "Importante: si tienes problemas con la ram de la gpu, disminuye el tama√±o de los batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB7cwLWpaSGs",
        "outputId": "b2f84c06-8546-42e8-8566-14bca2c076c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 22  # disminuir si hay problemas de ram.\n",
        "\n",
        "# Usar cuda si es que est√° disponible.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "# Dividir datos entre entrenamiento y test. Si van a hacer alg√∫n sort no puede ser sobre\n",
        "# el conjunto de testing ya que al hacer sus predicciones sobre el conjunto de test sin etiquetas\n",
        "# debe conservar el orden original para ser comparado con los golden_labels. \n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B21E1eAFId16"
      },
      "source": [
        "#### **M√©tricas de evaluaci√≥n**\n",
        "\n",
        "Adem√°s, definiremos las m√©tricas que ser√°n usadas tanto para la competencia como para evaluar el modelo: `precision`, `recall` y `micro f1-score`.\n",
        "**Importante**: Noten que la evaluaci√≥n solo se hace para las Named Entities (sin contar 'O'), toda esta funcionalidad nos la entrega la librer√≠a seqeval, pueden revisar m√°s documentaci√≥n aqu√≠: https://github.com/chakki-works/seqeval. No utilicen el c√≥digo entregado por sklearn para calcular las m√©tricas ya que esta lo hace a nivel de token y no a nivel de entidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o63ov69_rX2T",
        "outputId": "a01971ec-466b-41ce-8da6-729419e2a8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mUOOLEWiicU"
      },
      "outputs": [],
      "source": [
        "# Definimos las m√©tricas\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "\n",
        "    # filtramos <pad> para calcular los scores.\n",
        "    mask = [(y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    y_pred = [[NER_TAGS.vocab.itos[v] for v in y_pred]]\n",
        "    y_true = [[NER_TAGS.vocab.itos[v] for v in y_true]]\n",
        "    \n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
        "    precision = precision_score(y_true, y_pred, mode='strict')\n",
        "    recall = recall_score(y_true, y_pred, mode='strict')\n",
        "\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hod516H1aSG2"
      },
      "source": [
        "-------------------\n",
        "\n",
        "### **Modelo Baseline**\n",
        "\n",
        "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendr√° una capa de embedding, unas cuantas LSTM y una capa de salida y usar√° dropout en el entrenamiento.\n",
        "\n",
        "Este constar√° de los siguientes pasos: \n",
        "\n",
        "1. Definir la clase que contendr√° la red.\n",
        "2. Definir los hiperpar√°metros e inicializar la red. \n",
        "3. Definir el n√∫mero de √©pocas de entrenamiento\n",
        "4. Definir la funci√≥n de loss.\n",
        "\n",
        "\n",
        "\n",
        "Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMPL08XqaSG3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx,\n",
        "                                      )\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCl3530VaSG7"
      },
      "source": [
        "#### **Hiperpar√°metros de la red**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHdi3QdOaSG8"
      },
      "outputs": [],
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS = 3  # n√∫mero de capas.\n",
        "DROPOUT = 0.6\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "baseline_model_name = 'baseline'  # nombre que tendr√° el modelo guardado..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlF1DhJeaSHA"
      },
      "outputs": [],
      "source": [
        "baseline_n_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3u4imJGaSHE"
      },
      "source": [
        "#### Definimos la funci√≥n de loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G_4k99_aSHG"
      },
      "outputs": [],
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRYOEDiQaSHK"
      },
      "source": [
        "--------------------\n",
        "### Modelo 1\n",
        "\n",
        "* BiLSTM + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-forward\n",
        "* Hiperpar√°metros: \n",
        "  * hidden dimmension = 256\n",
        "  * number of layers = 5 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"git+https://github.com/flairNLP/flair\""
      ],
      "metadata": {
        "id": "24xdc50SCv1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "clinical_emb = FlairEmbeddings('es-clinical-forward')\n",
        "\n",
        "\n",
        "\n",
        "matrix_len = len(TEXT.vocab)\n",
        "EMBEDDINGS = torch.zeros((matrix_len, 2048))\n",
        "words_found = 0\n",
        "for i, word in enumerate(TEXT.vocab.itos):\n",
        "    try: \n",
        "        sw = Sentence(word)\n",
        "        clinical_emb.embed(sw)\n",
        "        for token in sw:\n",
        "          EMBEDDINGS[i] = token.embedding\n",
        "    except KeyError:\n",
        "        EMBEDDINGS[i] = torch.zeros((2048,1))"
      ],
      "metadata": {
        "id": "U3lo6cmhRme3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding, freeze=True, padding_idx=pad_idx)\n",
        "        self.embedding_dim = embedding.size()[1]\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(self.embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(self.dropout(embedded))\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "1kt8_ihoRn94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "EMBEDDING_DIM = EMBEDDINGS.size()[1]\n",
        "N_LAYERS = 5  # n√∫mero de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model_1 = NER_RNN(INPUT_DIM, EMBEDDINGS, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model_1_name = 'model_1'\n",
        "n_epochs_1 = 15"
      ],
      "metadata": {
        "id": "Lpkt_fK9Rqg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV9oLkN1aSHO"
      },
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 2\n",
        "\n",
        "* BiLSTM + GloVe + Embedding : \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperpar√°metros: \n",
        "  * hidden dimmension = 256\n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWPzETaNaSHP"
      },
      "outputs": [],
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import FlairEmbeddings, WordEmbeddings, StackedEmbeddings\n",
        "\n",
        "embedding_types = [\n",
        "                    FlairEmbeddings('es-clinical-forward'),\n",
        "                    FlairEmbeddings('es-clinical-backward'),\n",
        "                    WordEmbeddings('glove')\n",
        "]\n",
        "EMBEDDINGS_form = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "matrix_len = len(TEXT.vocab)\n",
        "EMBEDDINGS = torch.zeros((matrix_len, 4196))\n",
        "words_found = 0\n",
        "for i, word in enumerate(TEXT.vocab.itos):\n",
        "    try: \n",
        "        sw = Sentence(word)\n",
        "        EMBEDDINGS_form.embed(sw)\n",
        "        for token in sw:\n",
        "          EMBEDDINGS[i] = token.embedding\n",
        "    except KeyError:\n",
        "        EMBEDDINGS[i] = torch.zeros((4196,1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding, freeze=True, padding_idx=pad_idx)\n",
        "        self.embedding_dim = embedding.size()[1]\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(self.embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(self.dropout(embedded))\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "VmEWDiCUC0xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "HIDDEN_DIM = 256  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "EMBEDDING_DIM = EMBEDDINGS.size()[1]\n",
        "N_LAYERS = 3  # n√∫mero de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model_2 = NER_RNN(INPUT_DIM, EMBEDDINGS, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model_2_name = 'model_2'\n",
        "n_epochs_2 = 15"
      ],
      "metadata": {
        "id": "1wcfjqtzC3OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpy3p7YaaSHT"
      },
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 3\n",
        "\n",
        "* BiGRU + GloVe + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperpar√°metros: \n",
        "  * hidden dimmension = 1024 \n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w0CFjA8aSHU"
      },
      "outputs": [],
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import FlairEmbeddings, WordEmbeddings, StackedEmbeddings\n",
        "\n",
        "embedding_types = [\n",
        "                    FlairEmbeddings('es-clinical-forward'),\n",
        "                    FlairEmbeddings('es-clinical-backward'),\n",
        "                    WordEmbeddings('glove')\n",
        "]\n",
        "EMBEDDINGS_form = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "matrix_len = len(TEXT.vocab)\n",
        "EMBEDDINGS = torch.zeros((matrix_len, 4196))\n",
        "words_found = 0\n",
        "for i, word in enumerate(TEXT.vocab.itos):\n",
        "    try: \n",
        "        sw = Sentence(word)\n",
        "        EMBEDDINGS_form.embed(sw)\n",
        "        for token in sw:\n",
        "          EMBEDDINGS[i] = token.embedding\n",
        "    except KeyError:\n",
        "        EMBEDDINGS[i] = torch.zeros((4196,1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "\n",
        "# Definir la red\n",
        "class GRU_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding, freeze=True, padding_idx=pad_idx)\n",
        "        self.embedding_dim = embedding.size()[1]\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.GRU(self.embedding_dim, hidden_dim, num_layers=n_layers,\n",
        "                             bidirectional=bidirectional, dropout=dropout)\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        outputs, hidden = self.lstm(self.dropout(embedded))\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "Nah90_g6SCsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "HIDDEN_DIM = 1024  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "EMBEDDING_DIM = EMBEDDINGS.size()[1]\n",
        "N_LAYERS = 3  # n√∫mero de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model_3 = GRU_RNN(INPUT_DIM, EMBEDDINGS, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model_3_name = 'model_3'\n",
        "n_epochs_3 = 15"
      ],
      "metadata": {
        "id": "vc1w3DVnSD_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 4\n",
        "\n",
        "* BiRNN(tanh) + GloVe + Embedding: \n",
        "    * Flair Embedding: \n",
        "      * es-clinical-backward\n",
        "      * es-clinical-forward\n",
        "* Hiperpar√°metros: \n",
        "  * hidden dimmension = 1024 \n",
        "  * number of layers = 3 \n",
        "  * dropout = 0.5\n",
        "  * bidirectional = true\n",
        "  * epoch = 15\n"
      ],
      "metadata": {
        "id": "4GeIlk8xR2ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import FlairEmbeddings, WordEmbeddings, StackedEmbeddings\n",
        "\n",
        "embedding_types = [\n",
        "                    FlairEmbeddings('es-clinical-forward'),\n",
        "                    FlairEmbeddings('es-clinical-backward'),\n",
        "                    WordEmbeddings('glove')\n",
        "]\n",
        "EMBEDDINGS_form = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "matrix_len = len(TEXT.vocab)\n",
        "EMBEDDINGS = torch.zeros((matrix_len, 4196))\n",
        "words_found = 0\n",
        "for i, word in enumerate(TEXT.vocab.itos):\n",
        "    try: \n",
        "        sw = Sentence(word)\n",
        "        EMBEDDINGS_form.embed(sw)\n",
        "        for token in sw:\n",
        "          EMBEDDINGS[i] = token.embedding\n",
        "    except KeyError:\n",
        "        EMBEDDINGS[i] = torch.zeros((4196,1))"
      ],
      "metadata": {
        "id": "wTuT5R1bSHI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "\n",
        "# Definir la red\n",
        "class RNN_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding, freeze=True, padding_idx=pad_idx)\n",
        "        self.embedding_dim = embedding.size()[1]\n",
        "        # Capa rnn\n",
        "        self.lstm = nn.RNN(self.embedding_dim, hidden_dim, num_layers=n_layers,\n",
        "                             bidirectional=bidirectional, dropout=dropout)\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        outputs, hidden = self.lstm(self.dropout(embedded))\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "_Q4O1t7gSIXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "HIDDEN_DIM = 1024  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "EMBEDDING_DIM = EMBEDDINGS.size()[1]\n",
        "N_LAYERS = 3  # n√∫mero de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "model_4 = RNN_RNN(INPUT_DIM, EMBEDDINGS, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "model_4_name = 'model_4'\n",
        "n_epochs_4 = 15"
      ],
      "metadata": {
        "id": "wxZLFDFsSI95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 5\n",
        "\n",
        "* BiLSTM\n",
        "* Hiperpar√°metros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 64 \n",
        "  * number of layers = 2\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OMc3Q7ljR4yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class NER_RNN_LSTM(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx,\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "5zK6IAKjZMNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM_LSTM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM_LSTM = 200  # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM_LSTM = 64  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM_LSTM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS_LSTM = 2  # n√∫mero de capas.\n",
        "DROPOUT_LSTM = 0.5\n",
        "BIDIRECTIONAL_LSTM = False\n",
        "\n",
        "LSTM_n_epochs = 10\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "LSTM = NER_RNN_LSTM(INPUT_DIM_LSTM, \n",
        "                      EMBEDDING_DIM_LSTM, \n",
        "                      HIDDEN_DIM_LSTM, \n",
        "                      OUTPUT_DIM_LSTM,\n",
        "                      N_LAYERS_LSTM, \n",
        "                      BIDIRECTIONAL_LSTM, \n",
        "                      DROPOUT_LSTM, \n",
        "                      PAD_IDX,\n",
        "                      )\n",
        "\n",
        "LSTM_name = 'LSTM'  \n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "LSTM_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "Pp2BofgLZdDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM\n",
        "model_name = LSTM_name\n",
        "criterion = LSTM_criterion\n",
        "n_epochs = LSTM_n_epochs"
      ],
      "metadata": {
        "id": "ew_02UELZihl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights_modelLSTM(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM_LSTM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM_LSTM)\n",
        "        \n",
        "model.apply(init_weights_modelLSTM)"
      ],
      "metadata": {
        "id": "aGY-k2-iZlUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 6\n",
        "\n",
        "* BiLSTM + ReLU + Attention layer\n",
        "* Hiperpar√°metros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 3\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n",
        "\n"
      ],
      "metadata": {
        "id": "VvUoJBkQR50p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class LSTM_SelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, encoder_outputs):\n",
        "        # encoder_outputs = [batch size, sent len, hid dim]\n",
        "        energy = self.projection(encoder_outputs)\n",
        "        # energy = [batch size, sent len, 1]\n",
        "        weights = F.softmax(energy.squeeze(-1), dim=1)\n",
        "        # weights = [batch size, sent len]\n",
        "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
        "        # outputs = [batch size, hid dim]\n",
        "        return outputs, weights\n",
        "\n",
        "\n",
        "class NER_RNN_LSTM_Attention(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx,\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "        \n",
        "        self.attention = LSTM_SelfAttention(hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # embedded = self.dropout(self.embedding(text))\n",
        "        output, (hidden, cell) = self.lstm(self.embedding(text))\n",
        "        new_embed, weights = self.attention(output)\n",
        "        new_embed = self.dropout(output)\n",
        "        predictions = self.fc(self.dropout(new_embed))\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "vAD1dTq_Zobn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM_LSTMAttention = len(TEXT.vocab)\n",
        "EMBEDDING_DIM_LSTMAttention = 200  # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM_LSTMAttention = 256  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM_LSTMAttention = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS_LSTMAttention = 3  # n√∫mero de capas.\n",
        "DROPOUT_LSTMAttention = 0.5\n",
        "BIDIRECTIONAL_LSTMAttention = False\n",
        "\n",
        "LSTMAttention_n_epochs = 10\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "LSTMAttention = NER_RNN_LSTM_Attention(INPUT_DIM_LSTMAttention, \n",
        "                      EMBEDDING_DIM_LSTMAttention, \n",
        "                      HIDDEN_DIM_LSTMAttention, \n",
        "                      OUTPUT_DIM_LSTMAttention,\n",
        "                      N_LAYERS_LSTMAttention, \n",
        "                      BIDIRECTIONAL_LSTMAttention, \n",
        "                      DROPOUT_LSTMAttention, \n",
        "                      PAD_IDX,\n",
        "                      )\n",
        "\n",
        "LSTMAttention_name = 'LSTMAttention'  \n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "LSTMAttention_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "DPqFBb92ZvOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMAttention\n",
        "model_name = LSTMAttention_name\n",
        "criterion = LSTMAttention_criterion\n",
        "n_epochs = LSTMAttention_n_epochs"
      ],
      "metadata": {
        "id": "pgaqx2I6Z1IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights_modelLSTMAttention(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM_LSTMAttention)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM_LSTMAttention)\n",
        "        \n",
        "model.apply(init_weights_modelLSTMAttention)"
      ],
      "metadata": {
        "id": "LRne61t9Z6uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 7\n",
        "\n",
        "* BiGRU\n",
        "* Hiperpar√°metros: \n",
        "  * embedding dimmension = 100\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 2\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n"
      ],
      "metadata": {
        "id": "NkP71XZIR6vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class NER_RNN_GRU(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx,\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "        \n",
        "        self.lstm = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        outputs, hidden = self.lstm(embedded)\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "MHistYQUaGUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM_GRU = len(TEXT.vocab)\n",
        "EMBEDDING_DIM_GRU = 100  # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM_GRU = 256  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM_GRU = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS_GRU = 2  # n√∫mero de capas.\n",
        "DROPOUT_GRU = 0.5\n",
        "BIDIRECTIONAL_GRU = False\n",
        "\n",
        "GRU_n_epochs = 10\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "GRU = NER_RNN_GRU(INPUT_DIM_GRU, \n",
        "                      EMBEDDING_DIM_GRU, \n",
        "                      HIDDEN_DIM_GRU, \n",
        "                      OUTPUT_DIM_GRU,\n",
        "                      N_LAYERS_GRU, \n",
        "                      BIDIRECTIONAL_GRU, \n",
        "                      DROPOUT_GRU, \n",
        "                      PAD_IDX,\n",
        "                      )\n",
        "GRU_name = 'GRU'  \n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "GRU_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "n7FFei7NaPe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRU\n",
        "model_name = GRU_name\n",
        "criterion = GRU_criterion\n",
        "n_epochs = GRU_n_epochs"
      ],
      "metadata": {
        "id": "Yt0CXJgUaVGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights_modelGRU(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM_GRU)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM_GRU)\n",
        "        \n",
        "model.apply(init_weights_modelGRU)"
      ],
      "metadata": {
        "id": "PlW0XMhgaXWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 8\n",
        "\n",
        "* BiGRU + ReLU + Attention Layer\n",
        "* Hiperpar√°metros: \n",
        "  * embedding dimmension = 200\n",
        "  * hidden dimmension = 256 \n",
        "  * number of layers = 3\n",
        "  * dropout = 0.5\n",
        "  * epoch = 10\n"
      ],
      "metadata": {
        "id": "w7IOSkE1R8SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class GRU_SelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, encoder_outputs):\n",
        "        # check, same as lstm attention\n",
        "        energy = self.projection(encoder_outputs)\n",
        "        weights = F.softmax(energy.squeeze(-1), dim=1)\n",
        "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
        "        return outputs, weights\n",
        "        \n",
        "class NER_RNN_GRU_Attention(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx,\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "        \n",
        "        self.lstm = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout)\n",
        "\n",
        "        self.attention = GRU_SelfAttention(hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        output, hidden = self.lstm(self.embedding(text))\n",
        "        new_embed, weights = self.attention(output)\n",
        "        new_embed = self.dropout(output)\n",
        "        predictions = self.fc(self.dropout(new_embed))\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "4ia_ItOJalZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM_GRUAttention = len(TEXT.vocab)\n",
        "EMBEDDING_DIM_GRUAttention = 200  # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM_GRUAttention = 256  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM_GRUAttention = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS_GRUAttention = 3  # n√∫mero de capas.\n",
        "DROPOUT_GRUAttention = 0.5\n",
        "BIDIRECTIONAL_GRUAttention = False\n",
        "\n",
        "GRUAttention_n_epochs = 10\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "GRUAttention = NER_RNN_GRU_Attention(INPUT_DIM_GRUAttention, \n",
        "                      EMBEDDING_DIM_GRUAttention, \n",
        "                      HIDDEN_DIM_GRUAttention, \n",
        "                      OUTPUT_DIM_GRUAttention,\n",
        "                      N_LAYERS_GRUAttention, \n",
        "                      BIDIRECTIONAL_GRUAttention, \n",
        "                      DROPOUT_GRUAttention,\n",
        "                      PAD_IDX,\n",
        "                      )\n",
        "\n",
        "GRUAttention_name = 'GRUAttention'  \n",
        "\n",
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "GRUAttention_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "metadata": {
        "id": "OJtSsCYCamJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRUAttention\n",
        "model_name = GRUAttention_name\n",
        "criterion = GRUAttention_criterion\n",
        "n_epochs = GRUAttention_n_epochs"
      ],
      "metadata": {
        "id": "6Yf289kpat3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights_modelGRUAttention(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM_GRUAttention)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM_GRUAttention)\n",
        "        \n",
        "model.apply(init_weights_modelGRUAttention)"
      ],
      "metadata": {
        "id": "nt_Tk8kkawVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPGdirx7aSHZ"
      },
      "source": [
        "------\n",
        "### **Entrenamos y evaluamos**\n",
        "\n",
        "\n",
        "**Importante** : Fijen el modelo, el n√∫mero de √©pocas de entrenamiento, la loss y el optimizador que usar√°n para entrenar y evaluar en las siguientes variables!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8YlGnjxaSHZ"
      },
      "outputs": [],
      "source": [
        "model = model_3\n",
        "model_name = model_3_name\n",
        "criterion = baseline_criterion\n",
        "n_epochs = n_epochs_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu_lXic2aSHd"
      },
      "source": [
        "\n",
        "\n",
        "#### **Inicializamos la red**\n",
        "\n",
        "Iniciamos los pesos de la red de forma aleatoria (Usando una distribuci√≥n normal).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-G_NWFcaSHe"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "        \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjWDX2CJaSHh"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} par√°metros entrenables.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVqBqerlaSHk"
      },
      "source": [
        "Notar que definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZvHtwpaSHq"
      },
      "source": [
        "#### **Definimos el optimizador y el scheduler**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH6o8_cTaSHq"
      },
      "outputs": [],
      "source": [
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import lr_scheduler\n",
        "# learning rate\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
      ],
      "metadata": {
        "id": "OAPnDokXSOwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz39wa78wGYR"
      },
      "source": [
        "#### **Enviamos el modelo a cuda**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqr0AJ6_iicR"
      },
      "outputs": [],
      "source": [
        "# Enviamos el modelo y la loss a cuda (en el caso en que est√© disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Defininimos Early-Stopping**"
      ],
      "metadata": {
        "id": "PQHBEozTSVMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping():\n",
        "    \"\"\"\n",
        "    Early stopping to stop the training when the loss does not improve after\n",
        "    certain epochs.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        \"\"\"\n",
        "        :param patience: how many epochs to wait before stopping when loss is\n",
        "               not improving\n",
        "        :param min_delta: minimum difference between new loss and old loss for\n",
        "               new loss to be considered as an improvement\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss == None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss < self.min_delta:\n",
        "            self.counter += 1\n",
        "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                print(f'INFO: Early stopping')\n",
        "                self.early_stop = True"
      ],
      "metadata": {
        "id": "p9YG1JblSUzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xlq48WjiW6U"
      },
      "source": [
        "#### **Definimos el entrenamiento de la red**\n",
        "\n",
        "Algunos conceptos previos: \n",
        "\n",
        "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
        "- `batch`: una fracci√≥n de la √©poca. Se utilizan para entrenar mas r√°pidamente la red. (mas eficiente pasar n datos que uno en cada ejecuci√≥n del backpropagation)\n",
        "\n",
        "Esta funci√≥n est√° encargada de entrenar la red en una √©poca. Para esto, por cada batch de la √©poca actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\n",
        "\n",
        "Observaci√≥n: En algunos comentarios aparecer√° el tama√±o de los tensores entre corchetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV6YLt0oiicW"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la √©poca:\n",
        "    for batch in iterator:\n",
        "\n",
        "        # Extraemos el texto y los tags del batch que estamos procesado\n",
        "        text = batch.text\n",
        "        tags = batch.nertags\n",
        "\n",
        "        # Reiniciamos los gradientes calculados en la iteraci√≥n anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "\n",
        "\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "        \n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los par√°metros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las m√©tricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYNcwKnAz5Hf"
      },
      "source": [
        "#### **Definimos la funci√≥n de evaluaci√≥n**\n",
        "\n",
        "Evalua el rendimiento actual de la red usando los datos de validaci√≥n. \n",
        "\n",
        "Por cada batch de estos datos, calcula y reporta el loss y las m√©tricas asociadas al conjunto de validaci√≥n. \n",
        "Ya que las m√©tricas son calculadas por cada batch, estas son retornadas promediadas por el n√∫mero de batches entregados. (ver linea del return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsRuiUuHiicY"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.nertags\n",
        "\n",
        "            # Predecimos\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las m√©tricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las m√©tricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs-n9Y5yiica"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy3MVf5H0A94"
      },
      "source": [
        "\n",
        "#### **Entrenamiento de la red**\n",
        "\n",
        "En este cuadro de c√≥digo ejecutaremos el entrenamiento de la red.\n",
        "Para esto, primero definiremos el n√∫mero de √©pocas y luego por cada √©poca, ejecutaremos `train` y `evaluate`.\n",
        "\n",
        "**Importante: Reiniciar los pesos del modelo**\n",
        "\n",
        "Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez. \n",
        "Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la funci√≥n `init_weights`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK5lQqpviicf"
      },
      "outputs": [],
      "source": [
        "best_valid_loss = float('inf')\n",
        "early_stopping = EarlyStopping(patience=3)\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model, train_iterator, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validaci√≥n)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    scheduler.step(valid_loss)\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )\n",
        "    early_stopping(valid_loss)\n",
        "    if early_stopping.early_stop:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcZPraG-9duO"
      },
      "source": [
        "**Importante**: Recuerden que el √∫ltimo modelo entrenado no es el mejor (probablemente est√© *overfitteado*), si no el que guardamos con la menor loss del conjunto de validaci√≥n. Este problema lo pueden solucionar con *early stopping*.\n",
        "Para cargar el mejor modelo entrenado, ejecuten la siguiente celda.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y27CNYfrjtQ-"
      },
      "outputs": [],
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model.load_state_dict(torch.load('{}.pt'.format(model_name)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLuqFKFR9duO"
      },
      "outputs": [],
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "metadata": {
        "id": "GzjsUwYfIXWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBctQHTh0lxD"
      },
      "source": [
        "#### **Evaluamos el set de validaci√≥n con el modelo final**\n",
        "\n",
        "Estos son los resultados de predecir el dataset de evaluaci√≥n con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0gVbP8yiicj"
      },
      "outputs": [],
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF1ysw_Kw6zz"
      },
      "source": [
        "### **Predecir datos para la competencia**\n",
        "\n",
        "Ahora, a partir de los datos de **test** y nuestro modelo entrenado, vamos a predecir las etiquetas que ser√°n evaluadas en la competencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RBs3UU4wLk3"
      },
      "outputs": [],
      "source": [
        "def predict_labels(model, iterator, criterion, fields=fields):\n",
        "\n",
        "    # Extraemos los vocabularios.\n",
        "    text_field = fields[0][1]\n",
        "    nertags_field = fields[1][1]\n",
        "    tags_vocab = nertags_field.vocab.itos\n",
        "    words_vocab = text_field.vocab.itos\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            text_batch = batch.text\n",
        "            text_batch = torch.transpose(text_batch, 0, 1).tolist()\n",
        "\n",
        "            # Predecir los tags de las sentences del batch\n",
        "            predictions_batch = model(batch.text)\n",
        "            predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
        "\n",
        "            # por cada oraci√≥n predicha:\n",
        "            for sentence, sentence_prediction in zip(text_batch,\n",
        "                                                     predictions_batch):\n",
        "                for word_idx, word_predictions in zip(sentence,\n",
        "                                                      sentence_prediction):\n",
        "                    # Obtener el indice del tag con la probabilidad mas alta.\n",
        "                    argmax_index = word_predictions.topk(1)[1]\n",
        "\n",
        "                    current_tag = tags_vocab[argmax_index]\n",
        "                    # Obtenemos la palabra\n",
        "                    current_word = words_vocab[word_idx]\n",
        "\n",
        "                    if current_word != '<pad>':\n",
        "                        predictions.append([current_word, current_tag])\n",
        "                predictions.append(['EOS', 'EOS'])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict_labels(model, test_iterator, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "pcmEqmemHqJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwQp1Ru8Oht8"
      },
      "source": [
        "### **Generar el archivo para la submission**\n",
        "\n",
        "No hay problema si aparecen unk en la salida. Estos no son relevantes para evaluarlos, usamos solo los tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPfZkjJGkWyq"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "if (os.path.isfile('./predictions.zip')):\n",
        "    os.remove('./predictions.zip')\n",
        "\n",
        "if (not os.path.isdir('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "f = open('predictions/predictions.txt', 'w')\n",
        "for i, (word, tag) in enumerate(predictions[:-1]):\n",
        "    if word=='EOS' and tag=='EOS': f.write('\\n')\n",
        "    else: \n",
        "      if i == len(predictions[:-1])-1:\n",
        "        f.write(word + ' ' + tag)\n",
        "      else: f.write(word + ' ' + tag + '\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAtK7y43V7Z_"
      },
      "source": [
        "## **Resultados**\n",
        "\n",
        "A continuaci√≥n se muestra una tabla resumen de los 8 modelos mostrados a lo largo de este trabajo:\n",
        "\n",
        "\n",
        "| Model | Approach                 |Red| type    |Precision| Recall| F1Score  |\n",
        "|-----|----------------------------||-------------|---------|-------|----------|\n",
        "| 1   | flair-clinical forward | lstm | Train     |     0.85    | 0.87      |  0.83\n",
        "| | | | Val|  0.74| 0.76|0.72| \n",
        "2   | flair-clinical forward y backward + Glove| lstm | Train     |     0.86    | 0.88      |  0.85\n",
        "| | | | Val|  0.74| 0.76|0.72|\n",
        "| 3   | flair-clinical forward y backward + Glove| Gru | Train     |     0.66    | 0.73     |  061\n",
        "| | | | Val|  0.65| 0.65|0.66|\n",
        "| 4   | flair-clinical forward y backward + Glove| RNN tanh | Train     |     0.22    | 0.47      |  0.15\n",
        "| | | | Val|  0.25| 0.35|0.19|\n",
        "| 5   | Varir capas RNN, drop out | lstm | Train     |   0.75     |  0.74    |  0.74\n",
        "| | | | Val|  |  |  |\n",
        "| 6   | Capa de atenci√≥n | lstm | Train     | 0.75       |   0.73    |  0.74\n",
        "| | | | Val|  | | |\n",
        "| 7   | Variar capas RNN, drop out| Gru | Train     |    0.77     | 0.69    |  0.73\n",
        "| | | | Val| | ||\n",
        "| 8   | Capa de atenci√≥n| Gru | Train     |   0.75     | 0.71     | 0.73\n",
        "| | | |   | ||\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZEWJXrNaSIf"
      },
      "source": [
        "## **Conclusiones**\n",
        "\n",
        "El modelo que obtuvo mejores resultados fu√© el segundo, mostrando que la combinaci√≥n de los embeddings de Flair, m√°s WordEmbedding GloVe y con una red LSTM presentan el mejor setup para el modelo. Sin embargo, pese a que en este trabajo se obtuvieron valores sobre 0.85, en CodeLab este modelo obtuvo un m√°ximo de 0.62 en todas sus m√©tricas. Lo anterior podr√≠a deberse a que no se probaron tantos modelos ocupando el setup anterior, y quiz√° sea mejor encontrar buenas combinaciones de cantidad de capas y dimensiones ocultas que obtengan los mejores resultados.\n",
        "\n",
        "Por otro lado, los peores modelos son los que juntaron todos los embeddings descritos pero con redes de tipo GRU o RNN con tanh. Lo anterior indica que si bien los embeddings pueden presentar muy buenos resultados, estos var√≠an enormemente dependiendo del tipo de red en el que se entrenen.\n",
        "\n",
        "En cuanto a los modelos que probaron variaciones de par√°metros y de tipos de red, se observ√≥ que los resultados de todos estos no variaron mucho, lo que podr√≠a indicar que a mayor cantidad de capas o de dimensiones ocultas no se obtienen necesariamente mejores resultados. Por otro lado, el cambiar la loss function por ReLU no present√≥ mayores diferencias entre los √∫ltimos modelos.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LMgKjfYC_Go-"
      ],
      "name": "Competencia2_VANICCI_CC6205.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}