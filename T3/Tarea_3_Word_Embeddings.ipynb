{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tarea_3_Word_Embeddings.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ckbt7VPDhBwb"},"source":["# **Tarea 3 - Word Embeddings 游닄**\n","\n","**Integrantes:**\n","\n","**Fecha l칤mite de entrega 游늱:** 3 de mayo.\n","\n","**Tiempo estimado de dedicaci칩n:**"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:30:18.109327Z","start_time":"2020-03-19T18:30:18.103344Z"},"id":"q5CSRY4oNCHK"},"source":["\n","**Instrucciones:**\n","- El ejercicio consiste en:\n","    - Responder preguntas relativas a los contenidos vistos en los v칤deos y slides de las clases. \n","    - Entrenar Word2Vec y Word Context Matrix sobre un peque침o corpus.\n","    - Evaluar los embeddings obtenidos en una tarea de clasificaci칩n.\n","- La tarea se realiza en grupos de **m치ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a trav칠s de u-cursos a m치s tardar el d칤a estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo Jupyter Notebook.\n","- Al momento de la revisi칩n tu c칩digo ser치 ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci칩n. \n","- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav칠s del canal de Discord del curso. \n","\n","\n","**Referencias**\n","\n","V칤deos: \n","\n","- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n","- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n","- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"]},{"cell_type":"markdown","metadata":{"id":"G4wYf0vgnbTv"},"source":["## **Preguntas te칩ricas 游늿 (2 puntos).** ##\n","Para estas preguntas no es necesario implementar c칩digo, pero pueden utilizar pseudo c칩digo."]},{"cell_type":"markdown","metadata":{"id":"B5hUG6-8ngoK"},"source":["### **Parte 1: Modelos Lineales (1 ptos)**"]},{"cell_type":"markdown","metadata":{"id":"5yRvZbhsoi8f"},"source":["Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor칤as: pol칤tica, deporte, negocios y otros. "]},{"cell_type":"markdown","metadata":{"id":"irsqBVmCnx3M"},"source":["**Pregunta 1**: Dise침e un modelo lineal capaz de clasificar un documento seg칰n estas categor칤as donde el output sea un vector con una distribuci칩n de probabilidad con la pertenencia a cada clase. \n","\n","Especifique: representaci칩n de los documentos de entrada, par치metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci칩n de p칠rdida escogida. **(0.5 puntos)**\n","\n","**Respuesta**: \n"]},{"cell_type":"markdown","metadata":{"id":"G5FaWqBVvL90"},"source":["**Pregunta 2**: Explique c칩mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci칩n. **(0.5 puntos)**\n","\n","**Respuesta**: "]},{"cell_type":"markdown","metadata":{"id":"XkK7pc54njZq"},"source":["### **Parte 2: Redes Neuronales (1 ptos)** "]},{"cell_type":"markdown","metadata":{"id":"VUbJjlj_9AFC"},"source":["Supongamos que tenemos la siguiente red neuronal."]},{"cell_type":"markdown","metadata":{"id":"obUfuOYB_TOC"},"source":["![image.png](https://drive.google.com/uc?export=view&id=1fFTjtMvH6MY8o42_vj010y8eTuCVb5a3)"]},{"cell_type":"markdown","metadata":{"id":"s2z-8zKW0_6q"},"source":["**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem치tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci칩n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n","\n","Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.5 Puntos)**\n","\n","**Respuesta**: \n","\n","Formula:\n","$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) =$\n","\n","Dimensiones: \n","\n","**Pregunta 2**: Explique qu칠 es backpropagation. 쮺uales ser칤an los par치metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n","\n","**Respuesta**:\n","\n","**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.25 puntos)**\n","\n","**Respuesta**:"]},{"cell_type":"markdown","metadata":{"id":"ocS_vQhR1gcU"},"source":["## **Preguntas pr치cticas 游눹 (4 puntos).** ##"]},{"cell_type":"markdown","metadata":{"id":"Ol82nJ0FnmcP"},"source":["### **Parte 3: Word Embeddings**"]},{"cell_type":"markdown","source":["En la auxiliar 2 se nombraron dos formas de crear word vectors:\n","\n","-  Distributional Vectors.\n","-  Distributed Vectors.\n","\n","El objetivo de esta parte es comparar las dos embeddings obtenidos de estas dos estrategias en una tarea de clasificaci칩n."],"metadata":{"id":"Daw7Ee5cdQTb"}},{"cell_type":"code","source":["import re  \n","import pandas as pd \n","from time import time  \n","from collections import defaultdict \n","import string \n","import multiprocessing\n","import os\n","import gensim\n","import sklearn\n","from sklearn import linear_model\n","from collections import Counter\n","import numpy as np\n","import scipy\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n","\n","# word2vec\n","from gensim.models import Word2Vec, KeyedVectors, FastText\n","from gensim.models.phrases import Phrases, Phraser\n","from sklearn.model_selection import train_test_split\n","import logging\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","logger = logging.getLogger(__name__)"],"metadata":{"id":"E2G1qcb7AJqW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Parte A (1 punto)** \n","\n","En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librer칤as ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n","\n","```python\n","class WordContextMatrix:\n","\n","  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n","    # se sugiere agregar un una estructura de datos para guardar las\n","    # palabras del vocab y para guardar el conteo de coocurrencia\n","    ...\n","    \n","  def add_word_to_vocab(self, word):\n","    # Le puede ser 칰til considerar un token unk al vocab\n","    # para palabras fuera del vocab\n","    ...\n","  \n","  def build_matrix(self):\n","    ...\n","\n","  def matrix2dict(self):\n","    # se recomienda transformar la matrix a un diccionario de embedding.\n","    ...\n","\n","```\n","\n","puede modificar los par치metros o m칠todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n","\n","```python\n","corpus = [\n","  \"I like deep learning.\",\n","  \"I like NLP.\",\n","  \"I enjoy flying.\"\n","]\n","```\n","\n","Obteniendo una matriz parecia a esta:\n","\n","***Resultado esperado***: \n","\n","| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n","|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n","| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n","| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n","| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n","| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n","| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n","| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n","| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n","| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n","\n","``"],"metadata":{"id":"AuEAv-whdMCG"}},{"cell_type":"markdown","source":["**Respuesta:**"],"metadata":{"id":"ur16vkyO37B5"}},{"cell_type":"code","source":[""],"metadata":{"id":"gOI1FL8MlGZB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OgmeSFqKLpFL"},"source":["#### **Parte B (1.5 puntos)**\n","\n","En esta parte es debe entrenar Word2Vec de gensim y construir la matriz palabra contexto utilizando el dataset de di치logos de los Simpson. "]},{"cell_type":"markdown","metadata":{"id":"tZgN06q4QPi3"},"source":["Utilizando el dataset adjunto con la tarea:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eY3kmg4onnsu","outputId":"d3525a54-0c10-401e-b3e2-9c6e9e714a2c"},"source":["data_file = \"dialogue-lines-of-the-simpsons.zip\"\n","df = pd.read_csv(data_file)\n","stopwords = pd.read_csv(\n","    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",").values\n","stopwords = Counter(stopwords.flatten().tolist())\n","df = df.dropna().reset_index(drop=True) # Quitar filas vacias"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-05-05 17:58:59,568 : INFO : NumExpr defaulting to 2 threads.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"VAg5a5bmWk3T"},"source":["**Pregunta 1**: Ayud치ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec. **(0.75 punto)** (Hint, le puede servir explorar un poco los datos)"]},{"cell_type":"markdown","metadata":{"id":"MWw2fXFRXe5Y"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"Bvwplz7yTNcr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Pregunta 2**: Cree una matriz palabra contexto usando el mismo dataset. Configure el largo del vocabulario a 1000 o 2000 tokens, puede agregar valores mayores pero tenga en cuenta que la construcci칩n de la matriz puede tomar varios minutos. Puede que esto tarde un poco. **(0.75 punto)** "],"metadata":{"id":"3vBkF3hreGjg"}},{"cell_type":"markdown","source":["**Respuesta:**"],"metadata":{"id":"zzLuH6MneWIY"}},{"cell_type":"code","source":[""],"metadata":{"id":"9gPyW8fMeXNX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IRCB-jqgTNcs"},"source":["#### **Parte C (1.5 puntos): Aplicar embeddings para clasificar**"]},{"cell_type":"markdown","metadata":{"id":"zlqzlJRSTNcs"},"source":["Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n","\n","Para esto ocuparemos el lexic칩n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci칩n es positiva y un -1 si es negativa."]},{"cell_type":"code","metadata":{"id":"CMskFDmHTNcs"},"source":["AFINN = 'AFINN_full.csv'\n","df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uaKl8hsCTNcs"},"source":["Hint: Para w2v y la wcm son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr치n una representaci칩n en AFINN. Para el caso de la matriz palabra contexto se recomienda convertir su matrix a un diccionario. Pueden utilizar esta funci칩n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."]},{"cell_type":"code","metadata":{"id":"tWSSuctiTNcs"},"source":["def try_apply(model,word):\n","    try:\n","        aux = model[word]\n","        return True\n","    except KeyError:\n","        #logger.error('Word {} not in dictionary'.format(word))\n","        return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LrVPeEzgTNcs"},"source":["**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci칩n en embedding que acabamos de calcular (con ambos modelos). \n","\n","Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n","\n","Para ambos modelos, separar train y test de acuerdo a la siguiente funci칩n. **(0.5 puntos)**"]},{"cell_type":"code","metadata":{"id":"0Bkt26BwTNcs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["```python\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n","```\n"],"metadata":{"id":"dmFoKWKO2EKA"}},{"cell_type":"markdown","metadata":{"id":"iDcq5czXTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"upAn_eT4TNct"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDKe4gA3TNct"},"source":["**Pregunta 2**: Entrenar una regresi칩n log칤stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu칠 se obtienen estos resultados? C칩mo los mejorar칤as? Como podr칤as mejorar los resultados de la matriz palabra contexto? es equivalente al modelo word2vec? **(1 punto)**"]},{"cell_type":"markdown","metadata":{"id":"hJMzq_dETNct"},"source":["**Respuesta**:"]},{"cell_type":"code","source":[""],"metadata":{"id":"bj1r_BnKn_7L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"izppruGQTNct"},"source":["# Bonus: +0.25 puntos en cualquier pregunta"]},{"cell_type":"markdown","metadata":{"id":"YW0aeK2KTNct"},"source":["**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m치s grande y obtener mejores resultados. Les puede servir [칠sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."]},{"cell_type":"markdown","metadata":{"id":"qvHcVS3sTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"MSc8p-T8TNcu"},"source":[""],"execution_count":null,"outputs":[]}]}