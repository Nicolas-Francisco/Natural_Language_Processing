{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Tarea_3_Word_Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckbt7VPDhBwb"
      },
      "source": [
        "# **Tarea 3 - Word Embeddings üìö**\n",
        "\n",
        "**Integrantes:**  Nicol√°s Garc√≠a - Ricardo Valdivia\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** 3 de mayo.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-19T18:30:18.109327Z",
          "start_time": "2020-03-19T18:30:18.103344Z"
        },
        "id": "q5CSRY4oNCHK"
      },
      "source": [
        "\n",
        "**Instrucciones:**\n",
        "- El ejercicio consiste en:\n",
        "    - Responder preguntas relativas a los contenidos vistos en los v√≠deos y slides de las clases. \n",
        "    - Entrenar Word2Vec y Word Context Matrix sobre un peque√±o corpus.\n",
        "    - Evaluar los embeddings obtenidos en una tarea de clasificaci√≥n.\n",
        "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso. \n",
        "\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "V√≠deos: \n",
        "\n",
        "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
        "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
        "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wYf0vgnbTv"
      },
      "source": [
        "## **Preguntas te√≥ricas üìï (2 puntos).** ##\n",
        "Para estas preguntas no es necesario implementar c√≥digo, pero pueden utilizar pseudo c√≥digo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5hUG6-8ngoK"
      },
      "source": [
        "### **Parte 1: Modelos Lineales (1 ptos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yRvZbhsoi8f"
      },
      "source": [
        "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor√≠as: pol√≠tica, deporte, negocios y otros. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irsqBVmCnx3M"
      },
      "source": [
        "**Pregunta 1**: Dise√±e un modelo lineal capaz de clasificar un documento seg√∫n estas categor√≠as donde el output sea un vector con una distribuci√≥n de probabilidad con la pertenencia a cada clase. \n",
        "\n",
        "Especifique: representaci√≥n de los documentos de entrada, par√°metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci√≥n de p√©rdida escogida. **(0.5 puntos)**\n",
        "\n",
        "**Respuesta**: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5FaWqBVvL90"
      },
      "source": [
        "**Pregunta 2**: Explique c√≥mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci√≥n. **(0.5 puntos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Para evaluar un modelo lineal se necesita una funci√≥n de perdida L de la forma L(f(x, theta), y), determinando un algoritmo de training que encuentra los argumentos que minimicen los valores de L. Esta funci√≥n de p√©rdida recibe los valores predecidos por el modelo f con argumentos 'theta' y los valores reales de los casos de training 'y':\n",
        "\n",
        "\n",
        "Existen distintas funciones de p√©rdida, como hinge (para casos binarios), binary-cross-entropy (funci√≥n de p√©rdida logaistica utilizando la sigmodie), o la categorical-cross-entropy (similar a la anterior pero aplicado a m√°s dimensiones).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkK7pc54njZq"
      },
      "source": [
        "### **Parte 2: Redes Neuronales (1 ptos)** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUbJjlj_9AFC"
      },
      "source": [
        "Supongamos que tenemos la siguiente red neuronal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obUfuOYB_TOC"
      },
      "source": [
        "![image.png](https://drive.google.com/uc?export=view&id=1fFTjtMvH6MY8o42_vj010y8eTuCVb5a3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2z-8zKW0_6q"
      },
      "source": [
        "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem√°tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci√≥n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n",
        "\n",
        "Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.5 Puntos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Formula:\n",
        "$$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x})$$\n",
        "\n",
        "$$\\vec{h}^1 = \\vec{x}W^1 + \\vec{b}^1$$\n",
        "$$\\vec{h}^2 = g(\\vec{h}^1)W^2 + \\vec{b}^2$$\n",
        "$$\\vec{h}^3 = f(\\vec{h}^2)W^3 + \\vec{b}^3$$\n",
        "\n",
        "$$\\vec{\\hat{y}} = h(\\vec{h}^3)W^4 $$\n",
        "\n",
        "$$\\therefore \\ \\ \\ \\vec{\\hat{y}} = h(f(g(\\vec{x}W^1 + \\vec{b}^1)W^2 + \\vec{b}^2)W^3 + \\vec{b}^3)W^4 $$\n",
        "\n",
        "Dimensiones: \n",
        "$[fila, columna]$\n",
        "\n",
        "---\n",
        "\n",
        "$$\\vec{x} = [1,3]$$\n",
        "$$W^1 = [3,2]$$\n",
        "$$\\vec{b}^1 = [1,2]$$\n",
        "$$\\vec{h}^1 = [1,2]$$\n",
        "\n",
        "---\n",
        "\n",
        "$$ W^2 = [2, 3]$$\n",
        "$$\\vec{b}^2 = [1,3]$$\n",
        "$$\\vec{h}^2 = [1,3]$$\n",
        "\n",
        "---\n",
        "\n",
        "$$ W^3 = [3, 1] $$\n",
        "$$\\vec{b}^3 = [1,1] $$\n",
        "$$\\vec{h}^3 = [1,1] $$\n",
        "\n",
        "---\n",
        "\n",
        "$$ W^4 = [1, 4] $$\n",
        "$$\\vec{\\hat{y}}= [1, 4]$$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Pregunta 2**: Explique qu√© es backpropagation. ¬øCuales ser√≠an los par√°metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Corresponde a un algoritmo que tiene como objetivo ajustar los pesos (W), que se utilizan para cada layer, de modo que el error que se comete es minimo. Este algoritmo nos presenta, cuando se equivoca cada capa del error global. Los parametros a evaluar son \n",
        "$$Œ¥^l_{[j]} ‚â° \\frac{\\partial L}{‚àÇ\\vec{h}^l_{[j]}}$$\n",
        "\n",
        "$$ \\frac{‚àÇ\\vec{h}^l_{[j]}}{\\partial W^l_{[i, j]}} = \\vec{z}^{(l-1)}_{[i]}$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W^l_{[i, j]}} =  Œ¥^l_{[j]} √ó \\vec{z}^{(l-1)}_{[i]} $$\n",
        "\n",
        "Donde es necesario definir la loss function a utilizar.\n",
        "\n",
        "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.25 puntos)**\n",
        "\n",
        "**Respuesta**: \n",
        "Backpropagation se puede reducir a cuatro pasos menores, primero el vector de ingreso, se debe pasar por las capas de la red neuronal, usando \n",
        "$$ \\vec{h}^l_{[j]} = \\left( \\sum_{i}{}{W^l_{[i,j]} \\times \\vec{z}^{(l-1)}_{[i]}}\\right ) + \\vec{b}^l_{[j]} $$\n",
        "\n",
        "$$ \\vec{z}^l_{[j]} = g(\\vec{h}^l_{[j]})$$\n",
        "\n",
        "Para luego evaluar,\n",
        "\n",
        "$$\\vec{\\delta^m_{[j]}}$$ en la salida de cada capa.\n",
        "\n",
        "Luego se aplica la ecuacion\n",
        "$$ $$ para obtener\n",
        "$$\\vec{\\delta^l_{[j]}}$$\n",
        "de cada capa oculta, se va desde la capa m√°s alta a la m√°s baja de la red.\n",
        "\n",
        "Finalmente se utiliza, $$\\frac{\\partial L}{\\partial W^l_{[i, j]}} =  Œ¥^l_{[j]} √ó \\vec{z}^{(l-1)}_{[i]} $$\n",
        "donde se utiliza cada derivada antes calculada.\n",
        "\n",
        "\n",
        "Para esto debemos tener calculada cada.\n",
        "\n",
        "$$\\vec{\\delta^1_{[1]}} = \\frac{\\partial L}{\\partial \\vec{h}^1_{[1,1]}}$$ \n",
        "\n",
        "\n",
        "$$\\vec{\\delta^1_{[2]}} = \\frac{\\partial L}{\\partial \\vec{h}^1_{[1,2]}}$$ \n",
        "\n",
        "\n",
        "$$\\vec{\\delta^2_{[1]}} = \\frac{\\partial L}{\\partial \\vec{h}^2_{[1,1]}}$$ \n",
        "\n",
        "\n",
        "$$\\vec{\\delta^2_{[2]}} = \\frac{\\partial L}{\\partial \\vec{h}^2_{[1,2]}}$$ \n",
        "\n",
        "\n",
        "$$\\vec{\\delta^2_{[3]}} = \\frac{\\partial L}{\\partial \\vec{h}^2_{[1,3]}}$$\n",
        "\n",
        "\n",
        "$$\\vec{\\delta^3_{[1]}} = \\frac{\\partial L}{\\partial \\vec{h}^3_{[1,1]}}$$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocS_vQhR1gcU"
      },
      "source": [
        "## **Preguntas pr√°cticas üíª (4 puntos).** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol82nJ0FnmcP"
      },
      "source": [
        "### **Parte 3: Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la auxiliar 2 se nombraron dos formas de crear word vectors:\n",
        "\n",
        "-  Distributional Vectors.\n",
        "-  Distributed Vectors.\n",
        "\n",
        "El objetivo de esta parte es comparar las dos embeddings obtenidos de estas dos estrategias en una tarea de clasificaci√≥n."
      ],
      "metadata": {
        "id": "Daw7Ee5cdQTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re  \n",
        "import pandas as pd \n",
        "from time import time  \n",
        "from collections import defaultdict \n",
        "import string \n",
        "import multiprocessing\n",
        "import os\n",
        "import gensim\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import scipy\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "E2G1qcb7AJqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Parte A (1 punto)** \n",
        "\n",
        "En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librer√≠as ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n",
        "\n",
        "```python\n",
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Utilice el constructor para definir los parametros.\n",
        "    \"\"\"\n",
        "\n",
        "    # se sugiere agregar un una estructura de datos para guardar las\n",
        "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
        "    ...\n",
        "    \n",
        "  def add_word_to_vocab(self, word):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para agregar token\n",
        "    a sus vocabulario\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # Le puede ser √∫til considerar un token unk al vocab\n",
        "    # para palabras fuera del vocab\n",
        "    ...\n",
        "  \n",
        "  def build_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para crear la palabra contexto\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def matrix2dict(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para convertir la matriz a un diccionario de embeddings, donde las llaves deben ser los token del vocabulario y los embeddings los valores obtenidos de la matriz. \n",
        "    \"\"\"\n",
        "\n",
        "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
        "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "puede modificar los par√°metros o m√©todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n",
        "\n",
        "```python\n",
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]\n",
        "```\n",
        "\n",
        "Obteniendo una matriz parecia a esta:\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n",
        "|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n",
        "| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n",
        "| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n",
        "| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n",
        "| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n",
        "| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n",
        "| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n",
        "| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n",
        "| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n",
        "\n",
        "``"
      ],
      "metadata": {
        "id": "AuEAv-whdMCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:**"
      ],
      "metadata": {
        "id": "ur16vkyO37B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Utilice el constructor para definir los parametros.\n",
        "    \"\"\"\n",
        "\n",
        "    # se sugiere agregar un una estructura de datos para guardar las\n",
        "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
        "    self.vocab_size = vocab_size\n",
        "    self.windows_size = window_size\n",
        "    self.dataset = dataset\n",
        "    self.tokenizer = tokenizer\n",
        "    self.vocab = []\n",
        "    self.matrix = np.zeros((vocab_size, vocab_size))\n",
        "    self.dic_matrix = {}\n",
        "    self.tokenizer\n",
        "    \n",
        "  def add_word_to_vocab(self, word):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para agregar token\n",
        "    a sus vocabulario\n",
        "    \"\"\"\n",
        "    if word not in self.vocab and (len(self.vocab) <= self.vocab_size):\n",
        "      self.vocab.append(word)\n",
        "  \n",
        "  def doc_to_vocab(self):\n",
        "\n",
        "    words = []\n",
        "    for doc in self.dataset:\n",
        "      words += self.tokenizer(doc)\n",
        "    count = []\n",
        "    count.extend(Counter(words).most_common(self.vocab_size))\n",
        "    for word, _ in count:\n",
        "      self.add_word_to_vocab(word)\n",
        "\n",
        "\n",
        "  def build_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para crear la palabra contexto\n",
        "    \"\"\"\n",
        "    self.matrix = np.zeros((len(self.vocab), len(self.vocab)))\n",
        "    for doc in self.dataset:\n",
        "      tokenizada = self.tokenizer(doc)\n",
        "      for word, i in zip(tokenizada, range(len(tokenizada))):\n",
        "        # print(\"Word fija\", word)\n",
        "        try:\n",
        "          index = self.vocab.index(word)\n",
        "        except:\n",
        "          index = -1\n",
        "        if index != -1: # Esta en el vocab\n",
        "          # print(\"Esta en el vocab\")\n",
        "          consulta = i-self.windows_size\n",
        "          # print(\"indice inicial consultado\", consulta)\n",
        "          while (consulta <= (i + self.windows_size)):\n",
        "            if consulta >= 0 and consulta < len(tokenizada) and tokenizada[consulta] != word:\n",
        "              try:\n",
        "                vocab_consulta = self.vocab.index(tokenizada[consulta])\n",
        "                # print(\"Palabra a revisar\", tokenizada[consulta], \"en\", vocab_consulta)\n",
        "              except:\n",
        "                vocab_consulta = -1\n",
        "                # print(\"Palabra a revisar\", \"tokenizada[consulta]\", \"en\", vocab_consulta)\n",
        "              if vocab_consulta != -1:\n",
        "                self.matrix[index][vocab_consulta] += 1\n",
        "            #     print(self.matrix[index][vocab_consulta])\n",
        "            # print(\"Matriz total\", self.matrix)\n",
        "            consulta += 1\n",
        "    return self.matrix\n",
        "\n",
        "  def matrix2dict(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para convertir la matriz a un diccionario de embeddings, donde las llaves deben ser los token del vocabulario y los embeddings los valores obtenidos de la matriz. \n",
        "    \"\"\"\n",
        "\n",
        "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
        "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
        "    for word, i in zip(self.vocab, range(len(self.vocab))):\n",
        "      self.dic_matrix[word] = self.matrix[i]\n",
        "    return self.dic_matrix"
      ],
      "metadata": {
        "id": "gOI1FL8MlGZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]"
      ],
      "metadata": {
        "id": "AHUyVQs7pXeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgmeSFqKLpFL"
      },
      "source": [
        "#### **Parte B (1.5 puntos)**\n",
        "\n",
        "En esta parte es debe entrenar Word2Vec de gensim y construir la matriz palabra contexto utilizando el dataset de di√°logos de los Simpson. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZgN06q4QPi3"
      },
      "source": [
        "Utilizando el dataset adjunto con la tarea:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY3kmg4onnsu"
      },
      "source": [
        "data_file = \"dialogue-lines-of-the-simpsons.zip\"\n",
        "df = pd.read_csv(data_file)\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "content = df['spoken_words']"
      ],
      "metadata": {
        "id": "F6wwJXxQydz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n",
        "def simple_tokenizer(doc, lower=False):\n",
        "    if lower:\n",
        "        tokenized_doc = doc.translate(str.maketrans(\n",
        "            '', '', punctuation)).lower().split()\n",
        "\n",
        "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
        "    return tokenized_doc"
      ],
      "metadata": {
        "id": "toHtcYs8dXu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wcm = WordContextMatrix(1000,1,corpus,simple_tokenizer)"
      ],
      "metadata": {
        "id": "kG9KRjxL8Fzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wcm.doc_to_vocab()\n",
        "wcm_matrix = wcm.build_matrix()"
      ],
      "metadata": {
        "id": "7JZsUfCT8KkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(wcm_matrix, index = wcm.vocab, columns= wcm.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "TvGDDS4d8bAb",
        "outputId": "0b3f2cf3-e1ec-44e3-d8fd-9e8437c3fa79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            I  like  deep  learning  NLP  enjoy  flying\n",
              "I         0.0   2.0   0.0       0.0  0.0    1.0     0.0\n",
              "like      2.0   0.0   1.0       0.0  1.0    0.0     0.0\n",
              "deep      0.0   1.0   0.0       1.0  0.0    0.0     0.0\n",
              "learning  0.0   0.0   1.0       0.0  0.0    0.0     0.0\n",
              "NLP       0.0   1.0   0.0       0.0  0.0    0.0     0.0\n",
              "enjoy     1.0   0.0   0.0       0.0  0.0    0.0     1.0\n",
              "flying    0.0   0.0   0.0       0.0  0.0    1.0     0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72c313bf-066a-492f-a933-4ab2f5c3fa59\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>I</th>\n",
              "      <th>like</th>\n",
              "      <th>deep</th>\n",
              "      <th>learning</th>\n",
              "      <th>NLP</th>\n",
              "      <th>enjoy</th>\n",
              "      <th>flying</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>like</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>deep</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learning</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NLP</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enjoy</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flying</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72c313bf-066a-492f-a933-4ab2f5c3fa59')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72c313bf-066a-492f-a933-4ab2f5c3fa59 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72c313bf-066a-492f-a933-4ab2f5c3fa59');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 324
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAg5a5bmWk3T"
      },
      "source": [
        "**Pregunta 1**: Ayud√°ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec. **(0.75 punto)** (Hint, le puede servir explorar un poco los datos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWw2fXFRXe5Y"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_content = [simple_tokenizer(doc) for doc in content.values]"
      ],
      "metadata": {
        "id": "_C5svuCayv9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) \n",
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[cleaned_content]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3jrHsFM6_zE",
        "outputId": "1ec74417-6227-46e3-acd1-0ba8cb1608dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-30 05:36:15,420 : INFO : collecting all words and their counts\n",
            "2022-05-30 05:36:15,423 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
            "2022-05-30 05:36:15,517 : INFO : PROGRESS: at sentence #5000, processed 49035 words and 36026 word types\n",
            "2022-05-30 05:36:15,610 : INFO : PROGRESS: at sentence #10000, processed 96482 words and 62640 word types\n",
            "2022-05-30 05:36:15,695 : INFO : PROGRESS: at sentence #15000, processed 141538 words and 85150 word types\n",
            "2022-05-30 05:36:15,829 : INFO : PROGRESS: at sentence #20000, processed 195196 words and 109693 word types\n",
            "2022-05-30 05:36:15,931 : INFO : PROGRESS: at sentence #25000, processed 247786 words and 133380 word types\n",
            "2022-05-30 05:36:16,043 : INFO : PROGRESS: at sentence #30000, processed 303591 words and 157130 word types\n",
            "2022-05-30 05:36:16,159 : INFO : PROGRESS: at sentence #35000, processed 356281 words and 177689 word types\n",
            "2022-05-30 05:36:16,268 : INFO : PROGRESS: at sentence #40000, processed 403454 words and 195411 word types\n",
            "2022-05-30 05:36:16,362 : INFO : PROGRESS: at sentence #45000, processed 449880 words and 212946 word types\n",
            "2022-05-30 05:36:16,451 : INFO : PROGRESS: at sentence #50000, processed 495223 words and 229961 word types\n",
            "2022-05-30 05:36:16,541 : INFO : PROGRESS: at sentence #55000, processed 539312 words and 245935 word types\n",
            "2022-05-30 05:36:16,628 : INFO : PROGRESS: at sentence #60000, processed 580923 words and 260317 word types\n",
            "2022-05-30 05:36:16,718 : INFO : PROGRESS: at sentence #65000, processed 625928 words and 275730 word types\n",
            "2022-05-30 05:36:16,829 : INFO : PROGRESS: at sentence #70000, processed 676880 words and 294038 word types\n",
            "2022-05-30 05:36:16,934 : INFO : PROGRESS: at sentence #75000, processed 727546 words and 311277 word types\n",
            "2022-05-30 05:36:17,042 : INFO : PROGRESS: at sentence #80000, processed 778548 words and 328056 word types\n",
            "2022-05-30 05:36:17,152 : INFO : PROGRESS: at sentence #85000, processed 829240 words and 344507 word types\n",
            "2022-05-30 05:36:17,261 : INFO : PROGRESS: at sentence #90000, processed 877998 words and 360233 word types\n",
            "2022-05-30 05:36:17,373 : INFO : PROGRESS: at sentence #95000, processed 928648 words and 375729 word types\n",
            "2022-05-30 05:36:17,476 : INFO : PROGRESS: at sentence #100000, processed 978326 words and 391491 word types\n",
            "2022-05-30 05:36:17,582 : INFO : PROGRESS: at sentence #105000, processed 1029302 words and 407068 word types\n",
            "2022-05-30 05:36:17,686 : INFO : PROGRESS: at sentence #110000, processed 1080794 words and 423039 word types\n",
            "2022-05-30 05:36:17,788 : INFO : PROGRESS: at sentence #115000, processed 1130222 words and 437656 word types\n",
            "2022-05-30 05:36:17,890 : INFO : PROGRESS: at sentence #120000, processed 1180592 words and 452581 word types\n",
            "2022-05-30 05:36:17,993 : INFO : PROGRESS: at sentence #125000, processed 1230252 words and 466277 word types\n",
            "2022-05-30 05:36:18,093 : INFO : PROGRESS: at sentence #130000, processed 1279612 words and 478785 word types\n",
            "2022-05-30 05:36:18,131 : INFO : collected 483384 word types from a corpus of 1297999 words (unigram + bigrams) and 131853 sentences\n",
            "2022-05-30 05:36:18,134 : INFO : using 483384 counts as vocab in Phrases<0 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
            "2022-05-30 05:36:18,136 : INFO : source_vocab length 483384\n",
            "2022-05-30 05:36:22,826 : INFO : Phraser built with 70 phrasegrams\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvwplz7yTNcr"
      },
      "source": [
        "SpokenWords_w2v = Word2Vec(min_count=10,\n",
        "                      window=2,\n",
        "                      size=200,\n",
        "                      sample=6e-5,\n",
        "                      alpha=0.03,\n",
        "                      min_alpha=0.0007,\n",
        "                      negative=20,\n",
        "                      workers=multiprocessing.cpu_count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SpokenWords_w2v.build_vocab(sentences, progress_per=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIlbc2jN7IuN",
        "outputId": "7fe34b45-0e5b-4533-c96d-7fcf9a7cc04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-30 05:36:22,851 : INFO : collecting all words and their counts\n",
            "2022-05-30 05:36:22,854 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2022-05-30 05:36:23,210 : INFO : PROGRESS: at sentence #10000, processed 94692 words, keeping 12113 word types\n",
            "2022-05-30 05:36:23,570 : INFO : PROGRESS: at sentence #20000, processed 191523 words, keeping 18839 word types\n",
            "2022-05-30 05:36:23,954 : INFO : PROGRESS: at sentence #30000, processed 297968 words, keeping 24927 word types\n",
            "2022-05-30 05:36:24,297 : INFO : PROGRESS: at sentence #40000, processed 396027 words, keeping 29195 word types\n",
            "2022-05-30 05:36:24,636 : INFO : PROGRESS: at sentence #50000, processed 486084 words, keeping 33175 word types\n",
            "2022-05-30 05:36:24,936 : INFO : PROGRESS: at sentence #60000, processed 570185 words, keeping 36616 word types\n",
            "2022-05-30 05:36:25,264 : INFO : PROGRESS: at sentence #70000, processed 664434 words, keeping 40330 word types\n",
            "2022-05-30 05:36:25,618 : INFO : PROGRESS: at sentence #80000, processed 764408 words, keeping 43973 word types\n",
            "2022-05-30 05:36:25,980 : INFO : PROGRESS: at sentence #90000, processed 862283 words, keeping 47270 word types\n",
            "2022-05-30 05:36:26,320 : INFO : PROGRESS: at sentence #100000, processed 960954 words, keeping 50292 word types\n",
            "2022-05-30 05:36:26,681 : INFO : PROGRESS: at sentence #110000, processed 1061796 words, keeping 53614 word types\n",
            "2022-05-30 05:36:27,017 : INFO : PROGRESS: at sentence #120000, processed 1159986 words, keeping 56466 word types\n",
            "2022-05-30 05:36:27,358 : INFO : PROGRESS: at sentence #130000, processed 1257082 words, keeping 58619 word types\n",
            "2022-05-30 05:36:27,424 : INFO : collected 58974 word types from a corpus of 1275090 raw words and 131853 sentences\n",
            "2022-05-30 05:36:27,426 : INFO : Loading a fresh vocabulary\n",
            "2022-05-30 05:36:27,487 : INFO : effective_min_count=10 retains 7913 unique words (13% of original 58974, drops 51061)\n",
            "2022-05-30 05:36:27,490 : INFO : effective_min_count=10 leaves 1166803 word corpus (91% of original 1275090, drops 108287)\n",
            "2022-05-30 05:36:27,519 : INFO : deleting the raw counts dictionary of 58974 items\n",
            "2022-05-30 05:36:27,522 : INFO : sample=6e-05 downsamples 801 most-common words\n",
            "2022-05-30 05:36:27,524 : INFO : downsampling leaves estimated 487197 word corpus (41.8% of prior 1166803)\n",
            "2022-05-30 05:36:27,555 : INFO : estimated required memory for 7913 words and 200 dimensions: 16617300 bytes\n",
            "2022-05-30 05:36:27,557 : INFO : resetting layer weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = time()\n",
        "SpokenWords_w2v.train(sentences, total_examples=SpokenWords_w2v.corpus_count, epochs=15, report_delay=10)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xlavgnk7PWI",
        "outputId": "c36c57a6-f320-49d3-e6ab-47542bfb81b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-30 05:36:29,158 : INFO : training model with 2 workers on 7913 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2\n",
            "2022-05-30 05:36:30,226 : INFO : EPOCH 1 - PROGRESS: at 15.03% examples, 69805 words/s, in_qsize 0, out_qsize 0\n",
            "2022-05-30 05:36:36,017 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:36:36,024 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:36:36,026 : INFO : EPOCH - 1 : training on 1275090 raw words (486709 effective words) took 6.9s, 71046 effective words/s\n",
            "2022-05-30 05:36:37,052 : INFO : EPOCH 2 - PROGRESS: at 14.35% examples, 68630 words/s, in_qsize 0, out_qsize 0\n",
            "2022-05-30 05:36:42,966 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:36:42,968 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:36:42,973 : INFO : EPOCH - 2 : training on 1275090 raw words (486953 effective words) took 6.9s, 70239 effective words/s\n",
            "2022-05-30 05:36:44,016 : INFO : EPOCH 3 - PROGRESS: at 15.03% examples, 71224 words/s, in_qsize 1, out_qsize 0\n",
            "2022-05-30 05:36:49,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:36:49,686 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:36:49,687 : INFO : EPOCH - 3 : training on 1275090 raw words (486983 effective words) took 6.7s, 72660 effective words/s\n",
            "2022-05-30 05:36:50,711 : INFO : EPOCH 4 - PROGRESS: at 15.03% examples, 72172 words/s, in_qsize 0, out_qsize 0\n",
            "2022-05-30 05:36:56,388 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:36:56,398 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:36:56,399 : INFO : EPOCH - 4 : training on 1275090 raw words (487278 effective words) took 6.7s, 72728 effective words/s\n",
            "2022-05-30 05:36:57,479 : INFO : EPOCH 5 - PROGRESS: at 15.03% examples, 68508 words/s, in_qsize 0, out_qsize 0\n",
            "2022-05-30 05:37:03,093 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:37:03,105 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:37:03,108 : INFO : EPOCH - 5 : training on 1275090 raw words (487113 effective words) took 6.7s, 72745 effective words/s\n",
            "2022-05-30 05:37:04,307 : INFO : EPOCH 6 - PROGRESS: at 14.35% examples, 58699 words/s, in_qsize 0, out_qsize 0\n",
            "2022-05-30 05:37:10,057 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:37:10,072 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:37:10,075 : INFO : EPOCH - 6 : training on 1275090 raw words (486443 effective words) took 7.0s, 69958 effective words/s\n",
            "2022-05-30 05:37:11,127 : INFO : EPOCH 7 - PROGRESS: at 15.03% examples, 70370 words/s, in_qsize 0, out_qsize 0\n",
            "2022-05-30 05:37:16,781 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:37:16,792 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:37:16,793 : INFO : EPOCH - 7 : training on 1275090 raw words (487492 effective words) took 6.7s, 72678 effective words/s\n",
            "2022-05-30 05:37:17,810 : INFO : EPOCH 8 - PROGRESS: at 14.35% examples, 69204 words/s, in_qsize 0, out_qsize 0\n",
            "2022-05-30 05:37:23,526 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:37:23,540 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:37:23,544 : INFO : EPOCH - 8 : training on 1275090 raw words (487872 effective words) took 6.7s, 72397 effective words/s\n",
            "2022-05-30 05:37:24,579 : INFO : EPOCH 9 - PROGRESS: at 15.03% examples, 71632 words/s, in_qsize 0, out_qsize 0\n",
            "2022-05-30 05:37:30,278 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:37:30,283 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:37:30,285 : INFO : EPOCH - 9 : training on 1275090 raw words (487072 effective words) took 6.7s, 72368 effective words/s\n",
            "2022-05-30 05:37:31,312 : INFO : EPOCH 10 - PROGRESS: at 12.16% examples, 57129 words/s, in_qsize 1, out_qsize 0\n",
            "2022-05-30 05:37:37,189 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:37:37,206 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:37:37,207 : INFO : EPOCH - 10 : training on 1275090 raw words (487120 effective words) took 6.9s, 70483 effective words/s\n",
            "2022-05-30 05:37:38,225 : INFO : EPOCH 11 - PROGRESS: at 14.35% examples, 69173 words/s, in_qsize 0, out_qsize 0\n",
            "2022-05-30 05:37:43,899 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:37:43,916 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:37:43,917 : INFO : EPOCH - 11 : training on 1275090 raw words (487213 effective words) took 6.7s, 72742 effective words/s\n",
            "2022-05-30 05:37:44,964 : INFO : EPOCH 12 - PROGRESS: at 15.03% examples, 70711 words/s, in_qsize 0, out_qsize 0\n",
            "2022-05-30 05:37:50,627 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:37:50,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:37:50,640 : INFO : EPOCH - 12 : training on 1275090 raw words (486428 effective words) took 6.7s, 72492 effective words/s\n",
            "2022-05-30 05:37:51,688 : INFO : EPOCH 13 - PROGRESS: at 15.03% examples, 70323 words/s, in_qsize 1, out_qsize 0\n",
            "2022-05-30 05:37:57,503 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:37:57,514 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:37:57,515 : INFO : EPOCH - 13 : training on 1275090 raw words (487386 effective words) took 6.9s, 70983 effective words/s\n",
            "2022-05-30 05:37:58,578 : INFO : EPOCH 14 - PROGRESS: at 15.03% examples, 69739 words/s, in_qsize 1, out_qsize 0\n",
            "2022-05-30 05:38:04,194 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:38:04,199 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:38:04,202 : INFO : EPOCH - 14 : training on 1275090 raw words (487850 effective words) took 6.7s, 73101 effective words/s\n",
            "2022-05-30 05:38:05,261 : INFO : EPOCH 15 - PROGRESS: at 15.03% examples, 70046 words/s, in_qsize 1, out_qsize 0\n",
            "2022-05-30 05:38:10,907 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-30 05:38:10,923 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-30 05:38:10,924 : INFO : EPOCH - 15 : training on 1275090 raw words (486691 effective words) took 6.7s, 72539 effective words/s\n",
            "2022-05-30 05:38:10,928 : INFO : training on a 19126350 raw words (7306603 effective words) took 101.8s, 71800 effective words/s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to train the model: 1.7 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SpokenWords_w2v.init_sims(replace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7Xwrseu7ZxJ",
        "outputId": "4133c747-2dda-4d89-9a8c-ce49de12fea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-30 05:38:10,944 : INFO : precomputing L2-norms of word weight vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pregunta 2**: Cree una matriz palabra contexto usando el mismo dataset. Configure el largo del vocabulario a 1000 o 2000 tokens, puede agregar valores mayores pero tenga en cuenta que la construcci√≥n de la matriz puede tomar varios minutos. Puede que esto tarde un poco. **(0.75 punto)** "
      ],
      "metadata": {
        "id": "3vBkF3hreGjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:**"
      ],
      "metadata": {
        "id": "zzLuH6MneWIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
        ").values\n",
        "def simple_tokenizer(doc, lower=False):\n",
        "    if lower:\n",
        "        tokenized_doc = doc.translate(str.maketrans(\n",
        "            '', '', punctuation)).lower().split()\n",
        "\n",
        "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
        "\n",
        "    tokenized_doc = [\n",
        "        token for token in tokenized_doc if token.lower() not in stopwords\n",
        "    ]\n",
        "    return tokenized_doc"
      ],
      "metadata": {
        "id": "sfMAD8O5y7rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SpokenWords_wcm = WordContextMatrix(10000,2,content.values,simple_tokenizer)"
      ],
      "metadata": {
        "id": "9gPyW8fMeXNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SpokenWords_wcm.doc_to_vocab()\n",
        "sp_matrix = SpokenWords_wcm.build_matrix()"
      ],
      "metadata": {
        "id": "XUgUlDkF7rke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_matrix = pd.DataFrame(sp_matrix, index = SpokenWords_wcm.vocab, columns= SpokenWords_wcm.vocab)"
      ],
      "metadata": {
        "id": "Kowte1OE7-_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_matrix.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "SCXbQv7r91Pb",
        "outputId": "98aa320c-cb4d-458d-c2dd-83722e17b5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Homer    Hey   Bart  Marge  gonna   Dad  time  Yeah  Simpson   Lisa  \\\n",
              "Homer      0.0  180.0   40.0   67.0   52.0  16.0  44.0  29.0    495.0   24.0   \n",
              "Hey      180.0    0.0  101.0   57.0   45.0  83.0  18.0  24.0     48.0   36.0   \n",
              "Bart      40.0  101.0    0.0   23.0   70.0  40.0  47.0  17.0    274.0  110.0   \n",
              "Marge     67.0   57.0   23.0    0.0   56.0   1.0  41.0  16.0    132.0   27.0   \n",
              "gonna     52.0   45.0   70.0   56.0    0.0  37.0  53.0  29.0     25.0   24.0   \n",
              "Dad       16.0   83.0   40.0    1.0   37.0   0.0  30.0  18.0      4.0    9.0   \n",
              "time      44.0   18.0   47.0   41.0   53.0  30.0   0.0  18.0     18.0   21.0   \n",
              "Yeah      29.0   24.0   17.0   16.0   29.0  18.0  18.0   0.0      8.0   11.0   \n",
              "Simpson  495.0   48.0  274.0  132.0   25.0   4.0  18.0   8.0      0.0  181.0   \n",
              "Lisa      24.0   36.0  110.0   27.0   24.0   9.0  21.0  11.0    181.0    0.0   \n",
              "\n",
              "         ...  Squeaky  Oakland  Murdoch  PoliceCops  fondue  initials  swab  \\\n",
              "Homer    ...      0.0      0.0      0.0         0.0     0.0       0.0   0.0   \n",
              "Hey      ...      0.0      0.0      0.0         1.0     0.0       0.0   0.0   \n",
              "Bart     ...      0.0      0.0      0.0         0.0     0.0       0.0   0.0   \n",
              "Marge    ...      0.0      0.0      0.0         0.0     0.0       0.0   0.0   \n",
              "gonna    ...      0.0      0.0      0.0         0.0     0.0       1.0   0.0   \n",
              "Dad      ...      0.0      0.0      0.0         0.0     0.0       0.0   0.0   \n",
              "time     ...      0.0      0.0      0.0         0.0     0.0       0.0   0.0   \n",
              "Yeah     ...      0.0      0.0      0.0         0.0     0.0       0.0   0.0   \n",
              "Simpson  ...      0.0      0.0      0.0         0.0     0.0       0.0   0.0   \n",
              "Lisa     ...      0.0      1.0      0.0         0.0     0.0       0.0   0.0   \n",
              "\n",
              "         stretched  Champagne  agrees  \n",
              "Homer          0.0        0.0     0.0  \n",
              "Hey            0.0        0.0     0.0  \n",
              "Bart           0.0        0.0     1.0  \n",
              "Marge          0.0        0.0     0.0  \n",
              "gonna          0.0        0.0     0.0  \n",
              "Dad            0.0        0.0     0.0  \n",
              "time           0.0        0.0     0.0  \n",
              "Yeah           0.0        0.0     0.0  \n",
              "Simpson        0.0        0.0     0.0  \n",
              "Lisa           0.0        0.0     0.0  \n",
              "\n",
              "[10 rows x 10000 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f1541d7-179f-434e-a2af-19169e913cbd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Homer</th>\n",
              "      <th>Hey</th>\n",
              "      <th>Bart</th>\n",
              "      <th>Marge</th>\n",
              "      <th>gonna</th>\n",
              "      <th>Dad</th>\n",
              "      <th>time</th>\n",
              "      <th>Yeah</th>\n",
              "      <th>Simpson</th>\n",
              "      <th>Lisa</th>\n",
              "      <th>...</th>\n",
              "      <th>Squeaky</th>\n",
              "      <th>Oakland</th>\n",
              "      <th>Murdoch</th>\n",
              "      <th>PoliceCops</th>\n",
              "      <th>fondue</th>\n",
              "      <th>initials</th>\n",
              "      <th>swab</th>\n",
              "      <th>stretched</th>\n",
              "      <th>Champagne</th>\n",
              "      <th>agrees</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Homer</th>\n",
              "      <td>0.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>495.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hey</th>\n",
              "      <td>180.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bart</th>\n",
              "      <td>40.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>274.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Marge</th>\n",
              "      <td>67.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gonna</th>\n",
              "      <td>52.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dad</th>\n",
              "      <td>16.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>time</th>\n",
              "      <td>44.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Yeah</th>\n",
              "      <td>29.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Simpson</th>\n",
              "      <td>495.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>274.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lisa</th>\n",
              "      <td>24.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows √ó 10000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f1541d7-179f-434e-a2af-19169e913cbd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f1541d7-179f-434e-a2af-19169e913cbd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f1541d7-179f-434e-a2af-19169e913cbd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 335
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(SpokenWords_w2v.wv.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k92fDHbmB_xl",
        "outputId": "2f7cb907-3fc0-4aeb-e535-eb1c91f971e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7913"
            ]
          },
          "metadata": {},
          "execution_count": 336
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sp_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNpzB4G-CJIU",
        "outputId": "a4430a8c-dbf7-4bbc-ae21-c9866eb3774f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 337
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRCB-jqgTNcs"
      },
      "source": [
        "#### **Parte C (1.5 puntos): Aplicar embeddings para clasificar**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlqzlJRSTNcs"
      },
      "source": [
        "Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n",
        "\n",
        "Para esto ocuparemos el lexic√≥n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci√≥n es positiva y un -1 si es negativa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMskFDmHTNcs"
      },
      "source": [
        "AFINN = 'AFINN_full.csv'\n",
        "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaKl8hsCTNcs"
      },
      "source": [
        "Hint: Para w2v y la wcm son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr√°n una representaci√≥n en AFINN. Para el caso de la matriz palabra contexto se recomienda convertir su matrix a un diccionario. Pueden utilizar esta funci√≥n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWSSuctiTNcs"
      },
      "source": [
        "def try_apply(model,word):\n",
        "    try:\n",
        "        aux = model[word]\n",
        "        return True\n",
        "    except KeyError:\n",
        "        #logger.error('Word {} not in dictionary'.format(word))\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrVPeEzgTNcs"
      },
      "source": [
        "**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci√≥n en embedding que acabamos de calcular (con ambos modelos). \n",
        "\n",
        "Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n",
        "\n",
        "Para ambos modelos, separar train y test de acuerdo a la siguiente funci√≥n. **(0.5 puntos)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bkt26BwTNcs"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class BaseFeature(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "class Doc2VecTransformer(BaseFeature):\n",
        "    \"\"\" Transforma tweets a representaciones vectoriales usando alg√∫n modelo de Word Embeddings.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, aggregation_func):\n",
        "        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n",
        "        self.model = model.wv \n",
        "        \n",
        "        # indicamos la funci√≥n de agregaci√≥n (np.min, np.max, np.mean, np.sum, ...)\n",
        "        self.aggregation_func = aggregation_func\n",
        "\n",
        "    def simple_tokenizer(self, doc, lower=False):\n",
        "        \"\"\"Tokenizador. Elimina signos de puntuaci√≥n, lleva las letras a min√∫scula(opcional) y \n",
        "           separa el tweet por espacios.\n",
        "        \"\"\"\n",
        "        if lower:\n",
        "            doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
        "        return doc.translate(str.maketrans('', '', string.punctuation)).split()\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \n",
        "        doc_embeddings = []\n",
        "        for word in X:\n",
        "          selected_wv = []\n",
        "          if word in self.model.vocab:\n",
        "             selected_wv.append(self.model[word])   \n",
        "          if len(selected_wv) > 0:\n",
        "            doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n",
        "            doc_embeddings.append(doc_embedding)\n",
        "          else: \n",
        "              # print('No pude encontrar ning√∫n embedding para la palabra {}. Agregando vector de ceros.'.format(word))\n",
        "              doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n",
        "\n",
        "        return np.array(doc_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc2vec_mean = Doc2VecTransformer(SpokenWords_w2v, np.mean)\n",
        "doc2vec_sum = Doc2VecTransformer(SpokenWords_w2v, np.sum)\n",
        "doc2vec_max = Doc2VecTransformer(SpokenWords_w2v, np.max)\n"
      ],
      "metadata": {
        "id": "gBUuw6PZpujz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SpokenWords_dict = SpokenWords_wcm.matrix2dict()\n",
        "df_wcm = pd.DataFrame(columns=['embeding', 'sentimiento'])\n",
        "for word, i in df_afinn.values:\n",
        "    if try_apply(SpokenWords_dict, word):\n",
        "        df_wcm = df_wcm.append({'embeding': SpokenWords_dict[word], \n",
        "                                    'sentimiento': i}, ignore_index=True) \n"
      ],
      "metadata": {
        "id": "dVAmfAOgSc4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n",
        "```\n"
      ],
      "metadata": {
        "id": "dmFoKWKO2EKA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDcq5czXTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(df_afinn[0].values, df_afinn[1].values, \n",
        "                                                        random_state=0, test_size=0.1, stratify=df_afinn[1].values)"
      ],
      "metadata": {
        "id": "pTW9LHKNraGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upAn_eT4TNct"
      },
      "source": [
        "X_train_wcm, X_test_wcm, y_train_wcm, y_test_wcm = train_test_split(df_wcm['embeding'].values.tolist(), df_wcm['sentimiento'].values.tolist(), \n",
        "                                                        random_state=0, test_size=0.1, stratify=df_wcm['sentimiento'].values.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDKe4gA3TNct"
      },
      "source": [
        "**Pregunta 2**: Entrenar una regresi√≥n log√≠stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu√© se obtienen estos resultados? C√≥mo los mejorar√≠as? Como podr√≠as mejorar los resultados de la matriz palabra contexto? es equivalente al modelo word2vec? **(1 punto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJMzq_dETNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regresi√≥n Logistica y reporte para Word2Vec\n"
      ],
      "metadata": {
        "id": "O2hUD-wotHTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_w2v = LogisticRegression(max_iter=1000000)\n",
        "pipeline_w2v = Pipeline([('doc2vec', doc2vec_sum), ('clf', clf_w2v)])"
      ],
      "metadata": {
        "id": "bj1r_BnKn_7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_w2v.fit(X_train_w2v, y_train_w2v)\n",
        "y_pred = pipeline_w2v.predict(X_test_w2v)\n",
        "conf_matrix = confusion_matrix(y_test_w2v, y_pred)\n",
        "print(conf_matrix)\n",
        "print(classification_report(y_test_w2v, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxXKCF-tqtSI",
        "outputId": "46cd84cc-6fed-4fad-9fa4-7a017d5802f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[212   9]\n",
            " [ 95  23]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.69      0.96      0.80       221\n",
            "           1       0.72      0.19      0.31       118\n",
            "\n",
            "    accuracy                           0.69       339\n",
            "   macro avg       0.70      0.58      0.55       339\n",
            "weighted avg       0.70      0.69      0.63       339\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regresi√≥n Logistica  y reporte para WordContextMatrix"
      ],
      "metadata": {
        "id": "MC69hEhctIcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_wcm = LogisticRegression(max_iter=1000000)"
      ],
      "metadata": {
        "id": "pAvzp6NBq4v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_wcm = Pipeline([('clf', clf_wcm)])\n",
        "pipeline_wcm.fit(X_train_wcm, y_train_wcm)\n",
        "y_pred = pipeline_wcm.predict(X_test_wcm)\n",
        "conf_matrix = confusion_matrix(y_test_wcm, y_pred)\n",
        "print(conf_matrix)\n",
        "print(classification_report(y_test_wcm, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9OS7uiWtKWr",
        "outputId": "dd3cf075-f322-45a6-a23d-fdac0e751529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[57  9]\n",
            " [27 18]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.68      0.86      0.76        66\n",
            "           1       0.67      0.40      0.50        45\n",
            "\n",
            "    accuracy                           0.68       111\n",
            "   macro avg       0.67      0.63      0.63       111\n",
            "weighted avg       0.67      0.68      0.65       111\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los resultados obtenidos son muy similares, pero no totalmente equivalentes pues WCM obtiene mejor f1-score para el caso positivo en comparaci√≥n con W2V, mientras que W2V es levemente mejor en el resto de las m√©tricas.\n",
        "\n",
        "Para mejorar estos resultados, aumentar√≠amos en primer lugar el tama√±o del vocabulario para reducir el uso de la palabra comod√≠n 'unk' (permitiendo mayor representaci√≥n y significado individual de cada palabra), y adem√°s aumentar la ventana de contexto para que se pueda obtener mayor informaci√≥n de las palabras. Tambi√©n se podr√≠a usar un dataset m√°s grande para entranarlo y con el mejorar los resultados de la matriz.\n",
        "\n",
        "Las modificaciones anteriores mejorar√≠an directamente los resultados de la matriz palabra contexto, pues al aumentar el vocabulario y aumentar la ventana de contexto se obtienen mejores representaciones de cada palabra.\n",
        "\n",
        "Finalmente el modelo WordContextMatrix es equivalente al modelo word2vec, pero en el sentido del uso de vectores para la clasificaci√≥n y el calculo de distancia entre palabras. Si se habla en el sentido de representaci√≥n de documentos y de significado, Word Embeddings es totalmente distinto a Word2Vec."
      ],
      "metadata": {
        "id": "ffWSGsmxBWYO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izppruGQTNct"
      },
      "source": [
        "# Bonus: +0.25 puntos en cualquier pregunta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW0aeK2KTNct"
      },
      "source": [
        "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m√°s grande y obtener mejores resultados. Les puede servir [√©sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvHcVS3sTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSc8p-T8TNcu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}