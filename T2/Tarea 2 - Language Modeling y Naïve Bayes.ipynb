{"cells":[{"cell_type":"markdown","metadata":{"id":"2obO44rRIDIm"},"source":["# **Tarea 2 - CC6205 Natural Language Processing 📚**\n","\n","**Integrantes: Nicolás García**\n","\n","**Fecha límite de entrega 📆:** Martes 12 de abril.\n","\n","**Tiempo estimado de dedicación:**"]},{"cell_type":"markdown","metadata":{"id":"Zpupcv6QW2fd"},"source":["Bienvenid@s a la segunda tarea del curso de Natural Language Processing (NLP). En esta tarea estaremos modelando probabilísticamente el lenguaje mediante **Languaje Modeling** y clasificando textos mediante el método **Naïve Bayes**. Específicamente, la tarea consta de una parte teórica que busca evaluar conceptos vistos en clases sobre los **Language Models** y una parte práctica donde **programarán a mano** el método **Naïve Bayes**. \n","\n","**Instrucciones:**\n","- La tarea se realiza en grupos de **máximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a través de u-cursos a más tardar el día estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo Jupyter Notebook o el archivo .ipynb si lo ejecuto de forma local.\n","- Al momento de la revisión tu código será ejecutado. Por favor verifica que tu entrega no tenga errores de compilación. \n","- Está **PROHIBIDO** usar cualquier librería que implemente los algoritmos pedidos (Spacy, scikit, etc). Sólo se podrán utilizar las librerías importadas al inicio de la sección de práctica.\n","- En el horario de auxiliar pueden realizar consultas acerca de la tarea a través del canal de Discord del curso.\n","\n","Si aún no han visto las clases, se recomienda visitar los links de las referencias.\n","\n","**Referencias:**\n","\n","Slides:\n","    \n","- [Language Models](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/lmslides.pdf) (slides by Michael Collins)\n","- [Text Classification and Naive Bayes](https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf) (slides by Dan Jurafsky)\n","\n","Videos: \n","\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 1](https://www.youtube.com/watch?v=9E2jJ6kcb4Y&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=4)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 2](https://www.youtube.com/watch?v=ZWqbEQXLra0&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=5)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 3](https://www.youtube.com/watch?v=tsumFqwFlaA&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=6)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 4](https://www.youtube.com/watch?v=s3TWdv4sqkg&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=7)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 1](https://www.youtube.com/watch?v=kG9BK9Oy1hU)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 2](https://www.youtube.com/watch?v=Iqte5kKHvzE)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 3](https://www.youtube.com/watch?v=TSJg0_X3Abk)\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"JstKx3TiKcIp"},"source":["---------------------------\n","## Parte 1. Language Modeling (3 pts)\n","\n","En esta parte responderán preguntas **teóricas** sobre los Lenguage Models. Para comprender como funcionan muchas de las técnicas que veremos posteriormente durante el curso será muy útil que dominen estos modelos y sus fundamentos.\n","\n","Recuerden que los **Language Models** básicamente nos permiten, dado un corpus, estimar un modelo probabilista al que le podemos pasar una oración y determinar que tan probable es que esa oración haya sido generada. Para esto, tenemos que un modelo de $n$-gramas puede ser definido por una *cadena de Márkov* de orden $n-1$.\n","\n","En clases vimos los modelos basados en unigramas, bigramas y trigramas. En esta pregunta trabajaremos con modelos de lenguaje basados en 4-gramas (*cadena de Márkov* de tercer orden).\n","\n","**Nota:** Las preguntas deben ser resueltas con desarrollo, pero sin código. Por favor usen $\\LaTeX$ para las fórmulas."]},{"cell_type":"markdown","metadata":{"id":"2hwW-07MrRpt"},"source":["\n","### 1.1. (1 pt)\n","\n","Asuma que tenemos calculados los parámetros $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ para todas las posibles secuencias de tamaño 4 que aparecen en un corpus $\\mathcal{C}$. Dado esto, muestre cómo el modelo le asignaría una probabilidad a la frase `una persona corriendo rápido`"]},{"cell_type":"markdown","metadata":{"id":"YzlQDAVqWNdX"},"source":["` ` \n","\n","**Respuesta:** \n","\n","\n","\\begin{equation}\n","\\begin{split}\n","p(una\\ persona\\ corriendo\\ rápido) =& \\prod_{i=1}^{4} q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})\\ , \\ con \\ w_{-2} = w_{-1} = w_{0} =\\ *\\\\\n","=& \\ q(una\\ |\\ *,\\ * ,\\ *) \\times \\\\\n","& \\ q(persona\\ |\\ *,\\ * ,\\ una) \\times \\\\\n","& \\ q(corriendo\\ |\\ *,\\ una ,\\ persona) \\times \\\\\n","& \\ q(rápido\\ |\\ una ,\\ persona,\\ corriendo)\n","\\end{split}\n","\\end{equation}\n","\n","` `"]},{"cell_type":"markdown","metadata":{},"source":["recordemos que la probabilidad de un documento está dado por un proceso de Markov de primer órden cuando trabajamos con un modelo de lenguaje usando trigramas. Para el caso de usar 4-gramas, tendremos un proceso de Markov de segundo órden, que considera hasta 3 palabras previas en la pitatoria.\n","Dado que por enunciado tenemos que se conocen los parámetros $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$, solo basta expandir la pitatoria ya descrita."]},{"cell_type":"markdown","metadata":{"id":"pAxsHPGxbBMb"},"source":["Candidata a pregunta: Explique porque podría ser importante conocer las probabilidades en un Language Models y alguna aplicación en tareas de NLP."]},{"cell_type":"markdown","metadata":{"id":"Dv_FyGKbbpo5"},"source":["Podría ser importante ya que esta probabilidad considera tanto patrones de construcción de texto y palabras contextuales. Bag of Words calcula la probabilidad de cada palabra aislada, por lo que este método es una mejora para los modelos de predicción y clasificación de texto."]},{"cell_type":"markdown","metadata":{"id":"lAej_jqtVwm1"},"source":["### 1.2 Estimando las probabilidades (1 pt)"]},{"cell_type":"markdown","metadata":{"id":"gXSFlCIex8kq"},"source":["#### 1.2.1. Modelo simple (0.5 puntos)\n","\n","Si hubieses tenido que estimar las probabilidades condicionales (parámetros del modelo) $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ a partir de $\\mathcal{C}$, ¿cómo la definirías siguiendo el principio de máxima verosimilitud?"]},{"cell_type":"markdown","metadata":{"id":"RjNisxPzWsMG"},"source":["` ` \n","\n","**Respuesta:** \n","\n","\\begin{equation}\n","q_{ML}(w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) = \\frac{Count(w_{i-3}, w_{i-2}, w_{i-1}, w_{i})}{Count(w_{i-3}, w_{i-2}, w_{i-1})}\n","\\end{equation}\n","\n","donde $Count(w_{i-3}, w_{i-2}, w_{i-1})$ corresponde a las ocurrencias de la cadena $(w_{i-3}w_{i-2}w_{i-1})$ en $C$\n","\n","` `"]},{"cell_type":"markdown","metadata":{},"source":["Para el caso de tener que calcular cada grama del modelo, podrémos calcularlos usando el principio de máxima verosimilitud como la suma de las ocurrencias de la cadena propuesta, divida en la cantidad de ocurrencias de la cadena sin considerar el último parámetro. Eso simplemente es la extensión de los $q_{ML}$ vistos en clase, aplicado a gramas de 4 dimensiones en vez de 3 o 1."]},{"cell_type":"markdown","metadata":{"id":"bwNkPIXure0C"},"source":["#### 1.2.2. Modelo interpolado (0.5 puntos)\n","Muestre cómo se calcularía $q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})$ usando un modelo que interpola modelos de lenguajes basados en 4-grams, trigramas, bigramas y unigramas. ¿Te fue necesario definir nuevos parámetros? En caso afirmativo, ¿qué los diferencia de los parámetros del modelo simple y qué propiedades deben cumplir?"]},{"cell_type":"markdown","metadata":{"id":"zeLZAd0Tr9ne"},"source":["` ` \n","\n","**Respuesta:** \n","\\begin{equation}\n","q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) = \\lambda_{1} \\times q_{ML}(w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) + \\lambda_{2} \\times q_{ML}(w_{i} | w_{i-2}, w_{i-1}) + \\lambda_{3} \\times q_{ML}(w_{i} | w_{i-1}) + \\lambda_{4} \\times q_{ML}(w_{i})]\n","\\end{equation}\n","\n","donde $\\lambda_{1}+\\lambda_{2}+\\lambda_{3}+\\lambda_{4} = 1$, y para todo $\\lambda_{i}$ con $0 < i < 5$, $\\lambda_{i} > 0$\n","\n","` `\n"]},{"cell_type":"markdown","metadata":{},"source":["En este caso fue necesario definir 4 nuevos parámetros para poder considerar una *ponderación lineal* de cada modelo de i-grama. Estos se diferencian del modelo simple ya que considera tanto la probabilidad individual de cada palabra, como las ocurrencias de hasta 3 palabras anteriores.\n","Este modelo exige que cada parámetro lambda sea mayor a 0 y que su sumatoria sea exactamente 1."]},{"cell_type":"markdown","metadata":{"id":"sHqcRJ7Vr_8x"},"source":["### 1.3. Ventajas modelo interpolado (0.5 puntos)\n","¿Qué ventajas tiene el modelo interpolado sobre el modelo de 4-gramas simple?\n"]},{"cell_type":"markdown","metadata":{"id":"6F5R3Ji7sHjt"},"source":["` ` \n","\n","**Respuesta:** \n","\n","El modelo interpolado no solamente considera la probabilidad condicional de cuatro palabras consecutivas, si no que también la probabilidad de menos palabras consecutivas y la probabilidad individual de la misma palabra. Esto hace que el cálculo de la probabilidad de cada secuencia sea más preciso y considere más variables de contexto, comparado con solamente el modelo de 4-gramas simple o con un modelo unigrama / bag of words. \n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"vC2BGjlXxULB"},"source":["### 1.4 Evaluación de Language Model (0.5 puntos)\n","Suponga que usted está modelando un language model sobre un corpus con un vocabulario $\\nu$, en donde N es igual a $\\nu$ + 1. Según lo visto en clases, ¿Cómo evaluaría su modelo? Y ¿Cómo podría determinar que su modelo tiene un mal rendimiento?, Justifique su respuesta. "]},{"cell_type":"markdown","metadata":{"id":"ZnBqiOGhxYfG"},"source":["` ` \n","\n","**Respuesta:** \n","\n","escriba su respuesta aquí\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"rdmB-07ZKfaa"},"source":["-----------------------\n","## Parte 2. Naïve Bayes (3 pts)\n","En esta parte programaremos nuestro primer clasificador de documentos. Para esto implementaremos el método **Naïve Bayes Multinomial** usando **Laplace Smothing**.\n","\n","Por favor, documenten su código. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en algún lugar de su código hacen algo que creen que debe ser explicado, lox comentarios son bienvenidos.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PpjjKnJUvRiA"},"source":["### 2.1. Clase para clasificador (0.5 pt)\n","\n","Programa una clase `MyMultinomialNB` que en su inicializador reciba el parametro de generalización `alpha`.\n","\n","```python\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, ...):\n","    ...\n","```\n","\n","Para más informacion sobre la construcción de esta clase puedes revisar [aquí](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#classifier) \n","\n","Una llamada de ejemplo para crear un objeto de tu clase sería:\n","```python\n","my_clf = MyMultinomialNB(alpha=1)\n","``` \n","lo que debiera crear un clasificador con parámetro `alpha` igual a 1."]},{"cell_type":"markdown","metadata":{"id":"ROG50eH0xfxO"},"source":["### 2.2. Entrenamiento del clasificador (1 pt)\n","\n","Programa el entrenamiento de tu clasificador en el método `fit` de la clase `MyMultinomialNB`. La función debiera recibir el parámetro X que es un `DataFrame` de `pandas` con las columnas `words` y `class_`, donde `words` es una tupla con las palabras asociadas al cada documento y `class_` es el identificador de la clase a la que pertenece cada documento.\n","\n","Para computar el entrenamiento de nuestro clasificador debemos: \n","- extraer el vocabulario,\n","- determinar las probabilidades $p(c_j)$ para cada una de las clases posibles, \n","- determinar las probabilidades $p(w_i|c_j)$ para cada una de las palabras y cada una de las clases usando **Laplace Smothing**.\n","\n","El resultado del metodo `fit` será la misma instancia de nuestro clasificador `self`.\n","\n","```python\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, ...):\n","    ...\n","\n","  def fit(self, X):\n","    ...\n","    return self\n","```\n","\n","**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reemplácenlas por sumas de logaritmos para prevenir errores de precisión. Revisen la diapo 69 de las slides. "]},{"cell_type":"markdown","metadata":{"id":"FNouTCmR2FgY"},"source":["### 2.3. Predicción (1 pt)\n","\n","Programa la predicción de tu clasificador en el método `predict` de la clase `MyMultinomialNB`. Al igual que la función `fit`, `predict` debe recibir un `DataFrame` X con valores `None` en la columna `class_` y devolver una lista con las clases que maximizan la probabilidad de Bayes para cada uno de los elmentos de X (filas)."]},{"cell_type":"markdown","metadata":{"id":"8wyhFWeLgYDI"},"source":["### Implementación 2.1, 2.2 y 2.3 (2.5 pt)"]},{"cell_type":"code","execution_count":442,"metadata":{"id":"DYFEgTyw2ELL"},"outputs":[],"source":["# Acá implementarán las preguntas 2.1, 2.2 y 2.3\n","\n","# importamos algunos paquetes necesarios\n","import numpy as np\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.utils.validation import check_is_fitted\n","\n","# definimos nuestra clase de clasificador Naive Bayes\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, alpha=1.0):\n","    # inicializamos un clasificador con valor alpha\n","    self.alpha = alpha\n","\n","    # creamos un diccionario vacío con el vocabulario\n","    self.vocab = []\n","\n","    # inicializamos las probabilidades de cada clase\n","    self.class_prob_dicc = {}\n","\n","    # inicializamos las probabilidades de cada palabra en cada clase\n","    # utilizando un dataframe\n","    self.word_prob_df = pd.DataFrame(columns=['class_', 'word', 'prob'])\n","\n","  def fit(self, X):\n","    # entrenamos el clasificador con los datos de entrenamiento\n","    # en el ejemplo, X es un dataframe de documentos con columnas ['words', 'class_']\n","    # primero, extraemos el vocabulario del documento, asumiendo que \n","    # words es una tupla de palabras y evitando repetidos\n","    print(X)\n","    for index, row in X.iterrows():\n","      for w in row['words']:  \n","        if w not in self.vocab:\n","          self.vocab.append(w)\n","\n","    # Ahora iremos por cada clase, calculando su probabilidad para\n","    # posteriormente calcular la probabilidad de cada palabra en cada clase\n","    # p(w | c) utilizando el teorema de Bayes para probabilidades condicionales\n","    for c in X['class_'].unique():  # para cada clase\n","      # calculamos la probabilidad de la clase\n","      self.class_prob_dicc[c] = X[X['class_'] == c].shape[0] / X.shape[0]\n","\n","      # luego calculamos la probabilidad de cada palabra \n","      for w in self.vocab:\n","        # contamos la cantidad de veces que aparece la palabra w en documentos con clase c\n","        # y la cantidad de palabras en documentos con clase c\n","        count_w_c = 0\n","        count_w = 0\n","        for index, row in X.iterrows():\n","          if row['class_'] == c:\n","            if w in row['words']:\n","              count_w_c += row['words'].count(w)\n","            count_w += len(row['words'])\n","        # calculamos la probabilidad usando laplace smoothing\n","        nominator_alpha = (count_w_c + self.alpha)\n","        denominator_alpha = (count_w + len(self.vocab) * self.alpha)\n","        # Y se guarda en el dataframe word_prob_df\n","        self.word_prob_df = self.word_prob_df.append({'class_': c, 'word': w, 'prob': nominator_alpha / denominator_alpha}, ignore_index=True)\n","    return self\n","\n","  def predict(self, Y):\n","    # Chequea que fit ha sido ejecutado anteriormente\n","    # check_is_fitted(self)\n","    \n","    # ahora predecimos la clase de cada documento recibido en Y\n","    # usando los datos de entrenamiento y underflow prevention\n","\n","    # Inicializamos una lista vacía para guardar las clases predichas\n","    prediccions = []\n","\n","    # para cada documento\n","    for index, row in Y.iterrows():\n","      # inicializamos un diccinario vacío para guardar las probabilidades\n","      # este contiene primero el logaritmo de la probabilidad de la clase\n","      probs_c = {}\n","      # por cada clase en el diccionario\n","      for k in self.class_prob_dicc.keys():\n","        # para cada palabra en el documento\n","        doc_prob = np.log(self.class_prob_dicc[k])\n","        for w in row['words']:\n","          # calculamos la probabilidad sumando el logaritmo de la probabilidad guardada en word_prob_df\n","          word_prob = self.word_prob_df[(self.word_prob_df['class_'] == k) & (self.word_prob_df['word'] == w)]['prob']\n","          try: \n","            doc_prob += np.log(word_prob.values[0])\n","          except:   # Si arroja error de ambigous truth value por pandas, aplicamos all()\n","            doc_prob += np.log(word_prob.all())\n","        probs_c[k] = doc_prob\n","      # luego la clase con mayor probabilidad es la clase predecida\n","      prediccions.append(max(probs_c, key=probs_c.get))\n","    # retornamos las clases predichas\n","    return prediccions "]},{"cell_type":"markdown","metadata":{"id":"_KOMJ-CS8PRP"},"source":["### 2.4. Probando el clasificador (0.5 pt)"]},{"cell_type":"markdown","metadata":{"id":"hucdz-R7xerG"},"source":["A continuación probarán el funcionamiento de su clasificador. Para esto, les presentamos un conjunto de documentos de entrenamiento `train_set` divididos en 2 categorias distintas. Ustedes deberán primero entrenar su clasificador usando el método `fit` de su clase y luego, clasificar los documentos del conjunto de prueva `test_set` usando el método `predict`.\n","\n","**NOTA:** Como pueden ver, los objetos `namedtuple`s tienen dos atributos: `words` donde están las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`."]},{"cell_type":"code","execution_count":443,"metadata":{"id":"JjTnFLDGyCEL"},"outputs":[],"source":["import pandas as pd\n","\n","from collections import namedtuple\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"markdown","metadata":{"id":"JfS5wXfwxx6t"},"source":["#### 2.4.1 Primer Caso: Clasificación de ejemplo visto en clases (0.20 pt)"]},{"cell_type":"code","execution_count":444,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230,"status":"ok","timestamp":1648470177360,"user":{"displayName":"Ignacio Meza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhI9R2Sl9kpnSGYDYcmCbQ2_IwnY3_WeFFdC_YSXg=s64","userId":"07738957670140287594"},"user_tz":180},"id":"5yXBv2Kqxyyf","outputId":"ac27f7bf-e30a-41cc-e79c-02710804431a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Documentos de entrenamiento\n","                          words class_\n","0   (Chinese, Beijing, Chinese)      c\n","1  (Chinese, Chinese, Shanghai)      c\n","2              (Chinese, Macao)      c\n","3       (Tokyo, Japan, Chinese)      j\n","\n","Documentos de prueba:\n","                                       words class_\n","0  (Chinese, Chinese, Chinese, Tokyo, Japan)   None\n"]}],"source":["document = namedtuple(\n","    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",")\n","\n","train_set = [['Chinese Beijing\tChinese', 'c'],\n","             ['Chinese\tChinese\tShanghai','c'],\n","             ['Chinese\tMacao','c'],\n","             ['Tokyo\tJapan\tChinese','j']]\n","\n","train_set = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set]\n","X_train = pd.DataFrame(data=train_set)\n","\n","test_set = [['Chinese\tChinese\tChinese\tTokyo Japan', None]]\n","test_set = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set]\n","X_test = pd.DataFrame(data=test_set)\n","\n","X_train = pd.DataFrame(data=train_set)\n","print(\"Documentos de entrenamiento\")\n","print(X_train)\n","\n","X_test = pd.DataFrame(data=test_set)\n","print(\"\\nDocumentos de prueba:\")\n","print(X_test)"]},{"cell_type":"code","execution_count":445,"metadata":{"id":"UAx0t3zQx2PJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["                          words class_\n","0   (Chinese, Beijing, Chinese)      c\n","1  (Chinese, Chinese, Shanghai)      c\n","2              (Chinese, Macao)      c\n","3       (Tokyo, Japan, Chinese)      j\n","vocab:  ['Chinese', 'Beijing', 'Shanghai', 'Macao', 'Tokyo', 'Japan']\n","\n","Test predictions:\n","c <- Chinese Chinese Chinese Tokyo Japan\n"]}],"source":["# Acá probarán su clasificador\n","\n","# inicializamos el clasificador\n","my_clf = MyMultinomialNB(alpha=1)\n","\n","# entrenamos el clasificador para los datos de entrenamiento X_train\n","my_clf.fit(X_train)\n","\n","# acá puedes ver el vocabulario extraído por tu clasificador, \n","# intenta tenerlo guardado en my_clf.vocab\n","print('vocab: ', my_clf.vocab)\n","\n","# si implementaron el método predict_proba en el clasificador (no era obligatorio), \n","# acá lo pueden probar\n","# print('\\nTest probs:')\n","# print('\\n'.join([str(l) for l in my_clf.predict(X_test)]))\n","\n","# obtengamos las predicciones\n","print('\\nTest predictions:')\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"]},{"cell_type":"markdown","metadata":{"id":"dMyCgXxvx29L"},"source":["**Respuesta esperada:**\n","\n","**Nota:** No es necesario que obtenga exactamente la misma probabilidad, lo importante es que su clasificador genere la predicción expuesta.\n","\n","```python\n","vocab:  ['Beijing', 'Chinese', 'Macao', 'Tokyo', 'Japan', 'Shanghai']\n","\n","Test probs:\n","[0.68975861 0.31024139]\n","\n","Test predictions:\n","c <- Chinese Chinese Chinese Tokyo Japan\n","```"]},{"cell_type":"markdown","metadata":{"id":"mpG_t1wTx99m"},"source":["#### 2.4.2 Segundo Caso (0.30 pt)"]},{"cell_type":"code","execution_count":446,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1648470184312,"user":{"displayName":"Ignacio Meza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhI9R2Sl9kpnSGYDYcmCbQ2_IwnY3_WeFFdC_YSXg=s64","userId":"07738957670140287594"},"user_tz":180},"id":"HLi8PxdV2VQX","outputId":"9e8d2fbd-399d-4aab-f045-a965220360f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Documentos de entrenamiento\n","                                           words  class_\n","0            (w03, w01, w02, w06, w02, w08, w07)       0\n","1  (w05, w04, w00, w06, w09, w07, w06, w09, w05)       1\n","2  (w07, w06, w00, w08, w01, w08, w08, w09, w02)       0\n","3            (w08, w09, w02, w06, w05, w08, w07)       1\n","4            (w09, w08, w05, w08, w05, w00, w08)       1\n","5            (w05, w05, w06, w01, w06, w08, w02)       1\n","6            (w04, w03, w07, w05, w04, w00, w02)       0\n","7       (w01, w00, w01, w04, w09, w02, w04, w07)       1\n","\n","Documentos de prueba:\n","                                      words class_\n","0  (w02, w09, w06, w01, w05, w04, w03, w03)   None\n","1  (w03, w03, w04, w05, w01, w06, w09, w02)   None\n"]}],"source":["document = namedtuple(\n","    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",")\n","\n","train_set = (\n","    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n","    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n","    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n","    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n","    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n","    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n","    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n","    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",")\n","X_train = pd.DataFrame(data=train_set)\n","print(\"Documentos de entrenamiento\")\n","print(X_train)\n","\n","test_set = (document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),\n","            document(words=('w03', 'w03', 'w04', 'w05', 'w01', 'w06', 'w09', 'w02'), class_=None))\n","X_test = pd.DataFrame(data=test_set)\n","print(\"\\nDocumentos de prueba:\")\n","print(X_test)"]},{"cell_type":"code","execution_count":447,"metadata":{"id":"OXHwmOWB-4Aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                           words  class_\n","0            (w03, w01, w02, w06, w02, w08, w07)       0\n","1  (w05, w04, w00, w06, w09, w07, w06, w09, w05)       1\n","2  (w07, w06, w00, w08, w01, w08, w08, w09, w02)       0\n","3            (w08, w09, w02, w06, w05, w08, w07)       1\n","4            (w09, w08, w05, w08, w05, w00, w08)       1\n","5            (w05, w05, w06, w01, w06, w08, w02)       1\n","6            (w04, w03, w07, w05, w04, w00, w02)       0\n","7       (w01, w00, w01, w04, w09, w02, w04, w07)       1\n","vocab:  ['w03', 'w01', 'w02', 'w06', 'w08', 'w07', 'w05', 'w04', 'w00', 'w09']\n","\n","Test predictions:\n","0 <- w02 w09 w06 w01 w05 w04 w03 w03\n","0 <- w03 w03 w04 w05 w01 w06 w09 w02\n"]}],"source":["# Acá probarán su clasificador\n","\n","# inicializamos el clasificador\n","my_clf = MyMultinomialNB(alpha=1)\n","\n","# entrenamos el clasificador para los datos de entrenamiento X_train\n","my_clf.fit(X_train)\n","\n","# acá puedes ver el vocabulario extraído por tu clasificador, \n","# intenta tenerlo guardado en my_clf.vocab\n","print('vocab: ', my_clf.vocab)\n","\n","# si implementaron el método predict_proba en el clasificador (no era obligatorio), \n","# acá lo pueden probar\n","# print('\\nTest probs:')\n","# print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n","\n","# obtengamos las predicciones \n","print('\\nTest predictions:')\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"]},{"cell_type":"markdown","metadata":{"id":"5tDZnmns_1dW"},"source":["#### 2.4.3 (OPCIONAL) Oraciones reales\n","\n","Aquí intentaremos entrenar un clasificador para determinar cuando una oracion en inglés es interrogativa, afirmativa o negativa."]},{"cell_type":"code","execution_count":448,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1648470188138,"user":{"displayName":"Ignacio Meza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhI9R2Sl9kpnSGYDYcmCbQ2_IwnY3_WeFFdC_YSXg=s64","userId":"07738957670140287594"},"user_tz":180},"id":"YCWi3oytd2nf","outputId":"ff582f23-6756-4d7d-fe64-f1add154b4c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Documentos de entrenamiento:\n","                                                words class_\n","0                (Do, you, have, plenty, of, time, ?)      ?\n","1                 (Does, she, have, enough, money, ?)      ?\n","2           (Did, they, have, any, useful, advice, ?)      ?\n","3                           (What, day, is, today, ?)      ?\n","4                      (I, do, n't, have, much, time)      -\n","5                  (She, does, n't, have, any, money)      -\n","6      (They, did, n't, have, any, advice, to, offer)      -\n","7                    (Have, you, plenty, of, time, ?)      ?\n","8                        (Has, she, enough, money, ?)      ?\n","9                 (Had, they, any, useful, advice, ?)      ?\n","10                         (I, have, n't, much, time)      -\n","11                        (She, has, n't, any, money)      -\n","12             (He, had, n't, any, advice, to, offer)      -\n","13                                 (How, are, you, ?)      ?\n","14    (How, do, you, make, questions, in, English, ?)      ?\n","15             (How, long, have, you, lived, here, ?)      ?\n","16      (How, often, do, you, go, to, the, cinema, ?)      ?\n","17                    (How, much, is, this, dress, ?)      ?\n","18                            (How, old, are, you, ?)      ?\n","19     (How, many, people, came, to, the, meeting, ?)      ?\n","20                            (I, ’, m, from, France)      +\n","21                           (I, come, from, the, UK)      +\n","22               (My, phone, number, is, 61709832145)      +\n","23  (I, work, as, a, tour, guide, for, a, local, t...      +\n","24                     (I, ’, m, not, dating, anyone)      -\n","25           (I, live, with, my, wife, and, children)      +\n","26        (I, often, do, morning, exercises, at, 6am)      +\n","27                                 (I, run, everyday)      +\n","28                         (She, walks, very, slowly)      +\n","29               (They, eat, a, lot, of, meat, daily)      +\n","30                  (We, were, in, France, that, day)      +\n","31                           (He, speaks, very, fast)      +\n","32          (They, told, us, they, came, back, early)      +\n","33                  (I, told, her, I, 'll, be, there)      +\n","\n","Documentos de prueba:\n","                                                words class_\n","0                (Do, you, know, who, lives, here, ?)      ?\n","1                             (What, time, is, it, ?)      ?\n","2    (Can, you, tell, me, where, she, comes, from, ?)      ?\n","3                                  (How, are, you, ?)      ?\n","4                              (I, fill, good, today)      +\n","5              (There, is, a, lot, of, history, here)      +\n","6                              (I, love, programming)      +\n","7      (He, told, us, not, to, make, so, much, noise)      +\n","8   (We, were, asked, not, to, park, in, front, of...      +\n","9                      (I, do, n't, have, much, time)      -\n","10                 (She, does, n't, have, any, money)      -\n","11     (They, did, n't, have, any, advice, to, offer)      -\n","12                         (I, am, not, really, sure)      +\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","document = namedtuple(\n","    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",")\n","\n","train_set2 = [\n","              ['Do you have plenty of time?', '?'],\n","              ['Does she have enough money?','?'],\n","              ['Did they have any useful advice?','?'],\n","              ['What day is today?','?'],\n","              [\"I don't have much time\",'-'],\n","              [\"She doesn't have any money\",'-'],\n","              [\"They didn't have any advice to offer\",'-'],\n","              ['Have you plenty of time?','?'],\n","              ['Has she enough money?','?'],\n","              ['Had they any useful advice?','?'],\n","              [\"I haven't much time\",'-'],\n","              [\"She hasn't any money\",'-'],\n","              [\"He hadn't any advice to offer\",'-'],\n","              ['How are you?','?'],\n","              ['How do you make questions in English?','?'],\n","              ['How long have you lived here?','?'],\n","              ['How often do you go to the cinema?','?'],\n","              ['How much is this dress?','?'],\n","              ['How old are you?','?'],\n","              ['How many people came to the meeting?','?'],\n","              ['I’m from France','+'],\n","              ['I come from the UK','+'],\n","              ['My phone number is 61709832145','+'],\n","              ['I work as a tour guide for a local tour company','+'],\n","              ['I’m not dating anyone','-'],\n","              ['I live with my wife and children','+'],\n","              ['I often do morning exercises at 6am','+'],\n","              ['I run everyday','+'],\n","              ['She walks very slowly','+'],\n","              ['They eat a lot of meat daily','+'],\n","              ['We were in France that day', '+'],\n","              ['He speaks very fast', '+'],\n","              ['They told us they came back early', '+'],\n","              [\"I told her I'll be there\", '+']\n","]\n","train_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set2]\n","X_train2 = pd.DataFrame(data=train_set2)\n","print(\"Documentos de entrenamiento:\")\n","print(X_train2)\n","\n","test_set2 = [\n","             ['Do you know who lives here?','?'],\n","             ['What time is it?','?'],\n","             ['Can you tell me where she comes from?','?'],\n","             ['How are you?','?'],\n","             ['I fill good today', '+'],\n","             ['There is a lot of history here','+'],\n","             ['I love programming','+'],\n","             ['He told us not to make so much noise','+'],  # interesing case\n","             ['We were asked not to park in front of the house','+'],  # interesing case\n","             [\"I don't have much time\",'-'],\n","             [\"She doesn't have any money\",'-'],\n","             [\"They didn't have any advice to offer\",'-'],\n","             ['I am not really sure','+']\n","]\n","test_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set2]\n","X_test2 = pd.DataFrame(data=test_set2)\n","print(\"\\nDocumentos de prueba:\")\n","print(X_test2)"]},{"cell_type":"code","execution_count":449,"metadata":{"id":"6Wdp22w2ArUl"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                words class_\n","0                (Do, you, have, plenty, of, time, ?)      ?\n","1                 (Does, she, have, enough, money, ?)      ?\n","2           (Did, they, have, any, useful, advice, ?)      ?\n","3                           (What, day, is, today, ?)      ?\n","4                      (I, do, n't, have, much, time)      -\n","5                  (She, does, n't, have, any, money)      -\n","6      (They, did, n't, have, any, advice, to, offer)      -\n","7                    (Have, you, plenty, of, time, ?)      ?\n","8                        (Has, she, enough, money, ?)      ?\n","9                 (Had, they, any, useful, advice, ?)      ?\n","10                         (I, have, n't, much, time)      -\n","11                        (She, has, n't, any, money)      -\n","12             (He, had, n't, any, advice, to, offer)      -\n","13                                 (How, are, you, ?)      ?\n","14    (How, do, you, make, questions, in, English, ?)      ?\n","15             (How, long, have, you, lived, here, ?)      ?\n","16      (How, often, do, you, go, to, the, cinema, ?)      ?\n","17                    (How, much, is, this, dress, ?)      ?\n","18                            (How, old, are, you, ?)      ?\n","19     (How, many, people, came, to, the, meeting, ?)      ?\n","20                            (I, ’, m, from, France)      +\n","21                           (I, come, from, the, UK)      +\n","22               (My, phone, number, is, 61709832145)      +\n","23  (I, work, as, a, tour, guide, for, a, local, t...      +\n","24                     (I, ’, m, not, dating, anyone)      -\n","25           (I, live, with, my, wife, and, children)      +\n","26        (I, often, do, morning, exercises, at, 6am)      +\n","27                                 (I, run, everyday)      +\n","28                         (She, walks, very, slowly)      +\n","29               (They, eat, a, lot, of, meat, daily)      +\n","30                  (We, were, in, France, that, day)      +\n","31                           (He, speaks, very, fast)      +\n","32          (They, told, us, they, came, back, early)      +\n","33                  (I, told, her, I, 'll, be, there)      +\n","vocab:  109 ['Do', 'you', 'have', 'plenty', 'of', 'time', '?', 'Does', 'she', 'enough', 'money', 'Did', 'they', 'any', 'useful', 'advice', 'What', 'day', 'is', 'today', 'I', 'do', \"n't\", 'much', 'She', 'does', 'They', 'did', 'to', 'offer', 'Have', 'Has', 'Had', 'has', 'He', 'had', 'How', 'are', 'make', 'questions', 'in', 'English', 'long', 'lived', 'here', 'often', 'go', 'the', 'cinema', 'this', 'dress', 'old', 'many', 'people', 'came', 'meeting', '’', 'm', 'from', 'France', 'come', 'UK', 'My', 'phone', 'number', '61709832145', 'work', 'as', 'a', 'tour', 'guide', 'for', 'local', 'company', 'not', 'dating', 'anyone', 'live', 'with', 'my', 'wife', 'and', 'children', 'morning', 'exercises', 'at', '6am', 'run', 'everyday', 'walks', 'very', 'slowly', 'eat', 'lot', 'meat', 'daily', 'We', 'were', 'that', 'speaks', 'fast', 'told', 'us', 'back', 'early', 'her', \"'ll\", 'be', 'there']\n","\n","Test predictions:\n","? <- Do you know who lives here ?\n","? <- What time is it ?\n","? <- Can you tell me where she comes from ?\n","? <- How are you ?\n","+ <- I fill good today\n","+ <- There is a lot of history here\n","+ <- I love programming\n","- <- He told us not to make so much noise\n","? <- We were asked not to park in front of the house\n","- <- I do n't have much time\n","- <- She does n't have any money\n","- <- They did n't have any advice to offer\n","+ <- I am not really sure\n","              precision    recall  f1-score   support\n","\n","           ?       1.00      0.67      0.80         6\n","           +       0.75      1.00      0.86         3\n","           -       0.80      1.00      0.89         4\n","\n","    accuracy                           0.85        13\n","   macro avg       0.85      0.89      0.85        13\n","weighted avg       0.88      0.85      0.84        13\n","\n"]}],"source":["# Acá probarán su clasificador y computaremos algunas métricas de evaluacion\n","\n","from sklearn.metrics import classification_report\n","\n","# inicializamos el clasificador\n","my_clf2 = MyMultinomialNB(alpha=1)\n","\n","# entrenamos el clasificador para los datos de entrenamiento X_train2\n","my_clf2.fit(X_train2)\n","\n","# acá puedes ver el vocabulario extraído por tu clasificador, \n","# intenta tenerlo guardado en my_clf.vocab\n","print('vocab: ', len(my_clf2.vocab), my_clf2.vocab)\n","\n","# si implementaron el método predict_proba en el clasificador (no era obligatorio), \n","# acá lo pueden probar\n","# print('\\nTest probs:')\n","# print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test2)]))\n","\n","# obtengamos las predicciones para X_test2\n","print('\\nTest predictions:')\n","my_y_preds = my_clf2.predict(X_test2)\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_y_preds, X_test2['words'])]))\n","print(classification_report(y_true=X_test2['class_'], y_pred=my_y_preds, target_names=['?', '+', '-']))"]},{"cell_type":"markdown","metadata":{"id":"WXbg6sNTdAlO"},"source":["**Respuesta aproximada:**\n","\n","**Nota:** No es necesario que obtenga exactamente los mismos resultados.\n","\n","```python\n","              precision    recall  f1-score   support\n","\n","           ?       1.00      0.67      0.80         6\n","           +       0.75      1.00      0.86         3\n","           -       0.80      1.00      0.89         4\n","\n","    accuracy                           0.85        13\n","   macro avg       0.85      0.89      0.85        13\n","weighted avg       0.88      0.85      0.84        13\n","```"]}],"metadata":{"colab":{"collapsed_sections":["JstKx3TiKcIp","2hwW-07MrRpt","gXSFlCIex8kq","bwNkPIXure0C","sHqcRJ7Vr_8x","vC2BGjlXxULB","rdmB-07ZKfaa","8wyhFWeLgYDI","JfS5wXfwxx6t","mpG_t1wTx99m","5tDZnmns_1dW"],"name":"Tarea 2 - Language Modeling y Naïve Bayes.ipynb","provenance":[{"file_id":"1kQCSkkokIVSZkHeo8sGEKwn6ROSItQme","timestamp":1618336151140},{"file_id":"1jZG4e_poENFYDeljHjx9goskyQ3Jy3GU","timestamp":1618284831252}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
