{"cells":[{"cell_type":"markdown","metadata":{"id":"2obO44rRIDIm"},"source":["# **Tarea 2 - CC6205 Natural Language Processing üìö**\n","\n","**Integrantes: Nicol√°s Garc√≠a**\n","\n","**Fecha l√≠mite de entrega üìÜ:** Martes 12 de abril.\n","\n","**Tiempo estimado de dedicaci√≥n:**"]},{"cell_type":"markdown","metadata":{"id":"Zpupcv6QW2fd"},"source":["Bienvenid@s a la segunda tarea del curso de Natural Language Processing (NLP). En esta tarea estaremos modelando probabil√≠sticamente el lenguaje mediante **Languaje Modeling** y clasificando textos mediante el m√©todo **Na√Øve Bayes**. Espec√≠ficamente, la tarea consta de una parte te√≥rica que busca evaluar conceptos vistos en clases sobre los **Language Models** y una parte pr√°ctica donde **programar√°n a mano** el m√©todo **Na√Øve Bayes**. \n","\n","**Instrucciones:**\n","- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo Jupyter Notebook o el archivo .ipynb si lo ejecuto de forma local.\n","- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n","- Est√° **PROHIBIDO** usar cualquier librer√≠a que implemente los algoritmos pedidos (Spacy, scikit, etc). S√≥lo se podr√°n utilizar las librer√≠as importadas al inicio de la secci√≥n de pr√°ctica.\n","- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso.\n","\n","Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n","\n","**Referencias:**\n","\n","Slides:\n","    \n","- [Language Models](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/lmslides.pdf) (slides by Michael Collins)\n","- [Text Classification and Naive Bayes](https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf) (slides by Dan Jurafsky)\n","\n","Videos: \n","\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 1](https://www.youtube.com/watch?v=9E2jJ6kcb4Y&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=4)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 2](https://www.youtube.com/watch?v=ZWqbEQXLra0&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=5)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 3](https://www.youtube.com/watch?v=tsumFqwFlaA&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=6)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Language Models parte 4](https://www.youtube.com/watch?v=s3TWdv4sqkg&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=7)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 1](https://www.youtube.com/watch?v=kG9BK9Oy1hU)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 2](https://www.youtube.com/watch?v=Iqte5kKHvzE)\n","- [CC6205 - Procesamiento de Lenguaje Natural: Naive Bayes Parte 3](https://www.youtube.com/watch?v=TSJg0_X3Abk)\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"JstKx3TiKcIp"},"source":["---------------------------\n","## Parte 1. Language Modeling (3 pts)\n","\n","En esta parte responder√°n preguntas **te√≥ricas** sobre los Lenguage Models. Para comprender como funcionan muchas de las t√©cnicas que veremos posteriormente durante el curso ser√° muy √∫til que dominen estos modelos y sus fundamentos.\n","\n","Recuerden que los **Language Models** b√°sicamente nos permiten, dado un corpus, estimar un modelo probabilista al que le podemos pasar una oraci√≥n y determinar que tan probable es que esa oraci√≥n haya sido generada. Para esto, tenemos que un modelo de $n$-gramas puede ser definido por una *cadena de M√°rkov* de orden $n-1$.\n","\n","En clases vimos los modelos basados en unigramas, bigramas y trigramas. En esta pregunta trabajaremos con modelos de lenguaje basados en 4-gramas (*cadena de M√°rkov* de tercer orden).\n","\n","**Nota:** Las preguntas deben ser resueltas con desarrollo, pero sin c√≥digo. Por favor usen $\\LaTeX$ para las f√≥rmulas."]},{"cell_type":"markdown","metadata":{"id":"2hwW-07MrRpt"},"source":["\n","### 1.1. (1 pt)\n","\n","Asuma que tenemos calculados los par√°metros $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ para todas las posibles secuencias de tama√±o 4 que aparecen en un corpus $\\mathcal{C}$. Dado esto, muestre c√≥mo el modelo le asignar√≠a una probabilidad a la frase `una persona corriendo r√°pido`"]},{"cell_type":"markdown","metadata":{"id":"YzlQDAVqWNdX"},"source":["` ` \n","\n","**Respuesta:** \n","\n","\n","\\begin{equation}\n","\\begin{split}\n","p(una\\ persona\\ corriendo\\ r√°pido) =& \\prod_{i=1}^{4} q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})\\ , \\ con \\ w_{-2} = w_{-1} = w_{0} =\\ *\\\\\n","=& \\ q(una\\ |\\ *,\\ * ,\\ *) \\times \\\\\n","& \\ q(persona\\ |\\ *,\\ * ,\\ una) \\times \\\\\n","& \\ q(corriendo\\ |\\ *,\\ una ,\\ persona) \\times \\\\\n","& \\ q(r√°pido\\ |\\ una ,\\ persona,\\ corriendo)\n","\\end{split}\n","\\end{equation}\n","\n","` `"]},{"cell_type":"markdown","metadata":{},"source":["recordemos que la probabilidad de un documento est√° dado por un proceso de Markov de primer √≥rden cuando trabajamos con un modelo de lenguaje usando trigramas. Para el caso de usar 4-gramas, tendremos un proceso de Markov de segundo √≥rden, que considera hasta 3 palabras previas en la pitatoria.\n","Dado que por enunciado tenemos que se conocen los par√°metros $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$, solo basta expandir la pitatoria ya descrita."]},{"cell_type":"markdown","metadata":{"id":"pAxsHPGxbBMb"},"source":["Candidata a pregunta: Explique porque podr√≠a ser importante conocer las probabilidades en un Language Models y alguna aplicaci√≥n en tareas de NLP."]},{"cell_type":"markdown","metadata":{"id":"Dv_FyGKbbpo5"},"source":["Podr√≠a ser importante ya que esta probabilidad considera tanto patrones de construcci√≥n de texto y palabras contextuales. Bag of Words calcula la probabilidad de cada palabra aislada, por lo que este m√©todo es una mejora para los modelos de predicci√≥n y clasificaci√≥n de texto."]},{"cell_type":"markdown","metadata":{"id":"lAej_jqtVwm1"},"source":["### 1.2 Estimando las probabilidades (1 pt)"]},{"cell_type":"markdown","metadata":{"id":"gXSFlCIex8kq"},"source":["#### 1.2.1. Modelo simple (0.5 puntos)\n","\n","Si hubieses tenido que estimar las probabilidades condicionales (par√°metros del modelo) $q(w_i | w_{i-3}, w_{i-2}, w_{i-1})$ a partir de $\\mathcal{C}$, ¬øc√≥mo la definir√≠as siguiendo el principio de m√°xima verosimilitud?"]},{"cell_type":"markdown","metadata":{"id":"RjNisxPzWsMG"},"source":["` ` \n","\n","**Respuesta:** \n","\n","\\begin{equation}\n","q_{ML}(w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) = \\frac{Count(w_{i-3}, w_{i-2}, w_{i-1}, w_{i})}{Count(w_{i-3}, w_{i-2}, w_{i-1})}\n","\\end{equation}\n","\n","donde $Count(w_{i-3}, w_{i-2}, w_{i-1})$ corresponde a las ocurrencias de la cadena $(w_{i-3}w_{i-2}w_{i-1})$ en $C$\n","\n","` `"]},{"cell_type":"markdown","metadata":{},"source":["Para el caso de tener que calcular cada grama del modelo, podr√©mos calcularlos usando el principio de m√°xima verosimilitud como la suma de las ocurrencias de la cadena propuesta, divida en la cantidad de ocurrencias de la cadena sin considerar el √∫ltimo par√°metro. Eso simplemente es la extensi√≥n de los $q_{ML}$ vistos en clase, aplicado a gramas de 4 dimensiones en vez de 3 o 1."]},{"cell_type":"markdown","metadata":{"id":"bwNkPIXure0C"},"source":["#### 1.2.2. Modelo interpolado (0.5 puntos)\n","Muestre c√≥mo se calcular√≠a $q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1})$ usando un modelo que interpola modelos de lenguajes basados en 4-grams, trigramas, bigramas y unigramas. ¬øTe fue necesario definir nuevos par√°metros? En caso afirmativo, ¬øqu√© los diferencia de los par√°metros del modelo simple y qu√© propiedades deben cumplir?"]},{"cell_type":"markdown","metadata":{"id":"zeLZAd0Tr9ne"},"source":["` ` \n","\n","**Respuesta:** \n","\\begin{equation}\n","q(w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) = \\lambda_{1} \\times q_{ML}(w_{i} | w_{i-3}, w_{i-2}, w_{i-1}) + \\lambda_{2} \\times q_{ML}(w_{i} | w_{i-2}, w_{i-1}) + \\lambda_{3} \\times q_{ML}(w_{i} | w_{i-1}) + \\lambda_{4} \\times q_{ML}(w_{i})]\n","\\end{equation}\n","\n","donde $\\lambda_{1}+\\lambda_{2}+\\lambda_{3}+\\lambda_{4} = 1$, y para todo $\\lambda_{i}$ con $0 < i < 5$, $\\lambda_{i} > 0$\n","\n","` `\n"]},{"cell_type":"markdown","metadata":{},"source":["En este caso fue necesario definir 4 nuevos par√°metros para poder considerar una *ponderaci√≥n lineal* de cada modelo de i-grama. Estos se diferencian del modelo simple ya que considera tanto la probabilidad individual de cada palabra, como las ocurrencias de hasta 3 palabras anteriores.\n","Este modelo exige que cada par√°metro lambda sea mayor a 0 y que su sumatoria sea exactamente 1."]},{"cell_type":"markdown","metadata":{"id":"sHqcRJ7Vr_8x"},"source":["### 1.3. Ventajas modelo interpolado (0.5 puntos)\n","¬øQu√© ventajas tiene el modelo interpolado sobre el modelo de 4-gramas simple?\n"]},{"cell_type":"markdown","metadata":{"id":"6F5R3Ji7sHjt"},"source":["` ` \n","\n","**Respuesta:** \n","\n","El modelo interpolado no solamente considera la probabilidad condicional de cuatro palabras consecutivas, si no que tambi√©n la probabilidad de menos palabras consecutivas y la probabilidad individual de la misma palabra. Esto hace que el c√°lculo de la probabilidad de cada secuencia sea m√°s preciso y considere m√°s variables de contexto, comparado con solamente el modelo de 4-gramas simple o con un modelo unigrama / bag of words. \n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"vC2BGjlXxULB"},"source":["### 1.4 Evaluaci√≥n de Language Model (0.5 puntos)\n","Suponga que usted est√° modelando un language model sobre un corpus con un vocabulario $\\nu$, en donde N es igual a $\\nu$ + 1. Seg√∫n lo visto en clases, ¬øC√≥mo evaluar√≠a su modelo? Y ¬øC√≥mo podr√≠a determinar que su modelo tiene un mal rendimiento?, Justifique su respuesta. "]},{"cell_type":"markdown","metadata":{"id":"ZnBqiOGhxYfG"},"source":["` ` \n","\n","**Respuesta:** \n","\n","escriba su respuesta aqu√≠\n","\n","` ` "]},{"cell_type":"markdown","metadata":{"id":"rdmB-07ZKfaa"},"source":["-----------------------\n","## Parte 2. Na√Øve Bayes (3 pts)\n","En esta parte programaremos nuestro primer clasificador de documentos. Para esto implementaremos el m√©todo **Na√Øve Bayes Multinomial** usando **Laplace Smothing**.\n","\n","Por favor, documenten su c√≥digo. Escriban que hacen las funciones, que reciben, que entregan, etc. Si en alg√∫n lugar de su c√≥digo hacen algo que creen que debe ser explicado, lox comentarios son bienvenidos.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PpjjKnJUvRiA"},"source":["### 2.1. Clase para clasificador (0.5 pt)\n","\n","Programa una clase `MyMultinomialNB` que en su inicializador reciba el parametro de generalizaci√≥n `alpha`.\n","\n","```python\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, ...):\n","    ...\n","```\n","\n","Para m√°s informacion sobre la construcci√≥n de esta clase puedes revisar [aqu√≠](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#classifier) \n","\n","Una llamada de ejemplo para crear un objeto de tu clase ser√≠a:\n","```python\n","my_clf = MyMultinomialNB(alpha=1)\n","``` \n","lo que debiera crear un clasificador con par√°metro `alpha` igual a 1."]},{"cell_type":"markdown","metadata":{"id":"ROG50eH0xfxO"},"source":["### 2.2. Entrenamiento del clasificador (1 pt)\n","\n","Programa el entrenamiento de tu clasificador en el m√©todo `fit` de la clase `MyMultinomialNB`. La funci√≥n debiera recibir el par√°metro X que es un `DataFrame` de `pandas` con las columnas `words` y `class_`, donde `words` es una tupla con las palabras asociadas al cada documento y `class_` es el identificador de la clase a la que pertenece cada documento.\n","\n","Para computar el entrenamiento de nuestro clasificador debemos: \n","- extraer el vocabulario,\n","- determinar las probabilidades $p(c_j)$ para cada una de las clases posibles, \n","- determinar las probabilidades $p(w_i|c_j)$ para cada una de las palabras y cada una de las clases usando **Laplace Smothing**.\n","\n","El resultado del metodo `fit` ser√° la misma instancia de nuestro clasificador `self`.\n","\n","```python\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, ...):\n","    ...\n","\n","  def fit(self, X):\n","    ...\n","    return self\n","```\n","\n","**Underflow prevention:** En vez de hacer muchas multiplicacions de `float`s, reempl√°cenlas por sumas de logaritmos para prevenir errores de precisi√≥n. Revisen la diapo 69 de las slides. "]},{"cell_type":"markdown","metadata":{"id":"FNouTCmR2FgY"},"source":["### 2.3. Predicci√≥n (1 pt)\n","\n","Programa la predicci√≥n de tu clasificador en el m√©todo `predict` de la clase `MyMultinomialNB`. Al igual que la funci√≥n `fit`, `predict` debe recibir un `DataFrame` X con valores `None` en la columna `class_` y devolver una lista con las clases que maximizan la probabilidad de Bayes para cada uno de los elmentos de X (filas)."]},{"cell_type":"markdown","metadata":{"id":"8wyhFWeLgYDI"},"source":["### Implementaci√≥n 2.1, 2.2 y 2.3 (2.5 pt)"]},{"cell_type":"code","execution_count":442,"metadata":{"id":"DYFEgTyw2ELL"},"outputs":[],"source":["# Ac√° implementar√°n las preguntas 2.1, 2.2 y 2.3\n","\n","# importamos algunos paquetes necesarios\n","import numpy as np\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.utils.validation import check_is_fitted\n","\n","# definimos nuestra clase de clasificador Naive Bayes\n","class MyMultinomialNB(BaseEstimator, ClassifierMixin):\n","  def __init__(self, alpha=1.0):\n","    # inicializamos un clasificador con valor alpha\n","    self.alpha = alpha\n","\n","    # creamos un diccionario vac√≠o con el vocabulario\n","    self.vocab = []\n","\n","    # inicializamos las probabilidades de cada clase\n","    self.class_prob_dicc = {}\n","\n","    # inicializamos las probabilidades de cada palabra en cada clase\n","    # utilizando un dataframe\n","    self.word_prob_df = pd.DataFrame(columns=['class_', 'word', 'prob'])\n","\n","  def fit(self, X):\n","    # entrenamos el clasificador con los datos de entrenamiento\n","    # en el ejemplo, X es un dataframe de documentos con columnas ['words', 'class_']\n","    # primero, extraemos el vocabulario del documento, asumiendo que \n","    # words es una tupla de palabras y evitando repetidos\n","    print(X)\n","    for index, row in X.iterrows():\n","      for w in row['words']:  \n","        if w not in self.vocab:\n","          self.vocab.append(w)\n","\n","    # Ahora iremos por cada clase, calculando su probabilidad para\n","    # posteriormente calcular la probabilidad de cada palabra en cada clase\n","    # p(w | c) utilizando el teorema de Bayes para probabilidades condicionales\n","    for c in X['class_'].unique():  # para cada clase\n","      # calculamos la probabilidad de la clase\n","      self.class_prob_dicc[c] = X[X['class_'] == c].shape[0] / X.shape[0]\n","\n","      # luego calculamos la probabilidad de cada palabra \n","      for w in self.vocab:\n","        # contamos la cantidad de veces que aparece la palabra w en documentos con clase c\n","        # y la cantidad de palabras en documentos con clase c\n","        count_w_c = 0\n","        count_w = 0\n","        for index, row in X.iterrows():\n","          if row['class_'] == c:\n","            if w in row['words']:\n","              count_w_c += row['words'].count(w)\n","            count_w += len(row['words'])\n","        # calculamos la probabilidad usando laplace smoothing\n","        nominator_alpha = (count_w_c + self.alpha)\n","        denominator_alpha = (count_w + len(self.vocab) * self.alpha)\n","        # Y se guarda en el dataframe word_prob_df\n","        self.word_prob_df = self.word_prob_df.append({'class_': c, 'word': w, 'prob': nominator_alpha / denominator_alpha}, ignore_index=True)\n","    return self\n","\n","  def predict(self, Y):\n","    # Chequea que fit ha sido ejecutado anteriormente\n","    # check_is_fitted(self)\n","    \n","    # ahora predecimos la clase de cada documento recibido en Y\n","    # usando los datos de entrenamiento y underflow prevention\n","\n","    # Inicializamos una lista vac√≠a para guardar las clases predichas\n","    prediccions = []\n","\n","    # para cada documento\n","    for index, row in Y.iterrows():\n","      # inicializamos un diccinario vac√≠o para guardar las probabilidades\n","      # este contiene primero el logaritmo de la probabilidad de la clase\n","      probs_c = {}\n","      # por cada clase en el diccionario\n","      for k in self.class_prob_dicc.keys():\n","        # para cada palabra en el documento\n","        doc_prob = np.log(self.class_prob_dicc[k])\n","        for w in row['words']:\n","          # calculamos la probabilidad sumando el logaritmo de la probabilidad guardada en word_prob_df\n","          word_prob = self.word_prob_df[(self.word_prob_df['class_'] == k) & (self.word_prob_df['word'] == w)]['prob']\n","          try: \n","            doc_prob += np.log(word_prob.values[0])\n","          except:   # Si arroja error de ambigous truth value por pandas, aplicamos all()\n","            doc_prob += np.log(word_prob.all())\n","        probs_c[k] = doc_prob\n","      # luego la clase con mayor probabilidad es la clase predecida\n","      prediccions.append(max(probs_c, key=probs_c.get))\n","    # retornamos las clases predichas\n","    return prediccions "]},{"cell_type":"markdown","metadata":{"id":"_KOMJ-CS8PRP"},"source":["### 2.4. Probando el clasificador (0.5 pt)"]},{"cell_type":"markdown","metadata":{"id":"hucdz-R7xerG"},"source":["A continuaci√≥n probar√°n el funcionamiento de su clasificador. Para esto, les presentamos un conjunto de documentos de entrenamiento `train_set` divididos en 2 categorias distintas. Ustedes deber√°n primero entrenar su clasificador usando el m√©todo `fit` de su clase y luego, clasificar los documentos del conjunto de prueva `test_set` usando el m√©todo `predict`.\n","\n","**NOTA:** Como pueden ver, los objetos `namedtuple`s tienen dos atributos: `words` donde est√°n las palabras del documento y `class_` donde se guarda la clase de ese documento. Estos objetos son inmutables, lo que quiere decir que si quieren modificar un documento y cambiarle la clase, tienen que crear otro documento. Otra cosa es que son tuplas como cualquier otra, es decir se pueden acceder usando indices como `doc[0]` o `doc[1]`."]},{"cell_type":"code","execution_count":443,"metadata":{"id":"JjTnFLDGyCEL"},"outputs":[],"source":["import pandas as pd\n","\n","from collections import namedtuple\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"markdown","metadata":{"id":"JfS5wXfwxx6t"},"source":["#### 2.4.1 Primer Caso: Clasificaci√≥n de ejemplo visto en clases (0.20 pt)"]},{"cell_type":"code","execution_count":444,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230,"status":"ok","timestamp":1648470177360,"user":{"displayName":"Ignacio Meza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhI9R2Sl9kpnSGYDYcmCbQ2_IwnY3_WeFFdC_YSXg=s64","userId":"07738957670140287594"},"user_tz":180},"id":"5yXBv2Kqxyyf","outputId":"ac27f7bf-e30a-41cc-e79c-02710804431a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Documentos de entrenamiento\n","                          words class_\n","0   (Chinese, Beijing, Chinese)      c\n","1  (Chinese, Chinese, Shanghai)      c\n","2              (Chinese, Macao)      c\n","3       (Tokyo, Japan, Chinese)      j\n","\n","Documentos de prueba:\n","                                       words class_\n","0  (Chinese, Chinese, Chinese, Tokyo, Japan)   None\n"]}],"source":["document = namedtuple(\n","    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",")\n","\n","train_set = [['Chinese Beijing\tChinese', 'c'],\n","             ['Chinese\tChinese\tShanghai','c'],\n","             ['Chinese\tMacao','c'],\n","             ['Tokyo\tJapan\tChinese','j']]\n","\n","train_set = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set]\n","X_train = pd.DataFrame(data=train_set)\n","\n","test_set = [['Chinese\tChinese\tChinese\tTokyo Japan', None]]\n","test_set = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set]\n","X_test = pd.DataFrame(data=test_set)\n","\n","X_train = pd.DataFrame(data=train_set)\n","print(\"Documentos de entrenamiento\")\n","print(X_train)\n","\n","X_test = pd.DataFrame(data=test_set)\n","print(\"\\nDocumentos de prueba:\")\n","print(X_test)"]},{"cell_type":"code","execution_count":445,"metadata":{"id":"UAx0t3zQx2PJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["                          words class_\n","0   (Chinese, Beijing, Chinese)      c\n","1  (Chinese, Chinese, Shanghai)      c\n","2              (Chinese, Macao)      c\n","3       (Tokyo, Japan, Chinese)      j\n","vocab:  ['Chinese', 'Beijing', 'Shanghai', 'Macao', 'Tokyo', 'Japan']\n","\n","Test predictions:\n","c <- Chinese Chinese Chinese Tokyo Japan\n"]}],"source":["# Ac√° probar√°n su clasificador\n","\n","# inicializamos el clasificador\n","my_clf = MyMultinomialNB(alpha=1)\n","\n","# entrenamos el clasificador para los datos de entrenamiento X_train\n","my_clf.fit(X_train)\n","\n","# ac√° puedes ver el vocabulario extra√≠do por tu clasificador, \n","# intenta tenerlo guardado en my_clf.vocab\n","print('vocab: ', my_clf.vocab)\n","\n","# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio), \n","# ac√° lo pueden probar\n","# print('\\nTest probs:')\n","# print('\\n'.join([str(l) for l in my_clf.predict(X_test)]))\n","\n","# obtengamos las predicciones\n","print('\\nTest predictions:')\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"]},{"cell_type":"markdown","metadata":{"id":"dMyCgXxvx29L"},"source":["**Respuesta esperada:**\n","\n","**Nota:** No es necesario que obtenga exactamente la misma probabilidad, lo importante es que su clasificador genere la predicci√≥n expuesta.\n","\n","```python\n","vocab:  ['Beijing', 'Chinese', 'Macao', 'Tokyo', 'Japan', 'Shanghai']\n","\n","Test probs:\n","[0.68975861 0.31024139]\n","\n","Test predictions:\n","c <- Chinese Chinese Chinese Tokyo Japan\n","```"]},{"cell_type":"markdown","metadata":{"id":"mpG_t1wTx99m"},"source":["#### 2.4.2 Segundo Caso (0.30 pt)"]},{"cell_type":"code","execution_count":446,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1648470184312,"user":{"displayName":"Ignacio Meza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhI9R2Sl9kpnSGYDYcmCbQ2_IwnY3_WeFFdC_YSXg=s64","userId":"07738957670140287594"},"user_tz":180},"id":"HLi8PxdV2VQX","outputId":"9e8d2fbd-399d-4aab-f045-a965220360f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Documentos de entrenamiento\n","                                           words  class_\n","0            (w03, w01, w02, w06, w02, w08, w07)       0\n","1  (w05, w04, w00, w06, w09, w07, w06, w09, w05)       1\n","2  (w07, w06, w00, w08, w01, w08, w08, w09, w02)       0\n","3            (w08, w09, w02, w06, w05, w08, w07)       1\n","4            (w09, w08, w05, w08, w05, w00, w08)       1\n","5            (w05, w05, w06, w01, w06, w08, w02)       1\n","6            (w04, w03, w07, w05, w04, w00, w02)       0\n","7       (w01, w00, w01, w04, w09, w02, w04, w07)       1\n","\n","Documentos de prueba:\n","                                      words class_\n","0  (w02, w09, w06, w01, w05, w04, w03, w03)   None\n","1  (w03, w03, w04, w05, w01, w06, w09, w02)   None\n"]}],"source":["document = namedtuple(\n","    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",")\n","\n","train_set = (\n","    document(words=('w03', 'w01', 'w02', 'w06', 'w02', 'w08', 'w07'), class_=0),\n","    document(words=('w05', 'w04', 'w00', 'w06', 'w09', 'w07', 'w06', 'w09', 'w05'), class_=1),\n","    document(words=('w07', 'w06', 'w00', 'w08', 'w01', 'w08', 'w08', 'w09', 'w02'), class_=0),\n","    document(words=('w08', 'w09', 'w02', 'w06', 'w05', 'w08', 'w07'), class_=1),\n","    document(words=('w09', 'w08', 'w05', 'w08', 'w05', 'w00', 'w08'), class_=1),\n","    document(words=('w05', 'w05', 'w06', 'w01', 'w06', 'w08', 'w02'), class_=1),\n","    document(words=('w04', 'w03', 'w07', 'w05', 'w04', 'w00', 'w02'), class_=0),\n","    document(words=('w01', 'w00', 'w01', 'w04', 'w09', 'w02', 'w04', 'w07'), class_=1)\n",")\n","X_train = pd.DataFrame(data=train_set)\n","print(\"Documentos de entrenamiento\")\n","print(X_train)\n","\n","test_set = (document(words=('w02', 'w09', 'w06', 'w01', 'w05', 'w04', 'w03', 'w03'), class_=None),\n","            document(words=('w03', 'w03', 'w04', 'w05', 'w01', 'w06', 'w09', 'w02'), class_=None))\n","X_test = pd.DataFrame(data=test_set)\n","print(\"\\nDocumentos de prueba:\")\n","print(X_test)"]},{"cell_type":"code","execution_count":447,"metadata":{"id":"OXHwmOWB-4Aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                           words  class_\n","0            (w03, w01, w02, w06, w02, w08, w07)       0\n","1  (w05, w04, w00, w06, w09, w07, w06, w09, w05)       1\n","2  (w07, w06, w00, w08, w01, w08, w08, w09, w02)       0\n","3            (w08, w09, w02, w06, w05, w08, w07)       1\n","4            (w09, w08, w05, w08, w05, w00, w08)       1\n","5            (w05, w05, w06, w01, w06, w08, w02)       1\n","6            (w04, w03, w07, w05, w04, w00, w02)       0\n","7       (w01, w00, w01, w04, w09, w02, w04, w07)       1\n","vocab:  ['w03', 'w01', 'w02', 'w06', 'w08', 'w07', 'w05', 'w04', 'w00', 'w09']\n","\n","Test predictions:\n","0 <- w02 w09 w06 w01 w05 w04 w03 w03\n","0 <- w03 w03 w04 w05 w01 w06 w09 w02\n"]}],"source":["# Ac√° probar√°n su clasificador\n","\n","# inicializamos el clasificador\n","my_clf = MyMultinomialNB(alpha=1)\n","\n","# entrenamos el clasificador para los datos de entrenamiento X_train\n","my_clf.fit(X_train)\n","\n","# ac√° puedes ver el vocabulario extra√≠do por tu clasificador, \n","# intenta tenerlo guardado en my_clf.vocab\n","print('vocab: ', my_clf.vocab)\n","\n","# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio), \n","# ac√° lo pueden probar\n","# print('\\nTest probs:')\n","# print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test)]))\n","\n","# obtengamos las predicciones \n","print('\\nTest predictions:')\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_clf.predict(X_test), X_test['words'])]))"]},{"cell_type":"markdown","metadata":{"id":"5tDZnmns_1dW"},"source":["#### 2.4.3 (OPCIONAL) Oraciones reales\n","\n","Aqu√≠ intentaremos entrenar un clasificador para determinar cuando una oracion en ingl√©s es interrogativa, afirmativa o negativa."]},{"cell_type":"code","execution_count":448,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1648470188138,"user":{"displayName":"Ignacio Meza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhI9R2Sl9kpnSGYDYcmCbQ2_IwnY3_WeFFdC_YSXg=s64","userId":"07738957670140287594"},"user_tz":180},"id":"YCWi3oytd2nf","outputId":"ff582f23-6756-4d7d-fe64-f1add154b4c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Documentos de entrenamiento:\n","                                                words class_\n","0                (Do, you, have, plenty, of, time, ?)      ?\n","1                 (Does, she, have, enough, money, ?)      ?\n","2           (Did, they, have, any, useful, advice, ?)      ?\n","3                           (What, day, is, today, ?)      ?\n","4                      (I, do, n't, have, much, time)      -\n","5                  (She, does, n't, have, any, money)      -\n","6      (They, did, n't, have, any, advice, to, offer)      -\n","7                    (Have, you, plenty, of, time, ?)      ?\n","8                        (Has, she, enough, money, ?)      ?\n","9                 (Had, they, any, useful, advice, ?)      ?\n","10                         (I, have, n't, much, time)      -\n","11                        (She, has, n't, any, money)      -\n","12             (He, had, n't, any, advice, to, offer)      -\n","13                                 (How, are, you, ?)      ?\n","14    (How, do, you, make, questions, in, English, ?)      ?\n","15             (How, long, have, you, lived, here, ?)      ?\n","16      (How, often, do, you, go, to, the, cinema, ?)      ?\n","17                    (How, much, is, this, dress, ?)      ?\n","18                            (How, old, are, you, ?)      ?\n","19     (How, many, people, came, to, the, meeting, ?)      ?\n","20                            (I, ‚Äô, m, from, France)      +\n","21                           (I, come, from, the, UK)      +\n","22               (My, phone, number, is, 61709832145)      +\n","23  (I, work, as, a, tour, guide, for, a, local, t...      +\n","24                     (I, ‚Äô, m, not, dating, anyone)      -\n","25           (I, live, with, my, wife, and, children)      +\n","26        (I, often, do, morning, exercises, at, 6am)      +\n","27                                 (I, run, everyday)      +\n","28                         (She, walks, very, slowly)      +\n","29               (They, eat, a, lot, of, meat, daily)      +\n","30                  (We, were, in, France, that, day)      +\n","31                           (He, speaks, very, fast)      +\n","32          (They, told, us, they, came, back, early)      +\n","33                  (I, told, her, I, 'll, be, there)      +\n","\n","Documentos de prueba:\n","                                                words class_\n","0                (Do, you, know, who, lives, here, ?)      ?\n","1                             (What, time, is, it, ?)      ?\n","2    (Can, you, tell, me, where, she, comes, from, ?)      ?\n","3                                  (How, are, you, ?)      ?\n","4                              (I, fill, good, today)      +\n","5              (There, is, a, lot, of, history, here)      +\n","6                              (I, love, programming)      +\n","7      (He, told, us, not, to, make, so, much, noise)      +\n","8   (We, were, asked, not, to, park, in, front, of...      +\n","9                      (I, do, n't, have, much, time)      -\n","10                 (She, does, n't, have, any, money)      -\n","11     (They, did, n't, have, any, advice, to, offer)      -\n","12                         (I, am, not, really, sure)      +\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","document = namedtuple(\n","    \"document\", (\"words\", \"class_\")  # avoid python's keyword collision\n",")\n","\n","train_set2 = [\n","              ['Do you have plenty of time?', '?'],\n","              ['Does she have enough money?','?'],\n","              ['Did they have any useful advice?','?'],\n","              ['What day is today?','?'],\n","              [\"I don't have much time\",'-'],\n","              [\"She doesn't have any money\",'-'],\n","              [\"They didn't have any advice to offer\",'-'],\n","              ['Have you plenty of time?','?'],\n","              ['Has she enough money?','?'],\n","              ['Had they any useful advice?','?'],\n","              [\"I haven't much time\",'-'],\n","              [\"She hasn't any money\",'-'],\n","              [\"He hadn't any advice to offer\",'-'],\n","              ['How are you?','?'],\n","              ['How do you make questions in English?','?'],\n","              ['How long have you lived here?','?'],\n","              ['How often do you go to the cinema?','?'],\n","              ['How much is this dress?','?'],\n","              ['How old are you?','?'],\n","              ['How many people came to the meeting?','?'],\n","              ['I‚Äôm from France','+'],\n","              ['I come from the UK','+'],\n","              ['My phone number is 61709832145','+'],\n","              ['I work as a tour guide for a local tour company','+'],\n","              ['I‚Äôm not dating anyone','-'],\n","              ['I live with my wife and children','+'],\n","              ['I often do morning exercises at 6am','+'],\n","              ['I run everyday','+'],\n","              ['She walks very slowly','+'],\n","              ['They eat a lot of meat daily','+'],\n","              ['We were in France that day', '+'],\n","              ['He speaks very fast', '+'],\n","              ['They told us they came back early', '+'],\n","              [\"I told her I'll be there\", '+']\n","]\n","train_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in train_set2]\n","X_train2 = pd.DataFrame(data=train_set2)\n","print(\"Documentos de entrenamiento:\")\n","print(X_train2)\n","\n","test_set2 = [\n","             ['Do you know who lives here?','?'],\n","             ['What time is it?','?'],\n","             ['Can you tell me where she comes from?','?'],\n","             ['How are you?','?'],\n","             ['I fill good today', '+'],\n","             ['There is a lot of history here','+'],\n","             ['I love programming','+'],\n","             ['He told us not to make so much noise','+'],  # interesing case\n","             ['We were asked not to park in front of the house','+'],  # interesing case\n","             [\"I don't have much time\",'-'],\n","             [\"She doesn't have any money\",'-'],\n","             [\"They didn't have any advice to offer\",'-'],\n","             ['I am not really sure','+']\n","]\n","test_set2 = [document(words=tuple(word_tokenize(d[0])), class_=d[1]) for d in test_set2]\n","X_test2 = pd.DataFrame(data=test_set2)\n","print(\"\\nDocumentos de prueba:\")\n","print(X_test2)"]},{"cell_type":"code","execution_count":449,"metadata":{"id":"6Wdp22w2ArUl"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                words class_\n","0                (Do, you, have, plenty, of, time, ?)      ?\n","1                 (Does, she, have, enough, money, ?)      ?\n","2           (Did, they, have, any, useful, advice, ?)      ?\n","3                           (What, day, is, today, ?)      ?\n","4                      (I, do, n't, have, much, time)      -\n","5                  (She, does, n't, have, any, money)      -\n","6      (They, did, n't, have, any, advice, to, offer)      -\n","7                    (Have, you, plenty, of, time, ?)      ?\n","8                        (Has, she, enough, money, ?)      ?\n","9                 (Had, they, any, useful, advice, ?)      ?\n","10                         (I, have, n't, much, time)      -\n","11                        (She, has, n't, any, money)      -\n","12             (He, had, n't, any, advice, to, offer)      -\n","13                                 (How, are, you, ?)      ?\n","14    (How, do, you, make, questions, in, English, ?)      ?\n","15             (How, long, have, you, lived, here, ?)      ?\n","16      (How, often, do, you, go, to, the, cinema, ?)      ?\n","17                    (How, much, is, this, dress, ?)      ?\n","18                            (How, old, are, you, ?)      ?\n","19     (How, many, people, came, to, the, meeting, ?)      ?\n","20                            (I, ‚Äô, m, from, France)      +\n","21                           (I, come, from, the, UK)      +\n","22               (My, phone, number, is, 61709832145)      +\n","23  (I, work, as, a, tour, guide, for, a, local, t...      +\n","24                     (I, ‚Äô, m, not, dating, anyone)      -\n","25           (I, live, with, my, wife, and, children)      +\n","26        (I, often, do, morning, exercises, at, 6am)      +\n","27                                 (I, run, everyday)      +\n","28                         (She, walks, very, slowly)      +\n","29               (They, eat, a, lot, of, meat, daily)      +\n","30                  (We, were, in, France, that, day)      +\n","31                           (He, speaks, very, fast)      +\n","32          (They, told, us, they, came, back, early)      +\n","33                  (I, told, her, I, 'll, be, there)      +\n","vocab:  109 ['Do', 'you', 'have', 'plenty', 'of', 'time', '?', 'Does', 'she', 'enough', 'money', 'Did', 'they', 'any', 'useful', 'advice', 'What', 'day', 'is', 'today', 'I', 'do', \"n't\", 'much', 'She', 'does', 'They', 'did', 'to', 'offer', 'Have', 'Has', 'Had', 'has', 'He', 'had', 'How', 'are', 'make', 'questions', 'in', 'English', 'long', 'lived', 'here', 'often', 'go', 'the', 'cinema', 'this', 'dress', 'old', 'many', 'people', 'came', 'meeting', '‚Äô', 'm', 'from', 'France', 'come', 'UK', 'My', 'phone', 'number', '61709832145', 'work', 'as', 'a', 'tour', 'guide', 'for', 'local', 'company', 'not', 'dating', 'anyone', 'live', 'with', 'my', 'wife', 'and', 'children', 'morning', 'exercises', 'at', '6am', 'run', 'everyday', 'walks', 'very', 'slowly', 'eat', 'lot', 'meat', 'daily', 'We', 'were', 'that', 'speaks', 'fast', 'told', 'us', 'back', 'early', 'her', \"'ll\", 'be', 'there']\n","\n","Test predictions:\n","? <- Do you know who lives here ?\n","? <- What time is it ?\n","? <- Can you tell me where she comes from ?\n","? <- How are you ?\n","+ <- I fill good today\n","+ <- There is a lot of history here\n","+ <- I love programming\n","- <- He told us not to make so much noise\n","? <- We were asked not to park in front of the house\n","- <- I do n't have much time\n","- <- She does n't have any money\n","- <- They did n't have any advice to offer\n","+ <- I am not really sure\n","              precision    recall  f1-score   support\n","\n","           ?       1.00      0.67      0.80         6\n","           +       0.75      1.00      0.86         3\n","           -       0.80      1.00      0.89         4\n","\n","    accuracy                           0.85        13\n","   macro avg       0.85      0.89      0.85        13\n","weighted avg       0.88      0.85      0.84        13\n","\n"]}],"source":["# Ac√° probar√°n su clasificador y computaremos algunas m√©tricas de evaluacion\n","\n","from sklearn.metrics import classification_report\n","\n","# inicializamos el clasificador\n","my_clf2 = MyMultinomialNB(alpha=1)\n","\n","# entrenamos el clasificador para los datos de entrenamiento X_train2\n","my_clf2.fit(X_train2)\n","\n","# ac√° puedes ver el vocabulario extra√≠do por tu clasificador, \n","# intenta tenerlo guardado en my_clf.vocab\n","print('vocab: ', len(my_clf2.vocab), my_clf2.vocab)\n","\n","# si implementaron el m√©todo predict_proba en el clasificador (no era obligatorio), \n","# ac√° lo pueden probar\n","# print('\\nTest probs:')\n","# print('\\n'.join([str(l) for l in my_clf.predict_proba(X_test2)]))\n","\n","# obtengamos las predicciones para X_test2\n","print('\\nTest predictions:')\n","my_y_preds = my_clf2.predict(X_test2)\n","print('\\n'.join(['{} <- {}'.format(c, ' '.join(s)) for c, s in zip(my_y_preds, X_test2['words'])]))\n","print(classification_report(y_true=X_test2['class_'], y_pred=my_y_preds, target_names=['?', '+', '-']))"]},{"cell_type":"markdown","metadata":{"id":"WXbg6sNTdAlO"},"source":["**Respuesta aproximada:**\n","\n","**Nota:** No es necesario que obtenga exactamente los mismos resultados.\n","\n","```python\n","              precision    recall  f1-score   support\n","\n","           ?       1.00      0.67      0.80         6\n","           +       0.75      1.00      0.86         3\n","           -       0.80      1.00      0.89         4\n","\n","    accuracy                           0.85        13\n","   macro avg       0.85      0.89      0.85        13\n","weighted avg       0.88      0.85      0.84        13\n","```"]}],"metadata":{"colab":{"collapsed_sections":["JstKx3TiKcIp","2hwW-07MrRpt","gXSFlCIex8kq","bwNkPIXure0C","sHqcRJ7Vr_8x","vC2BGjlXxULB","rdmB-07ZKfaa","8wyhFWeLgYDI","JfS5wXfwxx6t","mpG_t1wTx99m","5tDZnmns_1dW"],"name":"Tarea 2 - Language Modeling y Na√Øve Bayes.ipynb","provenance":[{"file_id":"1kQCSkkokIVSZkHeo8sGEKwn6ROSItQme","timestamp":1618336151140},{"file_id":"1jZG4e_poENFYDeljHjx9goskyQ3Jy3GU","timestamp":1618284831252}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
