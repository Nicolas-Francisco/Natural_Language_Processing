{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Tarea 4 - HMM, MEMM, CRF, CNN, RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4lL5hGw07yP"
      },
      "source": [
        "# **Tarea 4 - CC6205 Natural Language Processing üìö**\n",
        "\n",
        "**Integrantes:** Lukas Gribbell e Ignacio N√∫√±ez\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** Martes 22 de junio.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6jB5fLGMCaI"
      },
      "source": [
        "Bienvenid@s a la cuarta tarea del curso de Natural Language Processing (NLP). \n",
        "En esta tarea estaremos tratando el problema de **tagging** (generaci√≥n de secuencias de etiquetas del mismo largo que la secuencia de input), el uso de **Convolutional Neural Networks** y **Recurrent Neural Networks**, e implementaremos una red usando PyTorch. \n",
        "\n",
        "Usen $\\LaTeX$ para las f√≥rmulas matem√°ticas. En la parte de programaci√≥n pueden usar lo que quieran, pero la [Axiliar 3](https://www.youtube.com/watch?v=r1pt9v_vuy8&list=PLwITEwTGVF6jXRu6l55W4OedqDiLgU9OM&index=2&t=5s) les puede ser de *gran ayuda*.\n",
        "\n",
        "**Instrucciones:**\n",
        "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n.\n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso.\n",
        "\n",
        "Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n",
        "\n",
        "**Referencias:**\n",
        "\n",
        "- [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/Tjgb-yQOg54), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n",
        "- [MEMMs and CRFs](slides/NLP-CRF.pdf) | ([tex source file](slides/NLP-CRF.tex)), [notes 1](http://www.cs.columbia.edu/~mcollins/crf.pdf), [notes 2](http://www.cs.columbia.edu/~mcollins/fb.pdf), [video 1](https://youtu.be/qlI-4lSUDkg), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/ZpUwDy6o28Y)\n",
        "- [Convolutional Neural Networks](slides/NLP-CNN.pdf) | ([tex source file](slides/NLP-CNN.tex)), [video](https://youtu.be/lLZW5Fn40r8)\n",
        "- [Recurrent Neural Networks](slides/NLP-RNN.pdf) | ([tex source file](slides/NLP-RNN.tex)), [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWXD3D7RYKJ-"
      },
      "source": [
        "# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF)\n",
        "\n",
        "### Pregunta 1 (1 pt)\n",
        "Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ \\text{DET}, \\text{NOUN}, \\text{VERB}, \\text{ADP} \\}$ y se tiene un Hidden Markov Model con los siguientes par√°metros estimados a partir de un corpus de entrenamiento:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "q(\\text{NOUN}| \\text{ VERB}, \\text{DET}) &= 0.3 \\\\\n",
        "q(\\text{NOUN}|\\ w, \\text{DET}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "q(\\text{DET}| \\text{ VERB}, \\text{NOUN}) &= 0.4 \\\\\n",
        "q(\\text{DET}|\\ w, \\text{NOUN}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "e(the|\\text{ DET}) &= 0.5 \\\\\n",
        "e(pasta|\\text{ NOUN}) &= 0.6\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Luego para la oraci√≥n: `the man is pouring sauce on the pasta`, se tiene una tabla de programaci√≥n din√°mica con los siguientes valores:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\pi(7,\\text{DET},\\text{DET})&=0.1\\\\\n",
        "\\pi(7,\\text{NOUN},\\text{DET})&=0.2\\\\\n",
        "\\pi(7,\\text{VERB},\\text{DET})&=0.01\\\\\n",
        "\\pi(7,\\text{ADP},\\text{DET})&=0.5\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Con esta informaci√≥n, calcule el valor de $\\pi(8,\\text{DET},\\text{NOUN})$. Puede dejar el resultado expresado como una fracci√≥n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UvNmJMvi83q"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "\\begin{equation}\n",
        "  \\pi(k,u,v)= \\max_{w\\in S_{k-2}} (\\pi(k-1,w,u))\\times q(v|w,u)\\times e(x_k|v))\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "  \\pi(8,DET,NOUN)= \\max_{w\\in S_{6}} (\\pi(7,w,DET))\\times q(NOUN|w,DET)\\times e(pasta|NOUN)) \n",
        "\\end{equation}\n",
        "\n",
        "Utilizando los valores del enunciado tenemos los siguientes casos:\n",
        "\n",
        "$w=VERB =>$\n",
        "\n",
        "\\begin{equation}\n",
        "  (\\pi(7,VERB,DET))\\times q(NOUN|VERB,DET)\\times e(pasta|NOUN) = \\frac{1}{100} * \\frac{3}{10} * \\frac{6}{10} = \\frac{18}{10000}\n",
        "\\end{equation}\n",
        "\n",
        "Luego, por la condici√≥n $ q(\\text{NOUN}|\\ w, \\text{DET}) = 0,  \\forall w \\in S, w \\neq \\text{VERB} $, tenemos que si $w=DET|NOUN|ADP =>$\n",
        "\n",
        "\\begin{equation}\n",
        "  (\\pi(7,w,DET))\\times q(NOUN|w,DET)\\times e(pasta|NOUN) = 0\n",
        "\\end{equation}\n",
        "\n",
        "y como tenemos un $VERB$ en la posicion 4 de la oraci√≥n podemos afirmar que:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\pi(8,DET,NOUN)= \\max_{w\\in S_{6}} (\\pi(7,w,DET))\\times q(NOUN|w,DET)\\times e(pasta|NOUN)) = \\frac{18}{10000}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiwJb_vmkKLZ"
      },
      "source": [
        "### Pregunta 2 (0.5 pts)\n",
        "Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n",
        "\n",
        "#### 2.1. ¬øPara qu√© tipo de tarea sirven? D√© dos ejemplo de este tipo de tarea y descr√≠balos brevemente. (0.1 pts)\n",
        "\n",
        "**Respuesta:** Estos modelos son especialmente √∫tiles para problemas de **sequence labeling** o tambi√©n llamados **tagging**. En ellos el input es una secuencia de palabras y se deben identificar etiquetar los conceptos o conjuntos de conceptos que pertenezcan a cierta categor√≠a. De ejemplos bien estudiados de esto son (i) Part of speech (POS) que etiqueta los tokens del input en base al tipo de palabra que representan, como por ejjemplo verbos, adjetivo, sustantivo, adverbio, etc y (ii) Named Entity Recognition (NER), en donde se busca etiquetar ciertas tokens dependiendo de si pertenecen a cierta categor√≠a definida o no. Un ejemplo de esto es la competencia 4, donde se pide encontrar dentro de un texto ciertas enfermedades, procedimientos m√©dicos, partes del cuerpo, etc.\n",
        "\n",
        "#### 2.2. ¬øQu√© modelos usan features? ¬øQu√© ventajas conlleva esto? (0.1 pts)\n",
        "\n",
        "**Respuesta:** Los modelos MEMM y CRF utilizan features. La ventaja de esto es que se pueden generar representaciones m√°s ricas del texto entregado como input, en comparaci√≥n al conteo de palabras que realiza HMM. As√≠ se pueden representar reglas sem√°nticas del lenguaje que permitan realizar una detecci√≥n m√°s precisa.\n",
        "\n",
        "#### 2.3. ¬øC√≥mo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train? (0.1 pts)\n",
        "\n",
        "**Respuesta:** Los modelos HMM agrupan los t√©rminos de baja frecuencia en categor√≠as que engloban palabras con caracter√≠sticas similares. De las categor√≠as vistas en clases podemos mencionar n√∫mero de dos d√≠gitos, n√∫mero de cuatro d√≠gitos, letra may√∫scula seguida de un punto (CapPeriod). La idea es que cada categor√≠a pueda representar algo. En el caso de los n√∫meros pueden representar a√±os y en el caso del CapPeriod podr√≠an ser las iniciales de un nombre.\n",
        "\n",
        "Por otro lado, los modelos MEMM y CRF no tienen el problema de que el conteo de palabras de baja frecuencia pueda ser 0, por lo que el manejo de ellas no es cr√≠tico. La manera en que se manejan pueden ser definidas en c√≥mo se forma el vector de caracter√≠sticas.\n",
        "\n",
        "#### 2.4. ¬øQu√© le permite a los CRF realizar decisiones globales? ¬øQu√© diferencia con respecto a los MEMMs permite lograr esto? ¬øPor qu√© los HMMs tampoco son capaces de tomar decisiones globales? (0.1 pts)\n",
        "\n",
        "**Respuesta:** Los CRF permiter realizar decisiones globales ya que se modela directamente la probabilidad de la secuencia completa de etiquetas dada la secuencia de palabras. Esto no es posible en los modelos MEMM porque √©stos descomponen el problema y lo modelan como probabilidades locales con restricciones Markovianas a s√≥lo algunas pocas etiquetas previas. En cambio, en CRF la probabilidad de una secuencia de etiquetas, considera a todas ellas a pesar de que igual usen representaciones locales, pero √©stas contribuyen a generar una representaci√≥n global del texto.\n",
        "\n",
        "Al igual que los modelos MEMM, los modelos HMM modelan el problema de manera local, mirando unas pocas etiquetas previas para calcular probabilidades locales para cada token del input. Es por esto que tampoco pueden tomar decisiones globales.\n",
        "\n",
        "#### 2.5 Dado una secuencia de $x_1, ..., x_m$ ¬øCu√°ntas posibles secuencias de etiquetas se pueden generar para un conjunto de etiquetas $S$ con $|S|=k$ ? ¬øAnalizarlas todas ser√≠a computacionalmente tratable? (0.1 pts)\n",
        "\n",
        "**Respuesta:** Se podr√≠an generar $k^m$ secuencias distintas. Este es un problema que es computacionalmente tratable en modelos CRF si es que se consideran ciertas restricciones y algoritmos eficientes.\n",
        "\n",
        "Para ello, vimos en clases que en modelos CRF la representaci√≥n global de la secuencia de tokens y etiquetas tienen que estar compuesta por la suma de representaciones locales con restricciones Markovianas, similares a lo que se hace en modelos MEMM. Por otro lado, usando el algoritmo de Forward-Backward y programaci√≥n din√°mica, hacen que se pueda realizar una iteraci√≥n eficiente entre todas las posibles combinaciones de secuencias de etiquetas distintas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClRAHR95Y8aB"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "### Pregunta 3 (1 pt)\n",
        "\n",
        "Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n",
        "\n",
        "La siguiente matriz de embeddings, donde la i-√©sima fila corresponde al vector de embedding de la i-√©sima palabra, ordenadas seg√∫n aparecen en la frase. (vectores de largo 2).\n",
        "\\begin{equation}\n",
        "E = \\begin{pmatrix}\n",
        "2 & 2\\\\\n",
        "0 & -2\\\\\n",
        "0 & 1\\\\\n",
        "-2 & 1\\\\\n",
        "1 & 0\\\\\n",
        "-1 & 1\\\\\n",
        "1 & 1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Los siguientes 3 filtros\n",
        "\\begin{equation}\n",
        "U = \\begin{pmatrix}\n",
        "-1 & 1 & 0\\\\\n",
        "1 & 1 & 0\\\\\n",
        "0 & 0 & -1\\\\\n",
        "1 & -1 & -1\\\\\n",
        "-1 & -1 & 1\\\\\n",
        "1 & 0 & -1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Y la funci√≥n de activaci√≥n\n",
        "\\begin{equation}\n",
        "tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
        "\\end{equation}\n",
        "\n",
        "Usando estos param√°tros escriba los pasos para calcular la representaci√≥n (vector) resultante de aplicar la operaci√≥n de convoluci√≥n (sin padding) + max pooling. ¬øDe qu√© tama√±o ser√≠a la ventana que debemos usar?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlQ30Arkq0u4"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "El primer paso es definir las ventanas, las cuales estar√°n determinadas por el tama√±o del filtro, ya que necesitamos que los tama√±os coincidan para realizar la multiplicacion.\n",
        "\n",
        "\\begin{equation}\n",
        "  \\vec{x_i} = \\oplus(w_{i:i+k-1}), x_i \\in \\mathcal{R}^{k\\cdot d_{emb}}\n",
        "\\end{equation}\n",
        "\n",
        "En este caso tenemos que $U$ es de tama√±o $6\\times 3$, por lo que necesitamos que la dimension de $\\vec{x_i}$ sea $6$, y como sabemos que $d_{emb}$ (tama√±o de fila de $E$) es $2$, tenemos que $k=3$.\n",
        "\n",
        "Por lo tanto $\\vec{x}$ estar√° compuesto por los trigramas de la frase $w$.\n",
        "\n",
        "El siguiente paso es aplicar los filtros (producto punto) a cada ventana. De esto saldr√°n vectores de dimensi√≥n $3$ (una columna por cada filtro aplicado), a los cuales se tendr√° que aplicar $tanh(x)$, para finalmente pasar al max pooling.\n",
        "\n",
        "Este √∫ltimo paso es resumir toda la matriz resultante. En este caso ser√≠a una matriz de $5\\times 3$ dado que se pueden sacar hasta $5$ trigramas distintos de $w$ y son $3$ los filtros aplicados. Lo que hace max pooling es obtener el valor m√°ximo de cada columna (cada filtro finalmente) y obtener asi un vector de tama√±o $3$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZdqJSNZKEBq",
        "outputId": "2b7ace60-24cb-4fe1-cc9d-12da9695058b"
      },
      "source": [
        "# puedes comprobar tus calculos con algunas lineas de c√≥digo python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "E = np.array([\n",
        "              [2,2],\n",
        "              [0,-2],\n",
        "              [0,1],\n",
        "              [-2,1],\n",
        "              [1,0],\n",
        "              [-1,1],\n",
        "              [1,1]\n",
        "])\n",
        "\n",
        "U = np.array([[-1, 1, 0],\n",
        "              [1,1,0],\n",
        "              [0,0,-1],\n",
        "              [1,-1,-1],\n",
        "              [-1,-1,1],\n",
        "              [1,0,-1]])\n",
        "\n",
        "x = []\n",
        "for i in range(len(E)-2):\n",
        "  x1 = []\n",
        "  for j in range(i,i+3):\n",
        "    x1.append(E[j][0])\n",
        "    x1.append(E[j][1])\n",
        "  x.append(x1)\n",
        "x=np.array(x)\n",
        "m = np.matmul(x,U)\n",
        "print('Tama√±o xi:')\n",
        "print(len(x[0]))\n",
        "print('Tama√±o x:')\n",
        "print(x.shape)\n",
        "print('Tama√±o despues de multiplicacion:')\n",
        "print(m.shape)\n",
        "print('Tama√±o max pooling')\n",
        "print(len(m[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tama√±o xi:\n",
            "6\n",
            "Tama√±o x:\n",
            "(5, 6)\n",
            "Tama√±o despues de multiplicacion:\n",
            "(5, 3)\n",
            "Tama√±o max pooling\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj1V_sAzZCHY"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "### Pregunta 4 (1 pt)\n",
        "Usando los embeddings de dos dimensiones de la pregunta anteror, la oraci√≥n `el fuego quema` la podemos representar por una secuencia de vectores $(\\vec{x}_1,\\vec{x}_2,\\vec{x}_3)$, con $\\vec{x}_i \\in \\mathbb{R}^{d_x}$ y $d_x=2$.\n",
        "\n",
        "Tenemos una red recurrente *Elman* definidad como: \n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n",
        "\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "donde\n",
        "\\begin{equation}\n",
        "\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s},\n",
        "\\end{equation}\n",
        "y los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n",
        "\n",
        "Sea\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_0 &= [0,0,0]\\\\\n",
        "W^x &= \\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix} \\\\\n",
        "W^s &= \\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix} \\\\\n",
        "\\vec{b} &= [0, 0, 0] \\\\\n",
        "g(x) &= ReLu(x) = max(0, x)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "<br>\n",
        "\n",
        "Calcule manualmente los valores de los vectores $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M7sqIQV-Q3a"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "Con los valores entregados en la pregunta anterior tenemos que `el fuego quema` corresponde a la siguiente secuencia de vectores:\n",
        "\n",
        "\\begin{equation}\n",
        "  ((2, 2),(-1, 1),(1, 1))\n",
        "\\end{equation}\n",
        "\n",
        "Por lo tanto, utilizando esos valores y las formulas dadas en el enunciado tenemos que\n",
        "\n",
        "\\begin{equation}\n",
        "  \\vec{x}_1 = (2,2) \\\\\n",
        "  \\vec{s}_{0}W^s + \\vec{x}_1 W^x + \\vec{b} = (0,0,0) \\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix}+ (2,2) \\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix} + (0,0,0) = (2,-2,2)\\\\\n",
        " => \\vec{s}_1 = (2,0,2)\n",
        "\\end{equation}\n",
        "\\\\\n",
        "\\begin{equation}\n",
        "  \\vec{x}_2 = (-1,1) \\\\\n",
        "  \\vec{s}_{1}W^s + \\vec{x}_2 W^x + \\vec{b} = (2,0,2) \\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix}+ (-1,1) \\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix} + (0,0,0) = (5,1,3)\\\\\n",
        " => \\vec{s}_2 = (5,1,3)\n",
        "\\end{equation}\n",
        "\\\\\n",
        "\\begin{equation}\n",
        "  \\vec{x}_3 = (1,1) \\\\\n",
        "  \\vec{s}_{2}W^s + \\vec{x}_3 W^x + \\vec{b} = (5,1,3) \\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix}+ (1,1) \\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix} + (0,0,0) = (9,3,8)\\\\\n",
        " => \\vec{s}_2 = (9,3,8)\n",
        "\\end{equation}\n",
        "\\\\\n",
        "\\begin{equation}\n",
        "  \\begin{split}\n",
        "    => &\\\\\n",
        "    \\vec{y}_1 & = (2,0,2)\\\\\n",
        "    \\vec{y}_2 & = (5,1,3)\\\\\n",
        "    \\vec{y}_3 & = (9,3,8)\\\\\n",
        "  \\end{split}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4rAT6ELxRZW"
      },
      "source": [
        "### Pregunta 5 (0.5 pts)\n",
        "¬øDe qu√© forma las RNN y las CNN logran aprender representaciones espec√≠ficas\n",
        "para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* dise√±adas manualmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6AXbQSgA_t8"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "Las RNN y las CNN logran aprender representaciones espec√≠ficas mediante el uso de funciones parametrizadas. En el caso de las redes convolucionales los kernels que se aplican son entrenables, de igual manera que las funciones R y O de las RNN.\n",
        "\n",
        "Esto permite que la m√°quina aprenda por s√≠ sola qu√© representaciones son convenientes alterando los par√°metros de dichas funciones mediante la optimizaci√≥n de la Loss y el consiguiente Back Propagation en cada iteraci√≥n.\n",
        "\n",
        "Una forma de verlo es que en los modelos donde los features se definen manualmente √©stos par√°metros son est√°ticos y son definidos por el usuario, lo que le quita expresividad a los datos de entrenamiento, pues est√°n restringidos a las features definidas previamente. √âstos modelos solo permiten entrenar la clasificaci√≥n de las representaciones, mientras que las RNN y las CNN permiten aprender tanto la representaci√≥n como la clasificaci√≥n al mismo tiempo. √âsta es la gran ventaja de √©stas arquitecturas.\n",
        "\n",
        "Si bien las features dise√±adas manualmente pueden hacernos m√°s sentido y en el √°rea de NLP pueden tener fundamentos, por ejemplo, en el estudio del lenguaje y su estructura sem√°ntica, es muy trabajoso cubrir todos los posibles escenarios que se puedan presentar y son menos flexibles a diferencias particulares en los corpus y en las tareas. Por eso que, como vimos en clases, las RNN sirven para abordar una gran cantidad de problemas con alta precisi√≥n.\n",
        "\n",
        "Es por esto que las redes neuronales presentan una gran ventaja al armar por s√≠ mismas las representaciones y adecuarse de mejor manera a las caracter√≠sticas particulares del corpus y la tarea que interesa realizar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxQIuO8axTUa"
      },
      "source": [
        "# Redes neuronales con PyTorch\n",
        "### Pregunta 6 (2 pts)\n",
        "En esta parte van a tener que implementar una red neuronal Feed Forward. Adem√°s, deber√°n entrenar el modelo usando uno de los datasets de TorchText. En la secci√≥n de la respuesta hay un esqueleto de lo que deben hacer, deber√°n completar los metodos del modelo e implementar la parte asociada al entrenamiento. Como les mencionamos en la [Auxiliar 3](https://www.youtube.com/watch?v=r1pt9v_vuy8&list=PLwITEwTGVF6jXRu6l55W4OedqDiLgU9OM&index=2&t=5s), el proceso de entrenamiento es bastante est√°ndar, as√≠ que se pueden guiar en gran medida por los ejemplos que ah√≠ mostramos y los que vamos a ver en las pr√≥ximas auxiliares.\n",
        "\n",
        "#### 6.1 Capa Convolucional (Opcional)\n",
        "Agregue a la arquitectura una capa convolucional. Para esto puede registrar el parametro $U$ en la red y realizar el computo de la convoluci√≥n en el metodo forward de la red, o puede usar la clase [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d) de `torch`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVKEaQXZ3eGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc59f6f-d240-4d37-f37c-c5b104f845ca"
      },
      "source": [
        "#%%capture\n",
        "# Nos aseguramos que torchtext este en la ultima version\n",
        "!pip install torchtext==0.9.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/ef/a9c28bc62be28294a4def5da3286f3c020e0d3ff9b110b97f1f24b463508/torchtext-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (7.1MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.1MB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.1) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.1) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.1) (2.23.0)\n",
            "Collecting torch==1.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 804.1MB 22kB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchtext==0.9.1) (3.7.4.3)\n",
            "\u001b[31mERROR: torchvision 0.10.0+cu102 has requirement torch==1.9.0, but you'll have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchtext\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed torch-1.8.1 torchtext-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ-wrzFO5mCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1513707-1a12-4ae1-b3d6-ae189e1af5af"
      },
      "source": [
        "# Trabajaremos con el dataset AG_NEWS de torchtext\n",
        "# https://pytorch.org/text/stable/datasets.html#ag-news\n",
        "import os\n",
        "import torch\n",
        "from random import choice\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "from itertools import zip_longest\n",
        "from torchtext.datasets import AG_NEWS\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "train_dataset, test_dataset = AG_NEWS(root=\"data\")\n",
        "\n",
        "train_list = list(train_dataset)\n",
        "test_list  = list(test_dataset)\n",
        "\n",
        "# Informacion relevante del dataset\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "vocab = build_vocab_from_iterator(tokenizer(x[1]) for x in train_list)\n",
        "num_classes = 4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.csv: 29.5MB [00:00, 94.3MB/s]\n",
            "test.csv: 1.86MB [00:00, 48.1MB/s]                  \n",
            "120000lines [00:03, 36606.82lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXngUm9HxKvA"
      },
      "source": [
        "# De aca para abajo viene su respuesta, completen las funciones en la red\n",
        "# y luego entrenen el modelo y evaluenlo usando los dataset que acaban de\n",
        "# cargar\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, num_classes=4, \n",
        "                 use_cnn=False, cnn_pool_channels=4, cnn_kernel_size=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Aca deben registrar sus parametros. A lo menos necesitan\n",
        "        # una capa de embedding y un MLP basico (una capa lineal + softmax)\n",
        "        #self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size,\n",
        "            embed_dim,\n",
        "            padding_idx=vocab[\"<pad>\"])\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Si se quiere usar m√°s de una capa\n",
        "        # self.fc1 = nn.Linear(embed_dim, hidden_size)\n",
        "        # self.fc2 = nn.Linear(hidden_size, num_class)\n",
        "        self.use_cnn = use_cnn\n",
        "        self.conv = nn.Conv1d(in_channels=1,out_channels=num_classes, kernel_size=cnn_kernel_size, stride=embed_dim)\n",
        "\n",
        "    def forward(self, batch): # Reemplacen el *args por sus argumentos\n",
        "        # Ac√° debe programar la pasada hacia adelante\n",
        "        h = self.embedding(batch)\n",
        "        #print(batch)\n",
        "        if self.use_cnn:\n",
        "          h = h.view(h.shape[0], 1, -1)\n",
        "          h = F.relu(self.conv(h))\n",
        "          return h.max(dim=-1).values\n",
        "        h = self.fc(h)\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPHyqgId96ra",
        "outputId": "76f57f77-1c3d-4de7-e633-6109ab55fee1"
      },
      "source": [
        "# El resto de su respuesta. Ac√° deben programar el entrenamiento de la red\n",
        "\n",
        "# Copiado de Aux 4\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def generate_batch(batch):\n",
        "  label_list, text_list = [], []\n",
        "  for (_label, _text) in batch:\n",
        "    label_list.append(_label - 1)\n",
        "    processed_text = torch.tensor([vocab.stoi[w] for w in tokenizer(_text)])\n",
        "    text_list.append(processed_text)\n",
        "  return (\n",
        "      pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"]), \n",
        "      torch.tensor(label_list)\n",
        "  )\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "N_EPOCHS = 20\n",
        "LEARN_RATE = 2.5\n",
        "STEP_SIZE = 1\n",
        "BATCH_SIZE = 128\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 200\n",
        "\n",
        "model = CNNClassifier(VOCAB_SIZE, embed_dim=EMBED_DIM, num_classes=num_classes, \n",
        "                      use_cnn=True, \n",
        "                      cnn_pool_channels=4, \n",
        "                      cnn_kernel_size=3*EMBED_DIM).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE, weight_decay=0.0001)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE, weight_decay=0.0001)\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4y6bBsvVGbF"
      },
      "source": [
        "# Ac√° deben programar el entrenamiento de la red.\n",
        "\n",
        "def train_func(sub_train_, model):\n",
        "\n",
        "  # Train the model\n",
        "  train_loss ,train_acc = 0, 0\n",
        "  data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                    collate_fn=generate_batch)\n",
        "  \n",
        "  for text, cls in data:\n",
        "    optimizer.zero_grad()\n",
        "    text, cls = text.to(device), cls.to(device)\n",
        "    output = model(text)\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
        "\n",
        "def test(data_, model):\n",
        "  loss, acc = 0, 0\n",
        "  data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "  for text, cls in data:\n",
        "    text, cls = text.to(device), cls.to(device)\n",
        "    with torch.no_grad():\n",
        "      output = model(text)\n",
        "      loss = criterion(output, cls)\n",
        "      loss += loss.item()\n",
        "      acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  return loss / len(data_), acc / len(data_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vQuwHpgVC93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9923b40e-db11-49d1-bd16-a4e4665e2e23"
      },
      "source": [
        "# Intenten superar un Accuracy de 90% en el conjunto de test\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(train_list, model)\n",
        "    valid_loss, valid_acc = test(test_list, model)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs // 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch + 1}\", f\" | time in {mins} minutes, {secs} seconds\",\n",
        "    )\n",
        "    print(\n",
        "        f\"\\tLoss: {train_loss:.4f}(train)\\t|\"\n",
        "        f\"\\tAcc: {train_acc * 100:.1f}%(train)\"\n",
        "    )\n",
        "    print(\n",
        "        f\"\\tLoss: {valid_loss:.4f}(valid)\\t|\"\n",
        "        f\"\\tAcc: {valid_acc * 100:.1f}%(valid)\"\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0252(train)\t|\tAcc: 66.6%(train)\n",
            "\tLoss: 0.0003(valid)\t|\tAcc: 72.4%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0085(train)\t|\tAcc: 77.0%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 76.9%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0052(train)\t|\tAcc: 81.3%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 81.9%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0038(train)\t|\tAcc: 84.5%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 82.9%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0031(train)\t|\tAcc: 86.9%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 86.5%(valid)\n",
            "Epoch: 6  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0027(train)\t|\tAcc: 88.4%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 86.1%(valid)\n",
            "Epoch: 7  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0024(train)\t|\tAcc: 89.6%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 87.9%(valid)\n",
            "Epoch: 8  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0022(train)\t|\tAcc: 90.3%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 88.2%(valid)\n",
            "Epoch: 9  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0021(train)\t|\tAcc: 90.9%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 88.3%(valid)\n",
            "Epoch: 10  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0020(train)\t|\tAcc: 91.3%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 87.1%(valid)\n",
            "Epoch: 11  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0020(train)\t|\tAcc: 91.6%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 87.7%(valid)\n",
            "Epoch: 12  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0019(train)\t|\tAcc: 91.8%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 88.8%(valid)\n",
            "Epoch: 13  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0019(train)\t|\tAcc: 92.1%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 88.6%(valid)\n",
            "Epoch: 14  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0018(train)\t|\tAcc: 92.2%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 88.7%(valid)\n",
            "Epoch: 15  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0018(train)\t|\tAcc: 92.5%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 88.7%(valid)\n",
            "Epoch: 16  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0018(train)\t|\tAcc: 92.6%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 88.5%(valid)\n",
            "Epoch: 17  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0017(train)\t|\tAcc: 92.7%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 87.8%(valid)\n",
            "Epoch: 18  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0017(train)\t|\tAcc: 92.7%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 88.4%(valid)\n",
            "Epoch: 19  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0017(train)\t|\tAcc: 92.9%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 88.7%(valid)\n",
            "Epoch: 20  | time in 0 minutes, 24 seconds\n",
            "\tLoss: 0.0017(train)\t|\tAcc: 92.9%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 88.9%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}